\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Automatic Differentiation}
\label{chp:auto-diff}

One of the central parts of the success of neural networks in machine learning
is the backpropagation algorithm of
\cref{eq:backprop-delta-recursive,eq:backprop-grads-recursive}, which is able to
describe how parameters should be updated in arbitrary networks by propagating
the error estimates backwards through the network. While we could derive these
equations relatively simply in the case of a pure feed-forward neural network
such as the ones described here, there are a whole world of variants for which
we would need to derive different equations, such as recurrent networks,
residual networks, convolutional networks etc. However, they all still rely on the basic
premise of applying the chain rule.

One of the perhaps most appealing attributes of \gls{ml} frameworks such as TensorFlow
is how we can design arbitrary networks \emph{without} having to manually work
out the associated derivatives. They way this is done is though \gls{ad}, an
algorithmic application of the chain rule that can compute derivatives of any
function specified by a computer program.

This is possible by exploiting the fact that any computer program must be built
up of elementary operations (\(+,-,*,/\)) and perhaps some elementary functions
(\(\exp, \sin\) etc.). We know how to handle derivatives for all of these, and
importantly we know how to combine them in arbitrary ways through the chain
rule. This allows us to construct a computation for any function which
will compute its derivative.

\Gls{ad} promises a general purpose implementation that can
calculate derivatives exactly (within numerical precision) in a manner that uses
at most a constant factor more operations than the original function. We can
also generalize to higher order derivatives by repeated application of the
algorithm. That is, viewing the derivative computation as a function in its own
right we can apply the same trick to construct a computation for the second
derivative.


\section{Example}

Let the function we wish to differentiate be the following:\footnote{This example is in large part similar to one found
    on the \gls{ad} Wikipedia page, simply because I quite like this explanation.}

\begin{align}
  f(x_1, x_2) &= x_1x_2 + \sin(x_1)\\
              &= w_1w_2 + \sin(w_1)\\
              &= w_3 + w_4\\
              &= w_5 = y,
\end{align}
where we have introduced the temporary variables $w_i$ for later notational brevity.
Assume that we are interested in $\pdv*{y}{x_1}$ and $\pdv*{y}{x_2}$. There are
two main approaches in use: Forward and Reverse mode.

\subsection{Forward Mode}

We first decide on one independent variable to differentiation with respect to,
say $x_1$. We then compute $\dot w_i = \pdv*{w_i}{x_1}$ in order of increasing
$i$ (forward through the computation).

When we step through the computation of $f(x_1, x_2)$, we can simultaneously
compute the derivative we want. By normal rules of differentiation:

\begin{center}
  \begin{tabular}{ll}
    Value & Derivative (w.r.t. $x_1$)\\\hline\\
    $w_1 = x_1$ & $\dot w_1 = 1$\\
    $w_2 = x_2$ & $\dot w_2 = 0$\\
    $w_3 = w_1w_2$ & $\dot w_2 = \dot w_1 w_2 + w_1\dot w_2$\\
    $w_4 = \sin(w_1)$ & $\dot w_4 = \cos(w_1)\dot w_1$\\
    $w_5 = w_3 + w_4$ & $\dot w_5 = \dot w_3 +\dot w_4$\\
  \end{tabular}
\end{center}
Substituting in the values from top to bottom we see that we indeed get
$\pdv{y}{x_1}=\cos(x_1)+x_2$ as one would expect. We can now repeat the process
once more for $x_2$. Evidently, we only need to change the initial values $\dot
w_1$ and $\dot w_2$, while the rest remains the same. Because of this, these
\say{trivial} first derivatives are sometimes referred to as \emph{seed} values.

\subsection{Reverse Mode}

Instead of first fixing the independent variable, we
instead fix the dependent variable which should be differentiated (\(y\)), and
then compute its derivative w.r.t. the different sub expressions $w_i$. We
define \(\bar w_i = \pdv*{y}{w_i}\), and construct the following table of
operations (backwards through the computation):

\begin{align*}
  \bar w_5 &= \pdv{y}{w_5} = 1\\
  \bar w_4 &= \pdv{y}{w_4} = \bar w_5\pdv{w_5}{w_4} = \bar w_5\\
  \bar w_3 &= \pdv{y}{w_3} = \bar w_5\pdv{w_5}{w_3} = \bar w_5\\
  \bar w_2 &= \pdv{y}{w_2} = \bar w_3\pdv{w_3}{w_2} = \bar w_3 w_1\\
  \bar w_1 &= \pdv{y}{w_1} = \bar w_4\pdv{w_4}{w_1} + \bar w_3 \pdv{w_3}{w_1}= \bar w_4 \cos(w_1) + \bar w_3w_2
\end{align*}
Notice how reverse mode required only one pass though the algorithm in order to
find the derivatives of both independent variables, compared to two passes for
forward mode.

Keen readers might also recognize backpropagation as a special case of reverse mode \gls{ad}.

\section{Forward Mode vs. Reverse Mode}

While the two described approaches both obtain the same result, one is generally
preferable to the other depending on the structure of the computation. Let the
computation be a function $f: \mathbb{R}^{m}\to\mathbb{R}^n$. Forward mode
requires $m$ passes (one per input), while reverse mode requires $n$ passes (one
per output). So in general, if $m \gg n$ we prefer reverse mode and when $m\ll
n$ we prefer forward.

Because of this, reverse mode automatic differentiation is by far the most
prevailing choice in \gls{ml} frameworks. This is because models almost always have
more inputs than outputs.

\section{Automatic Differentiation for \gls{vmc}}

\gls{ad} is the workhorse of modern \gls{ml} which enables rapid development of complex
models by removing the need to manually implement derivatives. As \gls{vmc} also needs
quite a lot of derivatives it is natural to wonder if we can have \gls{ad} tools work
to our advantage also in this area.

\subsection{First Order Derivatives}

To see how this could work, we should revisit the list of required operations
for wave functions in \cref{sec:arbitrary-models-as-trial-wave-functions}. In
the list we see the need for first order derivatives w.r.t. both the inputs to
the wave function and its parameters. Both of these are straightforward to
obtain in an efficient manner using reverse mode \gls{ad}. To illustrate why, we can
consider the \emph{Jacobian} matrix, $\mat J$, which for a function
$f:\mathbb{R}^m\to\mathbb{R}^n$ looks like:

\begin{align}
  \mat J &=
  \begin{pmatrix}
    \pdv{f_1}{x_1} & \pdv{f_1}{x_2} & \dots & \pdv{f_1}{x_m}\\
    \pdv{f_2}{x_2} & \pdv{f_2}{x_2} & \dots & \pdv{f_2}{x_m}\\
    \vdots & \vdots & \ddots & \vdots\\
    \pdv{f_n}{x_1} & \pdv{f_n}{x_2} & \dots & \pdv{f_n}{x_m}\\
  \end{pmatrix}
\end{align}
A forward mode sweep calculates a column of $\mat J$,
while a reverse mode sweep calculates a row of $\mat
J$~\cite{auto-diff-Berland}. In our case, $f$ is a wave function which has a
scalar output, and $\mat J$ is really a row vector. Reverse mode is clearly
superior in this case, requiring only a single pass.\\

\subsection{Higher Orders}

Returning to the list of operations, we still have the ominous Laplace
operator, $\laplacian$. To see why this will be harder, we can consider the
\emph{Hessian} matrix, $\mat H$:

\begin{align}
  \mat H &=
  \begin{pmatrix}
    \pdv[2]{f}{x_1} & \pdv{f}{x_1}{x_2} & \dots & \pdv{f}{x_1}{x_m}\\
    \pdv{f}{x_2}{x_1} & \pdv[2]{f}{x_2} & \dots & \pdv{f}{x_2}{x_m}\\
    \vdots & \vdots & \ddots & \vdots\\
    \pdv{f}{x_M}{x_1} & \pdv{f}{x_M}{x_2} & \dots & \pdv[2]{f}{x_m}\\
  \end{pmatrix}
\end{align}
We can compute these second order derivatives by applying \gls{ad} twice. First we do
a reverse sweep and obtain the Jacobian (the row vector). The act of computing
the Jacobian is it self a function, $f: \mathbb{R}^m\to\mathbb{R}^m$, and as
such we can apply \gls{ad} to it an obtain the elements of $\mat H$.\\

Computing the required Laplace operator equates to computing the trace (sum of
diagonal) of the Hessian matrix. Remember that a forward/reverse sweep
calculates a column/row of the matrix. That means that in order to get the
diagonal elements, we actually need $m$ sweeps, regardless of what direction we
go. Sadly, the complexity of this operation is so large that we have found it
unfeasible to use for even a moderate number of inputs.


\section{Why Automatic Differentiation Failed}

Because \gls{ad} is so central to most \gls{ml} frameworks, like TensorFlow and PyTorch, a
great deal of effort was put in to attempt to make a viable \gls{vmc} implementation
within these. That would give out-of-the-box functionality for neural networks,
optimization strategies, \acrshort{gpu} acceleration etc. Sadly, the inefficiency of the
calculation of the Laplacian operator put a stop to such a marriage. The time
spent by \gls{ad} to compute $\laplacian$ were orders of magnitude worse than the
manually coded analytically expressions, and scaled significantly worse with
increasing number of particles. This severe deficiency of \gls{ad} resulted
in the unfortunate need to depart from the existing \gls{ml} frameworks and
implement everything manually (i.e. analytically expressions for derivatives).
This decision was not made lightly, and only after a painstaking effort to
somehow attempt to defy the above mathematical argument. Unfortunately,
mathematics was right this time as well.

We sincerely hope that \gls{ad} can somehow be efficiently incorporated in future
works. Research into how \gls{ad} can be efficiently applied to higher order
derivatives is ongoing, see for instance~\textcite{wang2017}. If nothing else,
we could potentially use \gls{ad} for Hamiltonians which does not include the Laplacian
operator.

\glsreset{gpu}


\end{document}
