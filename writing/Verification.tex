\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Verification}
\label{chp:verfication}

This chapter is dedicated to presenting some of the tests that we have done to
verify that the software we have developed is indeed functioning as advertised.

\section{Setup}

We focus our tests on the idealized harmonic oscillator system in $D = 3$ dimensions
with $N$ non-interacting particles, i.e.\ the Hamiltonian given by
\cref{eq:ho-no-interaction-hamiltonian}:

\begin{align}
    \hat H_0 &= \sum_{i=1}^N\qty(-\frac{1}{2}\laplacian_i +
    \frac{1}{2}r_i^2),
\end{align}
where we have $\hbar = m = \omega = 1$, and
$r_i^2 = x_i^2 + y_i^2+z_i^2$. This has the ground state given by
\cref{eq:Phi-non-inter}, generalized to $N$ particles in three dimension and
omitting normalization constants:

\begin{align}
        \Phi(\mat X) &= \exp[-\frac{1}{2}\sum_{i=1}^N r_i^2],
\end{align}
where as before we have defined $\mat X$ as

\begin{align}
  \mat X &\defeq \mqty(\vx_1\\\vx_2\\\vdots\\\vx_N) \defeq \mqty(x_1&y_1&z_1\\x_2&y_2&z_2\\\vdots&\vdots&\vdots\\x_N&y_N&z_N)
\end{align}

For the trail wave function we shall use two different ones. First, the simple
Gaussian form of the ground state it self:

\begin{align}
  \psi_G(\mat X) = \exp(-\alpha\sum_{i = 1}^N r_i^2),
\end{align}
with $\alpha$ the only variational parameter. Learning the ideal parameters
should be trivial in this case, and we should expect perfect results.

Second, we use an ansatz resulting from a Gaussian-Binary Restricted Boltzmann
Machine (RBM), given in \cref{eq:rbm-def}:


\begin{align}
  \psi_{RBM}(\vX) &=
        \exp[-\sum_i^{M} \frac{\qty(X_i-a_i)^2}{2\sigma^2}]
        \prod_j^H \qty(1 + \exp[b_j+\sum_i^M \frac{X_iW_{ij}}{\sigma^2}]),
\end{align}
where $M = P\cdot D$ is the number of degrees of freedom and $H$ is
the number of hidden nodes (set to 4 through this section). Note also that $X_i$
in the above refers to the $i$'th degree of freedom, counting through $\mat X$
in row major order. The variational parameters are $\vb a, \vb b$ and $\mat W$,
and we hold $\sigma^2=1$ constant in this case.

We use this wave function simply to make learning the true ground state slightly
more challenging than proposing a simple Gaussian straight away. Note that
setting $\vb{a}, \vb b$ and $\mat W$ all to zero yields the correct ground state in this particular case.


\section{Optimization}

\subsection{Integration Test}
The simplest complete test is to initialize $\psi_G$ with a non-optimal
parameter, e.g.\ $\alpha=0.3$, and attempt to learn the optimal value.
Optimizing this is trivially accomplished, and
\cref{fig:verify-gaussian-simplest} shows a training progression using $N=10$
particles. The hyperparameters have here been artificially tuned to avoid
immediate convergence to $\alpha =0.5$ so as to better illustrate
the process.

If we allow the training to progress a little further (or use more optimal
hyperparameters), it eventually finds
$\alpha = 0.5$ to within machine precision and we get
$\flatfrac{\expval{E_L}}{N} = \flatfrac{D}{2}$ with exactly zero variance. While
this test is not the most challenging, it is nevertheless a useful check.

\begin{figure}[h]
  \centering
    \resizebox{\linewidth}{!}{%
      \input{scripts/verify-simple-gaussian.py.tex}
    }
  \caption{\label{fig:verify-gaussian-simplest}Example training progression using $\psi_G$ as trial wave function.
    Hyperparameters have been tuned so that we can see what happens, as opposed
    to immediate convergence to the perfect result.}
\end{figure}

\subsection{Learning Rate Dependency}

Successful training is highly dependent on using the correct hyperparameters.
Among the most important are the ones controlling the optimization scheme, such
as the learning rate in Stochastic Gradient Decent (SGD). The following plots
aim to illustrate this dependency, while also serving as a check that the
implemented optimization schemes work as expected.

Importantly, these results are not meant to infer that some schemes or learning
rates are superior to others. Which scheme works best for a given learning
problem will depend on a number of factors, such as the:

\begin{itemize}
  \item Magnitude of the gradients
  \item Number of parameters
  \item Variance in gradient estimates
  \item etc.
\end{itemize}


\subsubsection{Simple Problem - $\psi_G$}

\cref{fig:verify-lr-gaussian} shows the absolute error of $\psi_G$ during
training with several different schemes. For standard SGD we see that a learning
rate around $\eta = 0.1$ performs best among these results, and SGD with $\eta
=0.01$ showing the slowest convergence. Naturally, values of $0.1>\eta>0.01$
perform somewhere between the two.

From these results alone it might seem like
larger learning rates always perform better. To an extent this is true, as it
allows for more rapid learning. However, setting $\eta$ too high can lead to
divergence and unpredictable behavior. In less extreme cases, it can also keep
us from converging properly onto the correct parameters by oscillating around
the ideal values.

We have also included some runs using ADAM. Here we have more parameters to play
with, but only a few are shown here. In this trivial learning example it is hard
to beat properly tuned SGD, but we see how ADAM is able to follow closely. In
this particular case, we saw large improvements by reducing $\beta_1$, which
effectively reduces the momentum applied. An important fact is also that ADAM is
designed to be used with many parameters, with individual learning rates per
parameter. This enhancement does not show itself in this single-parameters example.


\begin{figure}[h]
  \centering
    \resizebox{\linewidth}{!}{%
      \input{scripts/verify-learning-rate-gaussian.py.tex}
    }
  \caption{\label{fig:verify-lr-gaussian}Example training progression using
    $\psi_G$ as trial wave function with different optimization schemes.}
\end{figure}

\subsubsection{More Complex Problem - $\psi_{RBM}$}

We run the same test as above, now with $\psi_{RBM}$ as the trial wave function
instead. We do this to illustrate a common pitfall of gradient based
optimization - local minima. \cref{fig:verify-lr-rbm} shows three runs
plateauing around an error $\sim 10^{-6}\si{\au}$. Interestingly, the worst
result is obtained with the middle most value of $\eta$. This shows the random
nature of SGD, in that it is quite unpredictable when and where we might get
stuck due to local minimum. Repeating the same experiment with different random
seeds does not consistently reproduce this result.

Similarly, we see that one of the ADAM runs did in fact stumble on to a
different, better local minimum. While this is also subject to randomness, we
find that ADAM tends to be at least as good as SGD whenever we have more than
one parameter to learn. This is to be expected, as ADAM can account for
different scales and variability in the components of the parameter gradient.

\begin{figure}[h]
  \centering
    \resizebox{\linewidth}{!}{%
      \input{scripts/verify-learning-rate-rbm.py.tex}
    }
  \caption{\label{fig:verify-lr-rbm}Example training progression using
    $\psi_{RBM}$ as trial wave function with different optimization schemes. We
    see evidence of learning getting stuck in local minima.}
\end{figure}


\end{document}
