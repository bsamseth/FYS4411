\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Quantum Dots}
\label{chp:quantum-dots}

We now present results for all methods discussed, applied on the example system
of Quantum Dots (QD) from \cref{sec:quantum-dots-theory}. We present first some
benchmarks using a typical Slater-Jastrow wave function form, followed by
introduction of various neural network-based wave function.

\section{Benchmark}

As we shall restrict this analysis to only two interacting particles in the QD,
$\vX = (\vx_1\ \vx_2)$, our benchmark wave function is rather simple. We build
it up using the product of single particle ground states (Gaussian), multiplied
by a Pade-Jastrow correlation term\footnote{We drop the constant factor
  from~\cref{eq:Phi-non-inter} because we have not normalized the wave function.}:

\begin{align}
  \label{eq:qd-pade-jastrow-anzats}
  \psi_{PJ}(\vX) &= \Phi(\vX) \,J_P(\vX)\\
  &= \exp(-\alpha_G\sum_{i=1}^N \norm{\vx_i}^2 + \sum_{i < j} \frac{\alpha_{PJ}
    r_{ij}}{1 + \beta_{PJ} r_{ij}}),
\end{align}
where $\alpha_{PJ} = 1$ is fixed by the cusp conditions, and $\alpha_G$ and $\beta_{PJ}$
are the two only variational parameters.

\subsubsection{Optimizing}

We have run a simple optimization of the above wave function, using initial
values of $\alpha_G = 0.5$ and $\beta_{PJ} = 1$. We used importance sampling and the
ADAM optimization scheme. We used $\num{2000}$ optimization steps, each with
$\num{5000}$ MC cycles. The values are somewhat arbitrarily, and we get
similar results for several other choices.

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-benchmark.py.tex}
    }
    \caption{\label{fig:QD-benchmark-pade-jastrow-training}Left: Performance of the
      wave function in \cref{eq:qd-pade-jastrow-anzats} as a function of
      training steps. Right: Progression of variational parameters as a function
      of training steps. The source code for this graphic can be found~\cite[TODO: Add
    path]{MS-thesis-repository}, and \LaTeX{} output generated
    by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

\begin{table}[h]
  \centering
  \input{scripts/QD-benchmark.py.table.tex}
  \caption{Energy benchmark using Pade-Jastrow wave function, using $2^{22}$
    Monte Carlo samples and errors estimated by an automated blocking algorithm
    by~\textcite{Jonsson-2018}. See \cref{fig:QD-benchmark-pade-jastrow-training}
    for source code reference.}
  \label{tab:pade-jastrow-benchmark-energy}
\end{table}

\cref{fig:QD-benchmark-pade-jastrow-training} shows the optimization as function
of percentage of training completed. We can observe that the optimizations
quickly settles down to a set of optimal values, where it only oscillates
slightly back and forth. \cref{tab:pade-jastrow-benchmark-energy} shows
statistics for the energy obtained with the final state. Comparing to the
analytical result of $\SI{3}{\au}$ these results are in very good agreement. For
reference we have also given the results obtained without the Pade-Jastrow term,
i.e. the non-interacting ground state.

\section{Restricted Boltzmann Machine}

\begin{figure}[h]
  \centering
  \input{scripts/rbm-diagram.tex}
  \caption{Example diagram of a Guassian-Binary Restricted Boltzmann Machine, showed with four visible nodes
    and three hidden nodes. The red values are the parameters, and consist of
    visible layer bias, $\vb a$, hidden layer bias, $\vb b$ and connection
    weights $\mat W$.}
  \label{fig:rbm-diagram-example}
\end{figure}


The first ML inspired model we have applied is a Restricted Boltzmann Machine (RBM).
This type of model has seen a significant rise in usage
since~\textcite{Carleo602} demonstrated the RBMs excellent capability to
represent the wave function for some selected Hamiltonians. However, all the
Hamiltonians for which they showed successful results were had a discrete
configuration space. The current system is continuous as the particles can have
any real valued coordinates, and so the type of RBM must change as well.

While several possible choices exist, we have used a Gaussian-Binary RBM. An
example diagram is given in~\cref{fig:rbm-diagram-example}. A full
introduction to RBMs, including the details needed for VMC can be found
in\cite{Flugsrud-2018}. For our purposes it suffices to say that the resulting
wave function looks as follows:

\begin{align}
  \label{eq:rbm-def}
  \psi_{RBM}(\vX) &=
        e^{-\sum_i^{M} \frac{\qty(X_i-a_i)^2}{2\sigma^2}}
        \prod_j^N \qty(1 + e^{b_j+\sum_i^M \frac{X_iW_{ij}}{\sigma^2}}),
\end{align}
where $M = P\cdot D$ is the number of degrees of freedom and $N$ is the number
of hidden nodes. Note also that $X_i$ in the above refers to the $i$'th degree
of freedom, counting through $\mat X$ in row major order. The parameters are
$\vb a, \vb b$ and $\mat W$, and $\sigma^2=1$ is held constant in this case.

If we set $\vb a = \vb 0$ we can recognize the first factor of~\cref{eq:rbm-def}
as the non-interacting ground state. That way we can consider the second factor
the Jastrow factor introduced by the RBM structure. It has a rather
unconventional form, as it is not a pure exponential.


\subsubsection{Optimizing}

We have run several variations of optimization on the RBM, all leading to rather
similar results. The following was produced with normally distributed random
initial values for the parameters, running $\num{60000}$ optimization steps with
$\num{2000}$ MC cycles each. We have also once again used importance sampling
and ADAM. A new addition (not strictly necessary for similar results) is the use
of mild $L2$ regularization, which serves to drive parameters that do not
contribute towards zero.

\cref{fig:QD-rbm-training} shows the ground state energy as a function of
training steps, along with the progression of the various variational
parameters. While we see a clear improvement in the initial stages, the RBM
fails to converge as accurately as the benchmark. \cref{tab:rbm-energy-results}
shows the precise results of the final model. While slightly different results
are possible with different training settings, we have never observed the RBM
achieve energies below $\SI{3.07}{\au}$, which is an error of about two orders
of magnitude larger than the benchmark.



% \begin{figure}[h]
%    \centering
%     \resizebox{\linewidth}{!}{%
%         \input{scripts/QD-rbm.py.tex}
%     }
%     \caption{\label{fig:QD-rbm-training}Left: Performance of the
%       wave function in ?? as a function of
%       training steps. Right: Progression of variational parameters as a function
%       of training steps. The source code for this graphic can be found~\cite[TODO: Add
%     path]{MS-thesis-repository}, and \LaTeX{} output generated
%     by~\cite{nico_schlomer_2018_1173090}.}
% \end{figure}

% \begin{table}[h]
%   \centering
%   \input{scripts/QD-rbm.py.table.tex}
%   \caption{Energy using RBM wave function, using $2^{22}$
%     Monte Carlo samples and errors estimated by an automated blocking algorithm
%     by~\textcite{Jonsson-2018}. See \cref{fig:QD-benchmark-pade-jastrow-training}
%     for source code reference.}
%   \label{tab:rbm-energy-results}
% \end{table}

% \begin{figure}[h]
%    \centering
%     \resizebox{0.7\linewidth}{!}{%
%         \input{scripts/QD-rbm.py.symmetry.tex}
%     }
%     \caption{\label{fig:QD-rbm-symmetry}Permutation symmetry of the RBM wave
% function in ?? as a function of training steps. The source code for this graphic
% can be found~\cite[TODO: Add path]{MS-thesis-repository}, and \LaTeX{} output
% generated by~\cite{nico_schlomer_2018_1173090}.}
% \end{figure}

An interesting consideration that arises from this form of wave function model
is that it has no guarantee of satisfying the required permutation symmetry.
This is an attribute of most neural network based models. Because we know the
true ground state must have the correct symmetry, we would hope that the RBM is
able to realize that a symmetric form is best. To this end we have defined a
metric $S(\psi)$, which has the property of being equal to 1 for symmetric
functions and $0$ for anti-symmetric ones. See \cref{app:symmetry-metric} for
its definition and details. The left plot in \cref{fig:QD-rbm-symmetry} shows a
plot of the symmetry metric of $\psi_{RBM}$ during training. Luckily we see
that the RBM, which starts completely non-symmetric, tends rapidly towards being
fully symmetric. Still it is not purely symmetric, and we see slight
oscillations around the maximum value.

The right plot in \cref{fig:QD-rbm-symmetry} shows a peculiar pattern emerging
from the weights $\mat W$ of the final model. Several weights are
inconsequential, while the remaining come in pairs of equal value. Additionally,
although not obvious from the plot, there are only two unique absolute values,
i.e. $\pm v_1$ and $\pm v_2$. Increasing the number of hidden nodes results in
the same two strips, with the extra elements similarly zeroing out. 

In attempts to properly fix the symmetry of $\psi_{RBM}$, we have made two meaningful
attempts. The first was setting $\vb a =a \vb 1, \vb b = b\vb 1$, i.e. removing
different biases for different degrees of freedom, and reducing the weights to
$\mat W\in\mathbb{R}^{D\times N}$. The motivation was that this would ensure all
degrees of freedom was treated in a symmetric manner. While this ensured
$S(\psi_{RBM})=1$, the resulting wave function proved unable to learn anything
useful. It could still learn the ground state if the particles where
non-interacting (i.e. the pure Gaussian), but failed remarkably in the full
system.

More successfully we imposed symmetry by sorting the inputs prior to feeding
them through $\psi_{RBM}$. While this was able to learn something, it stalled
out at $>\SI{3.1}{\au}$. In general it is our experience that imposing the
symmetry from the beginning of the training stops the RBM in getting anywhere.
This appears to be a manifestation of the classical problem in learning of balancing
exploitation and exploration, with this case suffering from to little
exploration.

The only truly successful way we found was to train the RBM as before, and then
apply sorted inputs on it once it had been fully trained. This lead to only a
marginal increase in energy.

\section{Neural Network}

\end{document}
