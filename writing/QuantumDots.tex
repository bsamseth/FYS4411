\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Quantum Dots}
\label{chp:quantum-dots}

We now present results for all methods discussed, applied on the example system
of Quantum Dots (QD) from \cref{sec:quantum-dots-theory}. We present first some
benchmarks using a typical Slater-Jastrow wave function form, followed by
introduction of neural network-based wave functions.

\section{Benchmark}

As we shall restrict this analysis to only two interacting particles in the QD,
$\vX = (\vx_1\ \vx_2)$, our benchmark wave function is simple. We build
it up using the product of single particle ground states (Gaussian), multiplied
by a Pade-Jastrow correlation term:\footnote{We drop the constant factor from~\cref{eq:Phi-non-inter} because we have not normalized the wave function.}

\begin{align}
  \label{eq:qd-pade-jastrow-anzats}
  \psi_{PJ}(\vX) &= \Phi(\vX) \,J_P(\vX)\\
  &= \exp(-\alpha_G\sum_{i=1}^N \norm{\vx_i}^2 + \sum_{i < j} \frac{\alpha_{PJ}
    r_{ij}}{1 + \beta_{PJ} r_{ij}}),
\end{align}
where cusp conditions fix $\alpha_{PJ} = 1$, and $\alpha_G$ and $\beta_{PJ}$
are the only two variational parameters.

\subsubsection{Optimizing}

We have run a simple optimization of the above wave function, using initial
values of $\alpha_G = 0.5$ and $\beta_{PJ} = 1$. We used importance sampling and the
ADAM optimization scheme. We used $\num{2000}$ optimization steps, each with
$\num{5000}$ MC cycles. The values are somewhat arbitrarily, and we get
similar results for other choices.

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-benchmark.py.tex}
    }
    \caption{\label{fig:QD-benchmark-pade-jastrow-training}Left: Performance of the
      wave function in \cref{eq:qd-pade-jastrow-anzats} as a function of
      training steps. Right: Progression of variational parameters as a function
      of training steps. The source code for this graphic can be found~\cite[TODO: Add
    path]{MS-thesis-repository}, and \LaTeX{} output generated
    by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

\begin{table}[h]
  \centering
  \input{scripts/QD-benchmark.py.table.tex}
  \caption{Energy benchmark using Pade-Jastrow wave function, using $2^{22}$
    Monte Carlo samples and errors estimated by an automated blocking algorithm
    by~\textcite{Jonsson-2018}. See \cref{fig:QD-benchmark-pade-jastrow-training}
    for source code reference.}
  \label{tab:pade-jastrow-benchmark-energy}
\end{table}

\cref{fig:QD-benchmark-pade-jastrow-training} shows the optimization as function
of percentage of training completed. We can observe that the optimizations
quickly settles down to a set of optimal values, where it only oscillates
slightly back and forth. \cref{tab:pade-jastrow-benchmark-energy} shows
statistics for the energy obtained with the final state. Comparing to the
analytical result of $\SI{3}{\au}$ these results are in good agreement. For
reference we have also given the results obtained without the Pade-Jastrow term,
i.e.\ the non-interacting ground state.

\section{Restricted Boltzmann Machine}

\begin{figure}[h]
  \centering
  \input{illustrations/rbm-diagram.tex}
  \caption{Example diagram of a Guassian-Binary Restricted Boltzmann Machine, showed with four visible nodes
    and three hidden nodes. The red values are the parameters, and consist of
    visible layer bias, $\vb a$, hidden layer bias, $\vb b$ and connection
    weights $\mat W$.}
  \label{fig:rbm-diagram-example}
\end{figure}


The first ML inspired model we have applied is a Restricted Boltzmann Machine (RBM).
This type of model has seen a significant rise in usage
since~\textcite{Carleo602} demonstrated the RBMs capability to
represent the wave function for some selected Hamiltonians. All the
Hamiltonians for which they showed successful results, however, had discrete
configuration spaces. The current system is continuous as the particles can have
any real valued coordinates, and so the type of RBM must change as well.

While more than one choice exist, we have used a Gaussian-Binary RBM\@.
\cref{fig:rbm-diagram-example} shows and example diagram of the network
structure. \textcite{Flugsrud-2018} has a full introduction to RBMs, including
all details needed for VMC\@. For our purposes it suffices to say that the
resulting wave function looks as follows:

\begin{align}
  \label{eq:rbm-def}
  \psi_{RBM}(\vX) &=
        e^{-\sum_i^{M} \frac{\qty(X_i-a_i)^2}{2\sigma^2}}
        \prod_j^N \qty(1 + e^{b_j+\sum_i^M \frac{X_iW_{ij}}{\sigma^2}}),
\end{align}
where $M = P\cdot D$ is the number of degrees of freedom and $N$ is the number
of hidden nodes. Note also that $X_i$ in the above refers to the $i$'th degree
of freedom, counting through $\mat X$ in row major order. The parameters are
$\vb a, \vb b$ and $\mat W$, and we hold $\sigma^2=1$ constant in this case.

If we set $\vb a = \vb 0$ we can recognize the first factor of~\cref{eq:rbm-def}
as the non-interacting ground state. That way we can consider the second factor
the Jastrow factor introduced by the RBM structure. It has an unconventional
form, as it is not a pure exponential.


\subsubsection{Optimizing}

We produced the following with normally distributed random
initial values for the parameters, running $\num{60000}$ optimization steps with
$\num{2000}$ MC cycles each. We have also once again used importance sampling
and ADAM\@. A new addition (not strictly necessary for similar results) is the use
of mild $L2$ regularization, which serves to drive parameters that do not
contribute towards zero. The results are similar for different hyper-parameter
choices, and the above is simply one such example.

\cref{fig:QD-rbm-training} shows the ground state energy as a function of
training steps, along with the progression of the variational
parameters. While we see a clear improvement in the initial stages, the RBM
fails to converge as accurately as the benchmark. \cref{tab:rbm-energy-results}
shows the precise results of the final model. While slightly different results
are possible with different training settings, we have never observed the RBM
achieve energies below $\SI{3.07}{\au}$, which is an error of about two orders
of magnitude larger than the benchmark.



\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-rbm.py.tex}
    }
    \caption{\label{fig:QD-rbm-training}Left: Performance of the
      wave function in ?? as a function of
      training steps. Right: Progression of variational parameters as a function
      of training steps. The source code for this graphic can be found~\cite[TODO: Add
    path]{MS-thesis-repository}, and \LaTeX{} output generated
    by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

\begin{table}[h]
  \centering
  \input{scripts/QD-rbm.py.table.tex}
  \caption{Energy using the RBM wave function in~\cref{eq:rbm-def}, along with
the same wave function using input sorting to impose symmetry. Results obtained
from $2^{23}$ Monte Carlo samples and errors estimated by an automated blocking
algorithm by~\textcite{Jonsson-2018}. See
\cref{fig:QD-rbm-training} for source code reference.}
  \label{tab:rbm-energy-results}
\end{table}

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-rbm.py.symmetry.tex}
    }
    \caption{\label{fig:QD-rbm-symmetry}Left: Permutation symmetry of the RBM wave
function in~\cref{eq:rbm-def} as a function of training steps. Right: Color map
of the weight matrix of the RBM after training. The source code for this graphic
can be found~\cite[TODO: Add path]{MS-thesis-repository}, and \LaTeX{} output
generated by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

An important consideration that arises from this form of wave function model
is that it has no guarantee of satisfying the required permutation symmetry.
This is an attribute of most neural network based models. Because we know the
true ground state must have the correct symmetry, we would hope that the RBM is
able to realize that a symmetric form is best. To this end we have defined a
metric $S(\psi)$, which has the property of being equal to 1 for symmetric
functions and $0$ for anti-symmetric ones. See \cref{app:symmetry-metric} for
its definition and details. The left plot in \cref{fig:QD-rbm-symmetry} shows a
plot of the symmetry metric of $\psi_{RBM}$ during training. Luckily we see
that the RBM, initially starting fully non-symmetric, tends rapidly towards
1. Still it is not purely symmetric, and we see significant
oscillations around the maximum value.

The right plot in \cref{fig:QD-rbm-symmetry} shows a peculiar pattern emerging
from the weights $\mat W$ of the final model. Half of the weights appear
inconsequential, while the remaining come in pairs of equal value. Additionally,
although not obvious from the plot, there are only two unique absolute values,
i.e.\ $\pm v_1$ and $\pm v_2$. Increasing the number of hidden nodes results in
the same two strips, with the extra elements similarly zeroing out.

In attempts to properly fix the symmetry of $\psi_{RBM}$, we have made two significant
attempts. The first was reducing the biases $\vb a$ to be $\vb a\in\mathbb{R}^D$
and reducing the weights to
$\mat W\in\mathbb{R}^{D\times N}$, i.e.\ have parameters per dimension and not
per degree of freedom. While this ensured
$S(\psi_{RBM})=1$, and the RBM could still learn the ground state if the particles where
non-interacting (i.e.\ the pure Gaussian), it failed in the full system.

More successfully we imposed symmetry by sorting the inputs prior to feeding
them through $\psi_{RBM}$. While this was able to learn something, it stalled
out at $>\SI{3.1}{\au}$. In general it is our experience that imposing the
symmetry from the beginning of the training stops the RBM in getting anywhere.
This is a manifestation of the classical problem in learning of balancing
exploitation and exploration, with this case suffering from a lack of exploration.

The only truly successful way we found was to train the RBM as before, and then
apply sorted inputs on it once it had been fully trained. The results of this is
also shown in \cref{tab:rbm-energy-results} by $\psi_{SRBM}$. This lead to only a
marginal, not statistically significant increase in energy, and as such was a
successful way to impose the symmetry.

\section{Neural Network}

Finally we turn our attention to the new contribution of this thesis - applying
general neural networks as wave functions.

Firstly, we have not been able to simply replace the entire wave function with a
neural network from the start. In order for VMC optimization to work we need a
stable enough starting point to avoid immediate divergence. The better approach
is to bootstrap the network with an existing wave function as a starting point.
In our case we use the benchmark wave function $\psi_{PJ}$. There are two
options for how to do this:

\begin{enumerate}
\item Pre-train the network to emulate $\psi_{PJ}$ before starting VMC
  optimization.
\item Use the network as an extra correlation factor multiplied with $\psi_{PJ}$.
\end{enumerate}
While there is a certain appeal to the first alternative, in the
sense that we would end up with a pure neural network wave function, the
downside is that pre-training will not be as accurate, as well as taking
some time. Small inaccuracies could also lead to instabilities from non-satisfied
cusp conditions/symmetry requirements.

Because of this, we opted to simply treat the network as a correction factor,
aimed to fix the small discrepancy between the benchmark and the true ground
state. This approach allows us to bootstrap learning and start where existing
techniques have already taken us, and improve from there. It is also trivial to
implement in QFLOW, because of built in support for composing wave functions by
multiplication.

The proposed wave function is:

\begin{align}\label{eq:pade-dnn}
  \psi_{DNN}(\vX) &= \psi_{PJ}(\vX)\, f(\vX),
\end{align}
where $f$ represents some arbitrary neural network.


\subsection{Network Architecture}

We consider here only simple Feed Forward Neural Networks (FFNN) and leave other
species of NNs for future research.

The size of the system determines the number of inputs.
The output layer is also essentially forced as we want a
scalar output from our network.\footnote{For applications that require complex
  wave functions, this can be easily modeled by having two outputs and
  interpret them as the real and imaginary component.} In addition, all
existing research implies that such correlation factors should be exponential.

For the hidden layers we have much more of a choice. By simple trail and error
we found $\tanh$ to be the most suitable activation function, and we have used this
in all cases. The number of nodes needs to be sufficiently large compared to the
number of inputs. This is to allow the network to learn a large amount of
different correlations, and not force it to settle right away.
The specific choices are again empirically motivated, and stems
from observing worse results with much smaller values and diminishing returns
for larger choices.

The later hidden layers should decrease in width as we approach the output, with
the idea that the network should gradually attempt to compact what it learns
into a smaller space. We have used powers of two for no particular reason other
than that it allows for even divisions.

While a large family of architectures are likely to perform well in this case,
we have found the following setup settled on the following setup:

\begin{center}
  \begin{tabular}{lcc}
    \toprule
    \addlinespace
    Layer & Nodes & Activation\\
    \addlinespace
    \midrule
    \addlinespace
    \addlinespace
    Input & 4 & ---\\
    Hidden 1& 32 & $\tanh$\\
    Hidden 2& 16 & $\tanh$\\
    Output & 1 & $\exp$\\
    \addlinespace
    \addlinespace
    \bottomrule
  \end{tabular}
\end{center}

\subsection{Optimization}

We produced the following results by optimizing $\psi_{DNN}$ using $\num{25000}$
iterations of $1000$ MC cycles each. We have not used any regularization in this
case, and as usual we use importance sampling and the ADAM optimizer.
Again, this is an example of hyper-parameters, and we can achieve good results
with a range of other choices.

\cref{fig:QD-pade-dnn-training} shows a graph of the absolute error in energy
during the course of training, both for $\psi_{DNN}$ and the benchmark
$\psi_{PJ}$. The two curves closely follow each other in early stages, likely
because the Pade-Jastrow factor contributes most of the improvements at this point. The
benchmark eventually plateaus after a short amount of time, while the
network however continues to improve beyond this point. We achieve about an
order of magnitude better accuracy, with a corresponding reduction in the
variance.

\cref{tab:pade-dnn-energy-results} shows the final energies produced from
$\psi_{DNN}$ after training. Also shown is $\psi_{PJ}$ after the same amount of
training. The results solidify the graphical impression
of~\cref{fig:QD-pade-dnn-training}, showing a clear improvement. These results
approach the accuracy of Diffusion Monte Carlo (DMC), a technique that can obtain
exact numerical results. \textcite{Pedersen-2011} lists $\SI{3.00000(1)}{\au}$
as the DMC result, which has an overlapping confidence interval with our results.

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-pade-dnn.py.tex}
    }
    \caption{\label{fig:QD-pade-dnn-training}Left: Performance of the wave
function in~\cref{eq:pade-dnn} as a function of training steps. Right:
Progression of variational parameters as a function of training steps. We only
show a small selection of the total number of parameters. The source code for this graphic
can be found~\cite[TODO: Add path]{MS-thesis-repository}, and \LaTeX{} output
generated by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

\begin{table}[h]
  \centering
  \input{scripts/QD-pade-dnn.py.table.tex}
  \caption{Energy using the neural network wave function in~\cref{eq:pade-dnn}, along with
the benchmark wave function after the same amount of optimization. Results obtained
from $2^{22}$ Monte Carlo samples and errors estimated by an automated blocking
algorithm by~\textcite{Jonsson-2018}. See
\cref{fig:QD-pade-dnn-training} for source code reference.}
  \label{tab:pade-dnn-energy-results}
\end{table}

% \begin{figure}[h]
%    \centering
%     \resizebox{0.7\linewidth}{!}{%
%         \input{scripts/QD-pade-dnn.py.symmetry.tex}
%     }
%     \caption{\label{fig:QD-pade-dnn-symmetry}Permutation symmetry of
% $\psi_{DNN}$ as a function of training steps. See
% \cref{fig:QD-pade-dnn-training} for source code reference.}
% \end{figure}

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-pade-dnn.py.weights.tex}
    }
    \caption{\label{fig:QD-rbm-symmetry}Color map representation of the weights
      and biases in the network of $\psi_{DNN}$. See
\cref{fig:QD-pade-dnn-training} for source code reference.}
\end{figure}


\subsection{Complexity}

While the neural network has demonstrated its capacity to learn an improved
correlation function, this does come at a considerable computational cost.

The benchmark function has complexity $\mathcal{O}(N^2)$ for $N$ particles,
rooted in the double sum in~\cref{eq:qd-pade-jastrow-anzats}. Thanks to
analytic expressions for the derivatives, the otherwise heavy Laplace operator
is also calculated in $\mathcal{O}(N^2)$ time.

The complexity of the neural network is a little harder to pin down, because of
the inherent flexibility in how the network can be structured.

The neural network computes a series of matrix-vector multiplications, both during
evaluation and for its derivatives. The input layer needs to scale equal to $N$,
while the other layers could in principle be constant. This means we could say
that the network only scales linearly, $\mathcal{O}(N)$, with increasing number
of particles. Still, the \say{constant} part of the computation time is very
large, and this kind of complexity analysis is slightly unfair.

In practice we found that we needed the second layer to have more
nodes than we have inputs. We have not determined a strict relationship, but
either $\mathcal{O}(N)$ or $\mathcal{O}{N^2}$ seems reasonable for the second
layer. Assuming the network shrinks from there on, the dominant operation is a
matrix-vector operation with matrix dimensions of order $(N\times N)$ or $(N\times N^2)$,
which works out to a complexity of $\mathcal{O}(N^2)$ or $\mathcal{O}(N^3)$. In
this more realistic view, the networks are not improving upon the complexity of
the benchmark.\\

Importantly the networks also depend highly on the depth (number of layers) as
well as obviously the sizes of subsequent layers. We should also note that while
asymptotic cost might be equal to the benchmark, a more precise count of the
required operations would show a substantial disparity.

However, a constant cost difference can always be brought down with optimized code, more
and better hardware, introducing GPUs etc. And while computing power may no
longer increase exponentially each year, it continues to grow nevertheless.
Large scale use of networks for quantum mechanics might therefore be well
within reach.


\end{document}
