\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Quantum Dots}
\label{chp:quantum-dots}

We now present results for all methods discussed, applied on the example system
of Quantum Dots (QD) from \cref{sec:quantum-dots-theory}. We present first some
benchmarks using a typical Slater-Jastrow wave function form, followed by
introduction of neural network-based wave functions.

\section{Benchmark}

As we shall restrict this analysis to only two interacting particles in the QD,
$\vX = (\vx_1\ \vx_2)$, our benchmark wave function is simple. We build
it up using the product of single particle ground states (Gaussian), multiplied
by a Pade-Jastrow correlation term:\footnote{We drop the constant factor from~\cref{eq:Phi-non-inter} because we have not normalized the wave function.}

\begin{align}
  \label{eq:qd-pade-jastrow-anzats}
  \psi_{PJ}(\vX) &= \Phi(\vX) \,J_P(\vX)\\
  &= \exp(-\alpha_G\sum_{i=1}^N \norm{\vx_i}^2 + \sum_{i < j} \frac{\alpha_{PJ}
    r_{ij}}{1 + \beta_{PJ} r_{ij}}),
\end{align}
where cusp conditions fix $\alpha_{PJ} = 1$, and $\alpha_G$ and $\beta_{PJ}$
are the only two variational parameters.

\subsection{Optimizing}

We have run a simple optimization of the above wave function, using initial
values of $\alpha_G = 0.5$ and $\beta_{PJ} = 1$. We used importance sampling and the
ADAM optimization scheme. We used $\num{2000}$ optimization steps, each with
$\num{5000}$ MC cycles. The values are somewhat arbitrarily, and we get
similar results for other choices.

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-benchmark.py.tex}
    }
    \caption{\label{fig:QD-benchmark-pade-jastrow-training}Left: Performance of the
      wave function in \cref{eq:qd-pade-jastrow-anzats} as a function of
      training steps. Right: Progression of variational parameters as a function
      of training steps.\citesource{writing/scripts/QD-benchmark.py}}
\end{figure}

\begin{table}[h]
  \centering
  \caption{Energy benchmark using Pade-Jastrow wave function, using $2^{22}$
    Monte Carlo samples and errors estimated by an automated blocking algorithm
    by~\textcite{Jonsson-2018}. See \cref{fig:QD-benchmark-pade-jastrow-training}
    for source code reference.}
  \input{scripts/QD-benchmark.py.table.tex}
  \label{tab:pade-jastrow-benchmark-energy}
\end{table}

\cref{fig:QD-benchmark-pade-jastrow-training} shows the optimization as function
of percentage of training completed. We can observe that the optimizations
quickly settles down to a set of optimal values, where it only oscillates
slightly back and forth. \cref{tab:pade-jastrow-benchmark-energy} shows
statistics for the energy obtained with the final state. Comparing to the
analytical result of $\SI{3}{\au}$ these results are in good agreement. For
reference we have also given the results obtained without the Pade-Jastrow term,
i.e.\ the non-interacting ground state.

\subsection{Two-Body Density}

Given that we now consider two interacting particles, a natural quantity to
investigate is the two-body density. Recall \cref{fig:verify-twobody} which
showed the two-body density for the non-interacting system as a simple Gaussian.
Computing $\rho(\vx_1, \vx_2)$ using $\psi_{PJ}$ with the final parameters
obtained previously we get \cref{fig:QD-benchmark-pade-jastrow-density}.

This plot shows a very different behavior. The particles no longer prefer to
(simultaneously) stay close to the center of the potential because of the Coulomb repulsion, and
the density maxima is pushed outwards. Additionally, there is a small dent in
the density when $r_1\approx r_2$, not only in the center. This effect is made
clearer by looking at the squared density. These results are quite intuitive,
while showcasing the highly non-trivial behavior that the two particles exhibit.

If we integrate out one of the particle positions, i.e.\ compute the one-body
density we get \cref{fig:QD-benchmark-pade-jastrow-onebody}. This shows that the
most probable position for any single particle is still in the center, but the
density is much more spread out to larger $r$.

\begin{figure}[h]
   \centering
    \resizebox{0.7\linewidth}{!}{%
        \input{scripts/QD-benchmark-onebody.py.tex}
    }
    \caption{\label{fig:QD-benchmark-pade-jastrow-onebody}One-body density
      described by $\psi_{PJ}$ with optimized parameters. For reference, the
      exact OBD for the non-interacting system is indicated by the dotted line.\citesource{writing/scripts/QD-benchmark-onebody.py}}
\end{figure}

\begin{figure}[h]
   \centering
    \resizebox{0.7\linewidth}{!}{%
        \input{scripts/QD-benchmark-density.py.tex}
    }
    \resizebox{0.7\linewidth}{!}{%
        \input{scripts/QD-benchmark-density.py.squared.tex}
    }
    \caption{\label{fig:QD-benchmark-pade-jastrow-density}Top: Two-body density
      described by $\psi_{PJ}$ with optimized parameters. Bottom: The squared
      equivalent of the top figure, included to emphasize the small decrease in
      density along the diagonal $r_1=r_2$.\citesource{writing/scripts/QD-benchmark-density.py}}
\end{figure}

\clearpage


\section{Restricted Boltzmann Machine}

\begin{figure}[h]
  \centering
  \input{illustrations/rbm-diagram.tex}
  \caption{Example diagram of a Guassian-Binary Restricted Boltzmann Machine, showed with four visible nodes
    and three hidden nodes. The red values are the parameters, and consist of
    visible layer bias, $\vb a$, hidden layer bias, $\vb b$ and connection
    weights $\mat W$.\citesource{writing/illustrations/rbm-diagram.tex}}
  \label{fig:rbm-diagram-example}
\end{figure}


The first ML-inspired model we have applied is a Restricted Boltzmann Machine (RBM).
This type of model has seen a significant rise in usage
since~\textcite{Carleo602} demonstrated the RBMs capability to
represent the wave function for some selected Hamiltonians. All the
Hamiltonians for which they showed successful results, however, had discrete
configuration spaces. The current system is continuous as the particles can have
any real valued coordinates, and so the type of RBM must change as well.

While more than one choice exist, we have used a Gaussian-Binary RBM\@.
\cref{fig:rbm-diagram-example} shows and example diagram of the network
structure. \textcite{Flugsrud-2018} has a full introduction to RBMs, including
all details needed for VMC\@. For our purposes it suffices to say that the
resulting wave function looks as follows:

\begin{align}
  \label{eq:rbm-def}
  \psi_{RBM}(\vX) &=
        e^{-\sum_i^{M} \frac{\qty(X_i-a_i)^2}{2\sigma^2}}
        \prod_j^N \qty(1 + e^{b_j+\sum_i^M \frac{X_iW_{ij}}{\sigma^2}}),
\end{align}
where $M = P\cdot D$ is the number of degrees of freedom and $N$ is the number
of hidden nodes. Note also that $X_i$ in the above refers to the $i$'th degree
of freedom, counting through $\mat X$ in row major order. The parameters are
$\vb a, \vb b$ and $\mat W$, and we hold $\sigma^2=1$ constant in this case.

If we set $\vb a = \vb 0$ we can recognize the first factor of~\cref{eq:rbm-def}
as the non-interacting ground state. That way we can consider the second factor
the Jastrow factor introduced by the RBM structure. It has an unconventional
form, as it is not a pure exponential.


\subsection{Optimizing}

We produced the following with normally distributed random
initial values for the parameters, running $\num{60000}$ optimization steps with
$\num{2000}$ MC cycles each. We have also once again used importance sampling
and ADAM\@. A new addition (not strictly necessary for similar results) is the use
of mild $L2$ regularization, which serves to drive parameters that do not
contribute towards zero. The results are similar for different hyper-parameter
choices, and the above is simply one such example.

\cref{fig:QD-rbm-training} shows the ground state energy as a function of
training steps, along with the progression of the variational
parameters. While we see a clear improvement in the initial stages, the RBM
fails to converge as accurately as the benchmark. \cref{tab:rbm-energy-results}
shows the precise results of the final model. While slightly different results
are possible with different training settings, we have never observed the RBM
achieve energies below $\SI{3.07}{\au}$, which is an error of about two orders
of magnitude larger than the benchmark.



\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-rbm.py.tex}
    }
    \caption{\label{fig:QD-rbm-training}Left: Performance of the
      wave function in ?? as a function of
      training steps. Right: Progression of variational parameters as a function
      of training steps.\citesource{writing/scripts/QD-rbm.py}}
\end{figure}

\begin{table}[h]
  \centering
  \caption{Energy using the RBM wave function in~\cref{eq:rbm-def}, along with
the same wave function using input sorting to impose symmetry. Results obtained
from $2^{23}$ Monte Carlo samples and errors estimated by an automated blocking
algorithm by~\textcite{Jonsson-2018}. See
\cref{fig:QD-rbm-training} for source code reference.}
  \input{scripts/QD-rbm.py.table.tex}
  \label{tab:rbm-energy-results}
\end{table}

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-rbm.py.symmetry.tex}
    }
    \caption{\label{fig:QD-rbm-symmetry}Left: Permutation symmetry of the RBM wave
function in~\cref{eq:rbm-def} as a function of training steps. Right: Color map
of the weight matrix of the RBM after training. Weights have been scaled to
$[-1, 1]$ for simplicity. See \cref{fig:QD-rbm-training} for source code reference.}
\end{figure}

An important consideration that arises from this form of wave function model
is that it has no guarantee of satisfying the required permutation symmetry.
This is an attribute of most neural network based models. Because we know the
true ground state must have the correct symmetry, we would hope that the RBM is
able to realize that a symmetric form is best. To this end we have defined a
metric $S(\psi)$, which has the property of being equal to 1 for symmetric
functions and $0$ for anti-symmetric ones. See \cref{app:symmetry-metric} for
its definition and details. The left plot in \cref{fig:QD-rbm-symmetry} shows a
plot of the symmetry metric of $\psi_{RBM}$ during training. Luckily we see
that the RBM, initially starting fully non-symmetric, tends rapidly towards
1. Still it is not purely symmetric, and we see significant
oscillations around the maximum value.


In attempts to properly fix the symmetry of $\psi_{RBM}$, we have made two
significant attempts. The first was reducing the biases $\vb a$ to be $\vb
a\in\mathbb{R}^D$ and reducing the weights to $\mat W\in\mathbb{R}^{D\times N}$,
i.e.\ have parameters per dimension and not per degree of freedom. While this ensured
$S(\psi_{RBM})=1$, and the RBM could still learn the ground state if the
particles where non-interacting (i.e.\ the pure Gaussian), it failed in the full
system. The next section looks closer at the weights, and explains why this
reduced form failed.

More successfully we imposed symmetry by sorting the inputs prior to feeding
them through $\psi_{RBM}$. While this was able to learn something, it stalled
out at $>\SI{3.1}{\au}$. In general it is our experience that imposing the
symmetry from the beginning of the training stops the RBM in getting anywhere.
This is a manifestation of the classical problem in learning of balancing
exploitation and exploration, with this case suffering from a lack of exploration.

The only truly successful way we found was to train the RBM as before, and then
apply sorted inputs on it once it had been fully trained. The results of this is
also shown in \cref{tab:rbm-energy-results} by $\psi_{SRBM}$. This lead to only a
marginal, not statistically significant increase in energy, and as such was a
successful way to impose the symmetry.

\subsection{Interpreting the Network}

The right plot in \cref{fig:QD-rbm-symmetry} shows a peculiar pattern emerging
from the weights $\mat W$ of the final model. Half of the weights appear
inconsequential, while the remaining come in pairs of equal value. If we
increase the number of hidden nodes, the result is always two similar columns with the extra elements
zeroing out. Exactly \emph{which} columns get the non zero values seems random,
and varies based on the random initialization of the weights.

Although not immediately obvious from the plot, there are only two unique
absolute values, i.e.\ $\pm v_1$ and $\pm v_2$. That is, in this particular
case:

\begin{align}
  \mat W = \mqty(
                    W_{1,1} & W_{1,2} & W_{1,3} & W_{1,4}\\
                    W_{2,1} & W_{2,2} & W_{2,3} & W_{2,4}\\
                    W_{3,1} & W_{3,2} & W_{3,3} & W_{3,4}\\
                    W_{4,1} & W_{4,2} & W_{4,3} & W_{4,4}
                )
                                                  \simeq \mqty(
                                                   v_1 & 0 & 0 &  v_2\\
                                                   v_2 & 0 & 0 & -v_1\\
                                                  -v_1 & 0 & 0 & -v_2\\
                                                  -v_2 & 0 & 0 &  v_1
                                                  )
\end{align}
This relation is only approximately true, as the RBM has not learned set the
weights exactly equal to each other, but it is close enough to show clear
pattern.

This pattern explains why the attempt at imposing symmetry with a reduced form
of the RBM failed. This pattern is simply not possible to express with one
weight per degree of freedom.

Having now observed this phenomena, we can seek to understand why the RBM learns
this particular weight matrix. If we ignore the Gaussian factor of $\psi_{RBM}$
and substitute in the above matrix for $\mat W$, we get:

\begin{align}
  \begin{split}
  \psi_{RBM} \propto &\qty{1 + \exp[b_1 + \frac{1}{\sigma^2}\qty(v_1(x_1-x_2) + v_2(y_1-y_2))]}\\
                   \times &\qty{1 + \exp[b_4 + \frac{1}{\sigma^2}\qty(v_2(x_1-x_2) - v_1(y_1-y_2))]}
  \end{split}
\end{align}
If $\vb{a}=a\vb{1}$ and $\vb{b}=b\vb{1}$ (i.e.\ biases are all equal in value),
this leads to an RBM that is deceptively close to a fully permutation symmetric
function, broken only by the sign on $(y_1-y_2)$. The RBM has, without direct
interactions, learned that the differences between the spacial coordinates of
the two particles are important. In contrast to $\psi_{PJ}$, which has the
distance $r_{12}$ explicitly incorporated, the RBM has learned the closest
equivalent it has.

The fact that the RBM is able to pick up on the importance of relative
differences in position is encouraging for further work with less constrained
neural networks. In this case, the RBM could not learn to square the differences
(e.g.\ $(x_1 - x_2)^2$ instead of $(x_1-x_2)$), simply because it had no means
of expressing this.

\section{Neural Network}

Finally we turn our attention to the new contribution of this thesis - applying
general neural networks as wave functions.

Firstly, we have not been able to simply replace the entire wave function with a
neural network from the start. In order for VMC optimization to work we need a
stable enough starting point to avoid immediate divergence. The better approach
is to bootstrap the network with an existing wave function as a starting point.
In our case we use the benchmark wave function $\psi_{PJ}$. There are two
options for how to do this:

\begin{enumerate}
\item Pre-train the network to emulate $\psi_{PJ}$ before starting VMC
  optimization.
\item Use the network as an extra correlation factor multiplied with $\psi_{PJ}$.
\end{enumerate}
While there is a certain appeal to the first alternative, in the
sense that we would end up with a pure neural network wave function, the
downside is that pre-training will not be as accurate, as well as taking
some time. Small inaccuracies could also lead to instabilities from non-satisfied
cusp conditions/symmetry requirements.

Because of this, we opted to simply treat the network as a correction factor,
aimed to fix the small discrepancy between the benchmark and the true ground
state. This approach allows us to bootstrap learning and start where existing
techniques have already taken us, and improve from there. It is also trivial to
implement in QFLOW, because of built in support for composing wave functions by
multiplication.

The proposed wave function is:

\begin{align}\label{eq:pade-dnn}
  \psi_{DNN}(\vX) &= \psi_{PJ}(\vX)\, f(\vX),
\end{align}
where $f$ represents some arbitrary neural network.


\subsection{Network Architecture}

We consider here only simple Feed Forward Neural Networks (FFNN) and leave other
species of NNs for future research.

The size of the system determines the number of inputs.
The output layer is also essentially forced as we want a
scalar output from our network.\footnote{For applications that require complex
  wave functions, this can be easily modeled by having two outputs and
  interpret them as the real and imaginary component.} In addition, all
existing research implies that such correlation factors should be exponential.

For the hidden layers we have much more of a choice. By simple trail and error
we found $\tanh$ to be the most suitable activation function, and we have used this
in all cases. The number of nodes needs to be sufficiently large compared to the
number of inputs. This is to allow the network to learn a large amount of
different correlations, and not force it to settle right away.
The specific choices are again empirically motivated, and stems
from observing worse results with much smaller values and diminishing returns
for larger choices.

The later hidden layers should decrease in width as we approach the output, with
the idea that the network should gradually attempt to compact what it learns
into a smaller space. We have used powers of two for no particular reason other
than that it allows for even divisions.

While a large family of architectures are likely to perform well in this case,
we have found the following setup settled on the following setup:

\begin{center}
  \begin{tabular}{lcc}
    \toprule
    \addlinespace
    Layer & Nodes & Activation\\
    \addlinespace
    \midrule
    \addlinespace
    \addlinespace
    Input & 4 & ---\\
    Hidden 1& 32 & $\tanh$\\
    Hidden 2& 16 & $\tanh$\\
    Output & 1 & $\exp$\\
    \addlinespace
    \addlinespace
    \bottomrule
  \end{tabular}
\end{center}

Finally, we have also tested the effect of imposing permutation symmetry on the
network by sorting the inputs, as discussed in
\cref{sec:sorting-inputs-symmetry}. The network remains unchanged, and the
symmetric version of $\psi_{DNN}$ is labeled $\psi_{SDNN}$ in all of the following.


\subsection{Optimization}

We produced the following results by optimizing $\psi_{PJ}, \psi_{DNN}$ and
$\psi_{SDNN}$ with all the same hyper-parameters. We also initialized the
parameters of the two network wave functions equally for better comparison.
We used $\num{30000}$ iterations of $1000$ MC cycles each. We did not use any
regularization in this case, and as usual we used importance sampling and the
ADAM optimizer. Again, this is an example of hyper-parameters, and we can
achieve good results with a range of other choices.

\cref{fig:QD-pade-dnn-training} shows a graph of the absolute error in energy
during the course of training, for all three wave functions. The three curves
closely follow each other in the early stages, likely because the Pade-Jastrow
factor contributes most of the improvements at this point. Additionally, we have
initialized the networks with rather small weights, which result in the having a
weak effect in the early stages. This was done to ensure stability.

While $\psi_{PJ}$ plateaus after a short amount of time, the
networks continues to improve beyond this point. Interestingly, they do not
continue immediately. This indicates that the networks are temporarily stuck in
a local optimum, and it takes some time before they are able to find a better
strategy. In our tests, $\psi_{SDNN}$ tend to exit the plateau quicker that its
asymmetric counterpart, although the exact timing is random.

In the end both networks converge to similar levels at about halfway through the
training. We achieve an order of magnitude better accuracy compared to $\psi_{PJ}$, with a corresponding
reduction in the variance.\\

\cref{tab:pade-dnn-energy-results} shows the final energies produced from the
three wave functions after training. The results solidify the graphical impression
of~\cref{fig:QD-pade-dnn-training}, showing a clear improvement. These results
approach the accuracy of Diffusion Monte Carlo (DMC), a technique that can obtain
exact numerical results. \textcite{Pedersen-2011} lists $\SI{3.00000(1)}{\au}$
as the DMC result, which has an overlapping confidence interval with our results.

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-pade-dnn.py.tex}
    }
    \caption{\label{fig:QD-pade-dnn-training}Left: Performance of the wave
function in~\cref{eq:pade-dnn} as a function of training steps. Right:
Progression of variational parameters as a function of training steps. We only
show a small selection of the total number of parameters.\citesource{writing/scripts/QD-pade-dnn.py}}
\end{figure}

\begin{table}[h]
  \centering
  \label{tab:pade-dnn-energy-results}
  \caption{Energy using the neural network wave function in~\cref{eq:pade-dnn}, along with
the benchmark wave function after the same amount of optimization. Results obtained
from $2^{22}$ Monte Carlo samples and errors estimated by an automated blocking
algorithm by~\textcite{Jonsson-2018}. See
\cref{fig:QD-pade-dnn-training} for source code reference.}
  \input{scripts/QD-pade-dnn.py.table.tex}
\end{table}

% \begin{figure}[h]
%    \centering
%     \resizebox{0.7\linewidth}{!}{%
%         \input{scripts/QD-pade-dnn.py.symmetry.tex}
%     }
%     \caption{\label{fig:QD-pade-dnn-symmetry}Permutation symmetry of
% $\psi_{DNN}$ as a function of training steps. See
% \cref{fig:QD-pade-dnn-training} for source code reference.}
% \end{figure}

\begin{figure}[h]
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/QD-pade-dnn.py.weights.tex}
    }
    \caption{\label{fig:QD-pade-dnn-weights}Color map representation of the
weights and biases in the network of $\psi_{DNN}$. The left column of plots
shows the weight matrices in each layer, and the corresponding biases are in the
right column. The bottom right plot has only one color because there is only one
output layer bias. Each rectangle represents a parameter, and they have been
scaled to have the same total size. The size of the rectangles are not
important. See \cref{fig:QD-pade-dnn-training} for source code reference.}
\end{figure}

\subsection{Interpreting the Networks}

Given the apparent ability of the networks to improve upon our theoretically
motivated ansatz, it would be interesting to see if we can learn something from
the structure of the network. This is, however, notoriously difficult to do with
neural networks. In most cases the interconnections that make up the network are
simply treated as a black box, simply because we lack the understanding to do
anything else.

Still, we can make an attempt. \cref{fig:QD-pade-dnn-weights} show a color map
representation of all the weights and biases of $\psi_{DNN}$ after training.
Arguably, we should not attempt to assign physical meanings to any of the later
layers. If anything, the weight matrix of the first layer could be insightful,
as this is directly multiplied with the coordinates $\mat X$. Interestingly,
two columns appear to have significant values. This is very similar to the
effect seen in~\cref{fig:QD-rbm-symmetry} for the RBM. In fact, the same
symmetry between the two columns is even present. The remaining columns are not
as clearly zeroed out in the networks, perhaps due to the lack of
regularization. While non-trivial to interpret, this points to the networks
learning, completely by them selves, some underlying physical structure in the
inputs.



\subsection{Complexity}

While the neural network has demonstrated its capacity to learn an improved
correlation function, this does come at a considerable computational cost.

The benchmark function has complexity $\mathcal{O}(N^2)$ for $N$ particles,
rooted in the double sum in~\cref{eq:qd-pade-jastrow-anzats}. Thanks to
analytic expressions for the derivatives, the otherwise heavy Laplace operator
is also calculated in $\mathcal{O}(N^2)$ time.

The complexity of the neural network is a little harder to pin down, because of
the inherent flexibility in how the network can be structured.

The neural network computes a series of matrix-vector multiplications, both during
evaluation and for its derivatives. The input layer needs to scale equal to $N$,
while the other layers could in principle be constant. This means we could say
that the network only scales linearly, $\mathcal{O}(N)$, with increasing number
of particles. Still, the \say{constant} part of the computation time is very
large, and this kind of complexity analysis is slightly unfair.

In practice we found that we needed the second layer to have more
nodes than we have inputs. We have not determined a strict relationship, but
either $\mathcal{O}(N)$ or $\mathcal{O}{N^2}$ seems reasonable for the second
layer. Assuming the network shrinks from there on, the dominant operation is a
matrix-vector operation with matrix dimensions of order $(N\times N)$ or $(N\times N^2)$,
which works out to a complexity of $\mathcal{O}(N^2)$ or $\mathcal{O}(N^3)$. In
this more realistic view, the networks are not improving upon the complexity of
the benchmark, but rather the opposite is true.\\

Importantly the networks also depend highly on the depth (number of layers) as
well as obviously the sizes of subsequent layers. We should also note that while
asymptotic cost might be equal to the benchmark, a more precise count of the
required operations would show a substantial disparity.

However, a constant cost difference can always be brought down with optimized code, more
and better hardware, introducing GPUs etc. And while computing power may no
longer increase exponentially each year, it continues to grow nevertheless.
Large scale use of networks for quantum mechanics might therefore be well
within reach.

\subsection{Other Observables}

So for we have only investigated the energies produced by the different wave
functions. Other observables of interest could be the mean distance from the origin,
$\expval{r}$, the mean squared distance from the origin, $\expval{r^2}$ and the mean
inter-particle distance, $\expval{r_{12}}$. These are all quantities that should
depend on the correlation between the particles.
\cref{tab:QD-mean-distance-metrics} shows the three observables as predicted
from the three wave functions trained previously.

\begin{table}[h]
  \centering
  \caption{Average distances predicted by the different wave functions. Results
    obtained by Monte Carlo integration using importance sampling and $2^{24}$
    samples. The first row shows the corresponding values for a single particle
    in an ideal harmonic oscillator, with the values coming from the analytic
    expressions $\expval{r}=\flatfrac{\sqrt\pi}{2\sqrt\omega}$ and
    $\expval{r^2}=\omega^{-1}$. While the differences between $\psi_{PJ}$ and
    the networks are small, the inter-particle distance shows the largest
    difference.\citesource{writing/scripts/QD-other-metrics.py}}
  \input{scripts/QD-other-metrics.py.fixed-table.tex}
  \label{tab:QD-mean-distance-metrics}
\end{table}
We see first that, as one would expect, the predicted radii are larger when we
have multiple interacting particles. Less clear is the differences between the
benchmark wave function $\psi_{PJ}$ and the networks. In general, the results
indicate that $\psi_{PJ}$ slightly underestimates the effect of the Coulomb
repulsion. While $\expval{r}$ and $\expval{r^2}$ are not significantly different
from the networks, we see a larger difference in $\expval{r_{12}}$. This might
be because $\expval{r_{12}}$ is the quantity which is most directly
connected to the strength of the interaction

The two network types seem to once again be equivalent, with the observed differences well
within statistical expectations.

\end{document}
