\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Machine Learning}
\label{chp:machine-learning}

Machine learning (ML) is a field of study concerned with building
models\footnote{The term \emph{model} is used in a very general sense, and
refers to anything that takes information as input and produces a corresponding
output.} that can perform a task without the need for explicit instructions of
how to do so. The key word is \emph{learning}; we want to enable the model to
discover the best way to solve the task at hand within the constraints that we
have imposed on it.

A simple example of a Machine Learning algorithm is linear regression, where we endeavor to
find a linear function that best fits some observed data, with respect to some
metric of what constitutes a \emph{good fit}. Other examples include
dimensionality reduction, image classification, algorithmic trading, playing
chess. Note that all of these examples \emph{could} be implemented in terms of a
large set of if-this-then-that rules, covering the explicit response to every
conceivable input. The problem with that is that we often have little to no idea
of how exactly to determine what to do for each case, not to mention the
potential infinity of rules we would need to cover all cases.

The idea of Machine Learning is to instead describe a parameterized model that
maps inputs to outputs, and a metric to quantitatively describe how good or bad
the outputs are. Then we ask the model to find the set of parameters that will
maximize the goodness (or minimize badness) of the metric. In short, Machine
Learning is saying \emph{what} you want done, rather than saying \emph{how} you
want it done.

This chapter is dedicated to presenting the relevant bits of ML that will be
relevant to the work in this thesis. As the field is vast, only a very minimal
selection of topics will be covered, and the reader is encouraged to dive deeper
into topics where interest is sparked. We start with a overview introduction of
the general process, exemplified by Linear Regression for simplicity. From there
we shall introduce Neural Networks, along with a more in depth discussion of
optimization strategies.



\section{General Procedure}

For our purposes, we will define the steps involved in developing a machine
learning model as follows:

\begin{enumerate}
  \item Define a model, $f_{\valpha}(\vx)$ dependent on some parameters $\valpha$
  \item Define a cost function $C(f)$ - a metric of how far away from the
      ideal model $f$ is
  \item Minimize $C(f)$ with respect to $\valpha$
\end{enumerate}

It is common to divide ML into two main sub-domains: Supervised and Unsupervised
Learning. Supervised Learning is characterized by us having access to a data set
where both inputs \emph{and} desired outputs are specified. This is possibly the
most common case, or at least the case where most \emph{want} to work with.
Examples are many, and include most regression and classification tasks.


Unsupervised Learning is (naturally) the other case, where we do not have
information about any output responses. In these cases we don't focus on
learning the output (as we have no idea of what we want that to be), but rather
on what we can learn about the inputs themselves. This can include
dimensionality reduction, clustering analysis and more.

Often we need to introduce another sub-domain for the cases that fall in
between; Reinforcement Learning. This is the domain of problems where we do not
have explicit knowledge of what outputs should be, but where we can still say
something about whether or not an output was good. An example of this is the
problem of playing chess. While we can't label all board states with a
corresponding ``correct move'', we can say that moves that turn out to lead to
checkmate are probably good. Reinforcement learning tries to adapt a model so as
to maximize/minimize the reward/punishment that follows as a consequence of its
actions. Variational Monte Carlo is a type of reinforcement learning.


\section{Neural Networks}

\subsection{Nodes and Connections}

\subsection{Activation Functions}

\subsection{Backpropagation}


\end{document}
