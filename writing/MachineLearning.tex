\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Machine Learning}
\label{chp:machine-learning}

Machine learning (ML) is a field of study concerned with building
models\footnote{The term \emph{model} is used in a very general sense, and
refers to anything that takes information as input and produces a corresponding
output.} that can perform a task without the need for explicit instructions of
how to do so. The key word is \emph{learning}; we want to enable the model to
discover the best way to solve the task at hand within the constraints that we
have imposed on it.

A simple example of a Machine Learning algorithm is linear regression, where we endeavor to
find a linear function that best fits some observed data, with respect to some
metric of what constitutes a \emph{good fit}. Other examples include
dimensionality reduction, image classification, algorithmic trading, playing
chess. Note that all of these examples \emph{could} be implemented in terms of a
large set of if-this-then-that rules, covering the explicit response to every
conceivable input. The problem with that is that we often have little to no idea
of how exactly to determine what to do for each case, not to mention the
potential infinity of rules we would need to cover all cases.

The idea of Machine Learning is to instead describe a parameterized model that
maps inputs to outputs, and a metric to quantitatively describe how good or bad
the outputs are. Then we ask the model to find the set of parameters that will
maximize the goodness (or minimize badness) of the metric. In short, Machine
Learning is saying \emph{what} you want done, rather than saying \emph{how} you
want it done.

This chapter is dedicated to presenting the relevant bits of ML that will be
relevant to the work in this thesis. As the field is vast, only a very minimal
selection of topics will be covered, and the reader is encouraged to dive deeper
into topics where interest is sparked. We start with a overview introduction of
the general process, exemplified by Linear Regression for simplicity. From there
we shall introduce Neural Networks, along with a more in depth discussion of
optimization strategies.



\section{General Procedure}

For our purposes, we will define the steps involved in developing a machine
learning model as follows:

\begin{enumerate}
  \item Define a model, $\hat f_{\hat\valpha}(\vx)$ dependent on some parameters $\hat\valpha$
  \item Define a cost function $C(\hat f_{\hat\valpha})$ - a metric of how far away from the
      ideal model $\hat f_{\hat\valpha}$ is
  \item Minimize $C(\hat f_{\hat\valpha})$ with respect to $\valpha$
\end{enumerate}

It is common to divide ML into two main sub-domains: Supervised and Unsupervised
Learning. Supervised Learning is characterized by us having access to a data set
where both inputs \emph{and} desired outputs are specified. This is possibly the
most common case, or at least the case where most \emph{want} to work with.
Examples are many, and include most regression and classification tasks.

Unsupervised Learning is (naturally) the other case, where we do not have
information about any output responses. In these cases we don't focus on
learning the output (as we have no idea of what we want that to be), but rather
on what we can learn about the inputs themselves. This can include
dimensionality reduction, clustering analysis and more.

Often we need to introduce another sub-domain for the cases that fall in
between; Reinforcement Learning. This is the domain of problems where we do not
have explicit knowledge of what outputs should be, but where we can still infer
something about whether or not an output was good. An example of this is the
problem of playing chess. While we can't label all board states with a
corresponding ``correct move'', we can say that moves that lead to
checkmate are probably good. Reinforcement learning tries to adapt a model so as
to maximize/minimize the reward/punishment that follows as a consequence of its
actions.

Variational Monte Carlo is a type of reinforcement learning. This is because we
do not have information about exactly what the value of the wave function should
be for every configuration, but we still have a way to evaluate the correctness
of the wave function by calculating the energy it predicts. We'll go more in
depth into how all this connects to VMC in~\cref{chp:machine-learning}.

\section{Supervised Learning}

Even though our particular interest in machine learning is for its use in VMC, a
lot of the ideas and techniques we need come from the supervised learning
domain. Because of that, we will quickly introduce all the relevant components in the
following sections. For simplicity and clarity, we will do this through the
simplest example we have; Linear Regression.

In general for a supervised learning task, we have the following ingredients:

\begin{itemize}
\item A data set, $\mathcal{D} = \{\vx_i, \vy_i\}_{i=1}^n$, of $n$ inputs $\vx_i$
  and corresponding expected outputs $\vy_i$
\item A proposed model $\hat\vy = \hat f_{\hat\valpha}(\vx)$ that we want to
    fit to the data
\item A cost function $\mathcal{C}(\hat f_{\hat\valpha}; \mathcal{D})$ which
  to minimize with respect to the model parameters $\hat \valpha$. Note that for
  supervised learning, the cost function is dependent on the data set, allowing
  us to define metrics of how well the model reproduces the data.
\item An optimization strategy for how we should approach the minimization
\end{itemize}


\subsection{Example: Linear Regression}

\begin{figure}[h]
  \centering
  \resizebox{\linewidth}{!}{%
      \input{scripts/linear_regression_example.py.tex}
  }
  \caption{Example of linear regression applied to a simple data set of a single
    input variable. The regression tries to filter away the noise and find the
    most likely actual trend line. The source code for this graphic can be found
    at ~\cite[TODO: Add
    path]{MS-thesis-repository}, and \LaTeX{} output generated
    by~\cite{nico_schlomer_2018_1173090}}
  \label{fig:linear-regression-example}
\end{figure}


We assume that the true underlying process, $f_{\valpha}(\vx)$, that generated
$\mathcal{D}$ is linear in the inputs $\vx$. For simplicity, we will also
assume that the outputs are scalar, $\vy \defeq y$. That is, we assume the data
was generated in the following way:
\begin{align}
  y_i = f_{\valpha}(\vx_i) + \epsilon = \vx^T\valpha + \epsilon
\end{align}
where $\valpha$ is a column vector of coefficients, and
$\epsilon\sim\mathcal{N}(0, \sigma^2)$ is normally distributed noise with zero
mean and variance $\sigma^2$, introduced via measurement inaccuracy
etc.\footnote{Note that this definition implies that $f_{\valpha}$
  has zero intercept, i.e. $f_{\valpha}(\vb{0}) = 0$. We can easily work around
  that, however, by adding a constant element to every input vector, i.e. let
  $\vx' \defeq (1\ \ \vx)$ be the new inputs. The second option is to simply
  center the data beforehand, by subtracting the mean $\overline y =
  \flatfrac{1}{n}\sum_{i=1}^n y_i$ from every $y_i$, and then optionally revert
  back whenever needed.}

\subsubsection{Model}

Based on the above assumption, we propose the following model:

\begin{align}
\hat f_{\hat\valpha}(\vx) &= \vx^T\hat\valpha.
\end{align}
Ideally we want to learn $\hat\valpha$ from the data such that
$\hat\valpha\simeq\valpha$.

\subsubsection{Cost Function}

In order to quantify which $\hat\valpha$ is optimal, we need metric to compare
by. There are several conceivable choices here, many of which might lead us to
the correct $\valpha$. Most common, at least in this case, is to use the
so-called Mean Square Error (MSE):

\begin{align}
  \mathcal{C}_\text{MSE}(\hat f_{\hat\valpha}; \mathcal{D}) &= \frac{1}{n}\sum_{i=1}^n \abs{\hat f_{\hat\valpha}(\vx_i) - y_i}^2.
\end{align}
The ``learning'' task is now expressed simply as the following optimization problem:
\begin{align}
  \label{eq:mse-optimization-problem-example}
  \hat\valpha = \argmin_{\valpha'}\,\mathcal{C}_\text{MSE}(\hat f_{\valpha'}; \mathcal{D})
\end{align}

\subsubsection{Optimization}

With data, model and cost function defined, the last step is to
solve~\cref{eq:mse-optimization-problem-example}. While this can in general be a
hard task (more on this in \cref{sec:ml-optimization}), we can
actually do this particular exercise analytically with some rather simple linear algebra. Skipping
the derivation, we get the following solution:
\begin{align}
  \hat\valpha = \qty(\vX^T\vX)^{-1}\vX\vy,
\end{align}
where $\vX = (\vx_1, \vx_2\,\dots\vx_n)^T$ is the matrix with inputs laid out in
rows, and $\vy$ is the column vector of all outputs.

\subsubsection{Regularization}

Often times it turns out to be useful to add an extra term to the cost function
that depends on the size of the parameters $\hat\valpha$. This typically takes
the following form:

\begin{align}
  \mathcal{C}(\hat f_{\hat\valpha};\mathcal{D}) &=   \mathcal{C}_\text{MSE}(\hat f_{\hat\valpha};\mathcal{D}) + \gamma\norm{\hat\valpha}_p^p,
\end{align}
where $\norm{\cdot}_p$ is the $L^p$ norm, and $\gamma$ is a parameter
we set to control the degree of regularization (typically $\gamma\ll 1$). Most
used are $p=1$ (called the LASSO loss) and $p=2$ (called the Ridge loss).

The motivation for why we might want to do this is as follows: Imagine we have
data set with $\vx_i = (x_i, 2x_i)$, that is to say we have two variables per
data point, but the two variables are directly correlated. Lastly, assume that
$y_i = 3x_i$ is the underlying function. Without any regularization, all of the
following models are equally good:

\begin{align}
  \hat\valpha^{(1)} = (3, 0),\ \hat\valpha^{(2)} = (1, 1),\ \hat\valpha^{(3)} = (-997, 500)
\end{align}
The problem is that without regularization, this particular
optimization problem ill-formed, and does not have a unique solution. While all
of the above give perfect reconstruction of the data, solutions like
$\valpha^{(3)}$ yield strange interpretations. According to this model, the
output is strongly negatively correlated with $x_1$, in complete contradiction
with the underlying truth.

While this particular example is very contrived, the point should be clear: In
some cases, depending on both the data and what model we choose, the model might
be too flexible for the task at hand. This is generally referred to as the
problem of \emph{overfitting}, and can lead to strange behavior. The opposite
problem called \emph{underfitting} refers to the case where the model is too
constrained, e.g. trying to fit a first order polynomial to
quadratic data.

\section{Gradient Based Optimization}
\label{sec:ml-optimization}

Most supervised learning tasks rely on a gradient based optimization strategy,
with the umbrella name \emph{Gradient Decent} (GD).
These methods can be summarized as follows: We want to find $\vx^*$ such that\footnote{For maximization, simply minimize $-f(\vx)$. }

\begin{align}
  \vx^* = \argmin_{\vx} g(\vx),
\end{align}
for some function $g$ that should be minimized.
We solve this by starting at some initial guess $\vx^{(0)}$ and improve it by
iterating the following recurrence relation:

\begin{align}
  \label{eq:gradient-decent-definition}
  \vx^{(n+1)} &= \vx^{(n)} + \Delta \vx^{(n)} \\
  \Delta\vx^{(n)} &= -\eta \grad{g}(\vx^{(n)})
\end{align}
where $\eta$ is a suitably chosen number, typically $\eta \ll 1$. In the
limit $n\to\infty$ (if $\eta$ is small enough), this will converge to a minimum
for $g$.

When $g=\mathcal{C}(\hat f_{\hat\valpha}; \mathcal{D})$ we have some choice in
exactly how we should compute $\grad{\mathcal{C}}$. We could compute the cost
for the entire data set, or just a single data point. These
two options are referred to as Gradient Decent (GD) and Stochastic Gradient
Decent (SGD), respectively. The latter is most often used because it is less
computationally expensive for large data sets.

Lastly we have various ways of extending the basic algorithm in
\cref{eq:gradient-decent-definition}, with more sophisticated ways of
determining $\Delta \vx^{(n)}$, some of which we will mention here.

\subsection{Fixed Learning Rate}

This is the version presented in \cref{eq:gradient-decent-definition}, in where
$\eta$ is chosen in advance and remains fixed throughout all iterations:
\begin{align}
  \label{eq:fixed-learning-rate-update-rule}
  \Delta \vx^{(n)} = -\eta \grad{g(\vx^{n})}.
\end{align}

\begin{itemize}
\item Pros:
  \begin{itemize}
    \item Easy to implement
    \item Intuitive and easy to adapt based on results
  \end{itemize}
\item Cons:
  \begin{itemize}
    \item If $\eta$ is to large it can lead to divergence or inaccurate results
    \item If $\eta$ is to small we have a higher chance of getting stuck in small, local minima.
    \item If $\eta$ is small we will need many iterations before convergence
  \end{itemize}
\end{itemize}

\subsection{Momentum}

Two common problems arise when using the update rule
in~\cref{eq:fixed-learning-rate-update-rule}, namely that we tend to get stuck
in local minima and that unstable gradients can throw us off the right track.
The idea of momentum, first introduced by \textcite{Rumelhart-1986}, aims to
combat this by letting the update be a linear combination of the previous step
and the current one:

\begin{align}
  \label{eq:momentum-update-rule-def}
  \Delta\vx^{(n)} &= \alpha \Delta \vx^{(n-1)} - \eta \grad{g(\vx^{(n)})}.
\end{align}
This introduces another hyper-parameter $\alpha$, which again should be set to
an appropriate value. Letting $\alpha=0$
recovers~\cref{eq:fixed-learning-rate-update-rule}, and increasing values
increases the update terms' inertia.

\subsection{Averaging}

Another idea useful to overcome unstable gradients or poorly converging updates
is that of \emph{averaging}, invented by \textcite{Polyak-1992}. We keep the
simple update rule from~\cref{eq:fixed-learning-rate-update-rule}, but the final
$\vx^*$ we use is set to the average of all the intermediate steps:

\begin{align}
  \label{eq:averaging-update-rule-def}
  \vx^* = \frac{1}{n}\sum_{i=0}^{n-1} \vx^{(i)}.
\end{align}

\subsection{ADAM: Adaptive Moment Estimation}

Among the state-of-the-art algorithms available, employing both momentum,
averaging and several other ideas, is the ADAM algorithm, invented by
\textcite{KingmaB14}. It is slightly more involved, and the reader is encouraged
to read the aforementioned paper for an excellent presentation. The short story
has the algorithm defined as follows:

\begin{align}
  m^{(n+1)} &= \beta_1 m^{(n)} + (1 - \beta_1)\grad{g(\vx^{(n)})}\\
  v^{(n+1)} &= \beta_2 v^{(n)} + (1 - \beta_2)\qty(\grad{g(\vx^{(n)})})^2\\
  \hat m &= \frac{m^{n+1}}{1 - \beta_1^{n+1}}\\
  \hat v &= \frac{v^{n+1}}{1 - \beta_2^{n+1}}\\
  \Delta \vx^{(n)}&= -\eta \frac{\hat m}{\sqrt{\hat v} + \epsilon},
  \label{eq:adam-update-rule-def}
\end{align}
where $\eta$ is as before, $\beta_1$ and $\beta_2$ are decay rates for moment
estimates, and $\epsilon$ is a small number used to avoid division by zero
issues. \textcite{KingmaB14} provide default values for all the parameters, and
in many cases these work excellently right out of the box.
\begin{align}
  \label{eq:adam-default-parameters}
  \begin{split}
    \eta &= 0.001\\
    \epsilon &= 10^{-8}
  \end{split}
  \begin{split}
    \beta_1 &= 0.9\\
    \beta_2 &= 0.999
  \end{split}
\end{align}

We will make extensive use of ADAM in our VMC calculations, and as we will later
see in \cref{chp:results}, it will prove to outperform vanilla SGD in all cases.


\section{Neural Networks}

\subsection{Nodes and Connections}

\subsection{Activation Functions}

\subsection{Backpropagation}


\end{document}
