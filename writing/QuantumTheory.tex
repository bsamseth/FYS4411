\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{The Quantum Problem}
\label{chp:the-quantum-problem}


\section{Problem Statement}

Say you want to investigate the properties of some quantum
mechanical system. The first step is then to firmly establish how we should describe this
system and the laws that govern its behaviour. For the entirety of this thesis, we shall
assume that the systems we consider do not show any significant relativistic behaviour, so
that no such considerations are necessary.

If our system of interest consisted of non-quantum entities (e.g.\ the
trajectory of a base ball as it's thrown through the air towards a batter), we
would likely turn to our classical laws, such as Newton's second law of motion
%
\begin{align}
    \sum_i \vec F_i(t) = \dv{\vec p(t)}{t},
\end{align}
%
\noindent where $\vec F_i$ are the forces acting on the ball, and $\vec p$ is its momentum at
any given point in time. Using the law of motion we can use our knowledge about how the
environment affects the object to \emph{deterministically} calculate the resulting
behaviour. The really nice thing is that, if we also know the mass of the object, we
can derive the value of any other measurable physical quantity of interest. As such,
we can say that \emph{solving} a classical system consists of the following steps:

\begin{enumerate}
    \item Define the environment, i.e.\ the forces acting on the object(s)
    \item Use the Second Law of Motion to obtain momentum $\vec p(t)$ and position $\vx(t)$
    \item Compute quantity of interest, $Q(\vec p, \vec x, t)$
\end{enumerate}

Moving to the quantum world, much of the same procedure remains the same. For the quantum
case, we have a different law of motion. In our non-relativistic view, this is the
\gls{tdse}:

\begin{align}
    \hat H\ket{\Psi} &= i\hbar \pdv{}{t}\ket{\Psi},\label{eq:schrodinger-time-dependent-general}
\end{align}
%
where $i=\sqrt{-1}$ is the imaginary unit and $\hbar=\flatfrac{h}{2\pi}$ is the reduced Planck constant.
The thing we want to solve for in this case is the so called wave function $\ket{\Psi}$
(explained momentarily), while the description of the system (analogous to the forces in
classical mechanics) goes into $\hat H$. We refer to the latter as the Hamiltonian
operator, and it should be a complete description of the kinetic and potential energies of
the particles involved. As an example, we write the equation for a single particle in an energy
potential $V$ as follows (where we explicitly use the position basis):

\begin{align}
    \qty[-\frac{\hbar^2}{2m} \laplacian + V(\vb x; t)]\Psi(\vb x;t) =
    i\hbar\pdv{\Psi(\vb x; t)}{t},\label{eq:schrodinger-time-dependent-position-basis}
\end{align}
%
where the first term constitutes the kinetic energy of the particle (with mass $m$), and the second term is
naturally the potential energy. For the systems that we shall consider in this thesis, the
Hamiltonians will all take this form, only varying the functional form of $V$.

Knowing the wave function $\Psi$ of a system is analogous to knowing position and momentum
in the classical view in that we can compute any observable quantity from it. As such,
obtaining the full expression for the correct wave function is of immense use.

The wave function lacks a clear physical intuition for what exactly it \emph{is},
like we have for classical mechanics. Perhaps the most helpful way to view $\Psi$ is
through the fact that its squared absolute value, $\abs{\Psi(\vb x; t)}^2$, is the
probability to find a particle at position $\vb x$ at time $t$. Thinking of the (squared
norm of the) wave function as a substitute for the classical position $\vb x$ can
therefore be a helpful aid, as long as we keep the probabilistic nature of it in mind.

Summarizing the steps for \emph{solving} a quantum system, analogous to the
classical approach, we have the following plan:
\begin{enumerate}
    \item Define the environment through choosing a form for the Hamiltonian $\hat H$
    \item Use the \gls{tdse} to obtain the wave function $\ket\Psi$
    \item Use wave function to compute quantity of interest, $Q$
\end{enumerate}

\section{Stationary States}

The \gls{tdse} (\cref{eq:schrodinger-time-dependent-position-basis}) is a partial differential equation,
as it contains partial derivatives of the wave function with respect to both position and
time. The standard approach to solving this equation is through \emph{separation of
variables}. We assume that we can factorize the full wave function as follows:

\begin{align}
    \Psi(\vb x; t) &= \psi(\vb x)\phi(t).\label{eq:separatable-wave-func-def}
\end{align}
%
In addition, we assume that $V(\vb x; t) = V(\vb x)$, i.e.\ that the potential is
time-independent.\footnote{There are systems for which this assumption does not hold. We
will, however, restrict ourself to consider only Hamiltonians for which this description
is valid} With these assumptions, we can divide through with $\Psi$ in
\cref{eq:schrodinger-time-dependent-position-basis} and obtain the
following:\footnote{It could be tempting to simply strike $\Psi$ from the lhs.\ of
\cref{eq:schrodinger-time-dependent-position-basis} when diving by $\Psi$. Nevertheless, we
must remember that the Hamiltonian is an operator (specifically seen through the
$\laplacian$ in this case), and so we must divide only after letting this operate on
$\Psi$.}

\begin{align}
    \qty[- \frac{\hbar^2}{2m} \laplacian \psi + V(\vb x)\psi]\psi^{-1} &=
    i\hbar\dv{\phi}{t} \phi^{-1}.
\end{align}
%
We now make the following subtle observation: Since the lhs., a function of $\vb x$, is
equal to the rhs, a function of $t$, they must both be equal to a constant. If
this was not true, we could vary one of $\vx$ or $t$ and alter only one side of
the equation, leaving it invalid.
As both sides have units of energy, let's denote this constant energy as $E$, and proceed
to solve each equation by it self.

The time dependent equation becomes:

\begin{align}
    i\hbar \dv{\phi}{t} &= E\phi(t),
\end{align}
which is trivially solvable:
\begin{align}
    \phi(t) &= Ae^{-iEt/\hbar},
\end{align}
for some constant $A=\phi(0)$ determined by boundary conditions.

The time-independent equation, known as the \gls{tise}, is:
\begin{align}
    - \frac{\hbar^2}{2m} \laplacian\psi + V(\vb x)\psi =
    E\psi\label{eq:schrodinger-time-independent-position-basis}
\end{align}
The solutions to this equation are the \emph{stationary states} of the system. If
we are able to find these solutions, then we automatically have also the full time
dependent solution through \cref{eq:separatable-wave-func-def}.

If we return \cref{eq:schrodinger-time-independent-position-basis} to the more general
form,
\begin{align}
    \hat H \ket{\psi} &= E\ket{\psi}
\end{align}
we can recognize the problem as an eigenvalue problem where we seek the eigenvalues ($E$)
and eigenvectors ($\ket{\psi}$) of the operator $\hat H$. In light of this, we prefer to
explicitly label the equation to account for the possibility that the equation could have
multiple (potentially infinite) solutions, and write this as
\begin{align}
    \hat H\ket{\psi_n} &= E_n\ket{\psi_n}
\end{align}
Each of the $\ket{\psi_n}$ represents one possible stationary state, and could for instance be
different levels of energy excitations within an atom. For our purposes, we will only care
about the so called \emph{ground state}, i.e.\ the state $\ket{\psi_n}$ corresponding to the
lowest possible $E_n$. By convention, we assume that the energies are ordered such that $E_i
\leq E_j$ if $i < j$, and denote the ground state as $\ket{\psi_0}$ and the corresponding ground
state energy as $E_0$.


\section{Many-Body Systems}

Up until now, for simplicity, we've only considered the description of single-particle
systems. Changing the number of particles is a change in the system description, and as
such it entails modifying the Hamiltonian operator accordingly. Everything presented thus
far generalizes well to the case of more than one particle, simply by introducing the
appropriate sums. The general form of the many-body Hamiltonian we will consider is now:

\begin{align}
    \hat H &= - \sum_{i=1}^N \frac{\hbar^2}{2m_i} \laplacian_i + V(\vb x_1, \vb x_2,\dots,
    \vb x_N)\\
    &= -\sum_{i=1}^N \frac{\hbar^2}{2m_i} \laplacian_i + V(\vb
    X),\label{eq:Hamiltonian-operator-general}
\end{align}
were $\mat X \defeq (\vb x_1\ \vb x_2\ \dots\ \vb x_N)^T\in \mathbb{R}^{N\times D}$ is the matrix
of $D$-dimensional row vectors of coordinates for each particle. For further clarity,
$\vb x_i \defeq \sum_{d=1}^D x_{i,d} \vb e_d$ is a $D$-dimensional vector described by
its coordinates $x_{i,d}$ (with unit vectors $\vb{e}_d$), and the corresponding Laplacian
operator is
\begin{align}
    \laplacian_i \defeq \sum_{d = 1}^D \pdv[2]{}{x_{i,d}}.
\end{align}


\section{Requirements of Wave Functions}\label{sec:requirements-of-wave-functions}

We have stated earlier that by solving the Schrödinger equation and obtaining the wave
function, we can compute any desirable quantity of interest. In order for the wave
function to fulfill this rather impressive encoding of everything about the system, it has
to satisfy certain criteria. We now devote some special consideration to make these
requirements explicit.

In order to represent a physically observable system, a wave function $\Psi$ must:
\begin{enumerate}
    \item Be a solution to the Schrödinger equation
    \item Be normalizable (in order to represent a probability)
    \item Be a continuous function of space
    \item Have a continuous first order spacial derivative
    \item Obey suitable symmetry requirements
\end{enumerate}
%
While the first requirement are obvious, points 2-4 boil down to $\Psi$ taking a
functional form that is well behaved, satisfying required boundary conditions and being
possible to view as a probability distribution. The last point is perhaps less clear, and
we devote some further attention to this point in particular.


\subsection{Symmetry of Wave Functions}

Nature has many example of systems made up of particles of the same
species. That is, the particles all have the same mass, spin, electromagnetic
charge etc.\ such that there is no way to  distinguish one from the other by
measuring their properties. An example could be the electrons of an atom, all of
which have the exact same physical properties.

In classical mechanics, we can still distinguish identical particles by other
means. Imagine for instance a set of perfectly identical planets in orbit. Even
though they have all of the same physical properties, we can still enumerate
them and keep track of which is which. This is due to the fact that their
position in time and space is deterministically defined by their current state,
which allows us to track them.

In quantum mechanics, however, we no longer have this deterministic view. In
this world, even if we know where all the individual electron are at a specific
point in time, we cannot say with certainty where they will be at a later
time. We blame this on the uncertainty principle. The result is that systems of
identical particles become systems of \emph{indistinguishable} particles in
quantum mechanics.

Consider now a system of two indistinguishable particles, labeled $\vec x_1$ and
$\vec x_2$, where $\vec x_i$ contains all the quantum numbers required to describe
particle $i$ (e.g.\ position coordinates and the $z$ component of spin). The
system is then described by a wave function

\begin{align}
    \Psi(\vec x_1, \vec x_2).
\end{align}
Because the particles are indistinguishable, this labeling of 1 and 2 is
arbitrary, and so we should be able to relabel them:

\begin{align}
    \Psi(\vec x_2, \vec x_1)
\end{align}

These two equations, which represents exchanging the two particles, \emph{must}
describe the same physical system. That is, the probabilities of both states
must be equal:

\begin{align}
    \abs{\Psi(\vec x_1, \vec x_2)}^2 &= \abs{\Psi(\vec x_2, \vec x_1)}^2\\
    \iff \Psi(\vec x_1,\vec x_2) &= e^{i\alpha}\Psi(\vec x_2, \vec x_1),
\end{align}
i.e.\ they can only differ in their complex phase, which doesn't affect any measurable
quantity. Repeating the exchange once more yields the original wave function, and so

\begin{align}
    \Psi(\vec x_1,\vec x_2) &= e^{2i\alpha}\Psi(\vec x_1, \vec x_2)\\
    \iff e^{i\alpha} &= \pm 1.
\end{align}
This results states that any wave function, upon the exchange of
indistinguishable particles, must be either symmetric (same sign) or
anti-symmetric (opposite sign) to that of the original. This is generalizable to
any number of particles, and is known as the \emph{Pauli exclusion principle}.
The following theorem summarizes the result~\cite{PhysRev-58-716}:

\begin{theorem}[Spin-Statistic
    Theorem]\label{theorem:spin-statistic}

    The wave function of a system of identical integer-spin particles has the same value
    when the positions of any two particles are swapped. Particles with wave functions
    symmetric under exchange are called bosons.

    The wave function of a system of identical half-integer–spin particles changes sign
    when two particles are swapped. Particles with wave functions antisymmetric under
    exchange are called fermions.
\end{theorem}

\section{Observables - From Wave Function to Measurement}

We have previously stated that armed with the correct wave function we can compute any
measurable quantity of interest. This statement would not be of much use without going
into exactly how one can go about this.

Assume we want to compute an observable $O$. The first step is to determine the
corresponding \emph{operator} $\hat O$. This is in general done by taking the classical
description of the observable and performing a canonical transformation.\footnote{There are
also quantities that do not have a classical analog (e.g.\ spin) for which we can still
find operator forms.} Most notably, we have for the following transformations for
position and momentum:

\begin{align}
    \vb x &\rightarrow \hat\vx\\
    \vb p &\rightarrow -i\hbar\grad
\end{align}
For example, as is often the case, let's say we want to compute the total energy of the
system. For $N$ particles that would classically be
\begin{align}
    H = \sum_{i=1}^N \frac{p_i^2}{2m_i} + V(\vb X)
\end{align}
where $\vb p_i$ denotes the momentum of particle $i$, all of which are placed in some
spacial potential $V$. It is easily verified that if we perform the above mentioned
substitutions we will recover \cref{eq:Hamiltonian-operator-general} and recognize
it as the Hamiltonian operator, $\hat H$.

Finally, having both the wave function and the appropriate operator $\hat O$ we can
compute the expectation of that observable:

\begin{align}
    \expval{O}=\expval{\hat O}&= \frac{\expval{\hat O}{\Psi}}{\braket{\Psi}} \\
    &= \frac{\int\dd{\vb X} \Psi^*(\vb X)\hat O(\vb X)\Psi(\vb X)}{\int\dd{\vb X}
    \abs{\Psi(\vb X)}^2}
\end{align}
where $\int\dd{\vb X}\qty(\cdot)$ indicates an integral over all possible configurations of the
system (e.g.\ all possible position and spin values for each particle). Often we have
required the wave function to be normalized in such a way that the denominator is equal to
unity, and it can then be omitted.

For many-body systems it should be apparent that this integral quickly becomes intractable
to compute analytically. In practice we employ a numerical strategy to evaluate these
integrals, where the technique we use depends on the dimensionality of the integral and
the required level of accuracy. In our case, due to the large number
of degrees of freedom in the systems we shall investigate, we will use
\gls{mci}. This is will be discussed in more detail in \cref{chp:monte-carlo}.

\section{Example Systems}

So we have not presented any particular systems. In this thesis we focus our
attention on two particular systems for illustration purposes. We chose these systems
for their simplicity and/or the amount of preexisting results available
in the literature. We do this so as to be able to benchmark the eventual results against
know exact solutions, or when this does not exist, against the state-of-the-art results
available.

\subsection{Quantum Dots}
\label{sec:quantum-dots-theory}

We consider a system of electrically charged particles (e.g.\ electrons) confined in a pure
isotropic harmonic oscillator potential, with an idealized total Hamiltonian
given by:

\begin{align}
    \begin{split}
        \hat H &= \sum_{i=1}^N\qty(-\frac{1}{2}\laplacian_i + V_{ext}(\vec r_i)) +
        \sum_{i < j} V_{int}(\vec r_i, \vec r_j)\\
        &= \sum_{i=1}^N\qty(-\frac{1}{2}\laplacian_i + \frac{1}{2}\omega^2
        r_i^2) + \sum_{i < j} \frac{1}{r_{ij}},
    \end{split}\label{eq:H-QD-def}
\end{align}
where we use natural units ($\hbar=c=m_e=1$) with energies in
atomic units (a.u.), $N$ denotes the number of particles in the system, and
$\omega$ is the oscillator frequency of the trap. Further, $\vec r_i$
denotes the position vector of particle $i$, with $r_i \defeq \norm{\vec r}$ and
$r_{ij}\defeq \norm{\vec r_i - \vec r_j}$ defined for notational brevity.

This system describes particles trapped in a parabolic potential well that pulls
them towards the bottom at all times, while simultaneously feeling the repulsive
Coulomb forces from the other particles. This hinders all particles from settling together
at the bottom. Even for this somewhat idealized system, the interplay between these two
opposing forces gives rise to a surprisingly complex problem, which will prove remarkably
hard to solve analytically even for two particles, and utterly impossible for higher $N$.

With the natural units in place, the only involved quantity without a proper
unit is length, i.e.\ what unit does the $\vb r_i$ have. A convenient
choice is to consider the mean square vibrational amplitude, $\expval{r^2}$, for
a single particle at $T = \SI{0}{\kelvin}$ placed in the oscillator trap.
Computing the expectation value we get
$\expval{r^2}=\flatfrac{\hbar}{2m\omega}$, and we define the unit of length as the
characteristic length of the trap, $a_{ho}=\qty(2\expval{r^2})^{\flatfrac{1}{2}}=\qty(\flatfrac{\hbar}{m\omega})^{\flatfrac{1}{2}}$.

In our case, we limit ourselves to $N=2$ interacting electrons in two
dimensions in a trap with a frequency such that $\hbar \omega =
1$.\footnote{Note that, due to the natural units, this implies that $\omega =
  1$, which further means that $a_{ho} = 1$. It should be apparent why we use these
  definitions, as it simplifies both units and expressions.} We do this because for
this case we have exact, analytical solutions for the ground state energy. With the
interaction term included, the ground state energy is $E_0 = \SI{3}{\au}$~\cite{Taut1993}.
This limitation is purely one of convenience, as having exact benchmarks makes for better
verification of results. Furthermore, limiting the size of the problem makes the required
computation time manageable, which is good when experimenting with different techniques.

\subsubsection{Simple Non-Interacting Case}\label{sec:simple-non-inter-HO}
If we omit the interacting terms in \cref{eq:H-QD-def} we have
the standard harmonic oscillator Hamiltonian:
\begin{align}
  \label{eq:ho-no-interaction-hamiltonian}
    \hat H_0 &= \sum_{i=1}^N\qty(-\frac{1}{2}\laplacian_i +
    \frac{1}{2}\omega^2 r_i^2).
\end{align}
This Hamiltonian lends it self to analytical solutions, and the stationary
single particle states are (in 2D)~\cite{griffiths_schroeter_2018}:
\begin{align}\label{eq:ho-single-particle-orbitals}
    \phi_{n_x, n_y}(x, y) &= A H_{n_x}(\sqrt\omega x)H_{n_y}(\sqrt\omega y)
    e^{-\frac{\omega}{2}\qty(x^2 + y^2)},
\end{align}
for quantum numbers $n_x, n_y = 0, 1,\dots$, and the Hermite polynomials
$H_n$ (not to be confused with the Hamiltonians, and never to be mentioned again). The
ground state, $n_x=n_y=0$ is simply
\begin{align}
  \label{eq:ho-no-interaction-ground-state}
    \phi_{00}(x,y) =
    \sqrt{\frac{\omega}{\pi}}e^{-\frac{\omega}{2}\qty(x^2+y^2)}.
\end{align}
Using this wavefunction we can calculate the ground state
energy,
\begin{align}
    \epsilon_{00} = \frac{\expval{\hat H_0}{\phi_{00}}}{\braket{\phi_{00}}}
    = \omega = \SI{1}{\au}
\end{align}
The ground state wavefunction for the (unperturbed) two-electron case is simply the
product of the one-electron wave functions,
\begin{align}
    \begin{split}
        \Phi(\vec r_1, \vec r_2) &= \phi_{00}(\vec r_1)\phi_{00}(\vec r_2)\\
        &= \frac{\omega}{\pi} e^{-\frac{\omega}{2}\qty(r_1^2+r_2^2)}.
    \end{split}\label{eq:Phi-non-inter}
\end{align}
We can once again evaluate the ground state energy analytically, which yields
\begin{align}
    E_0 = \frac{\expval{\hat H_0}{\Phi}}{\braket{\Phi}}
    = 2\omega =\SI{2}{\au}
\end{align}
This result is not surprising, as adding one more particle, without any
interactions, should simply double the energy. Another way to look at it is
that the simple harmonic oscillator solution gives $\flatfrac{\omega}{2}$
per degree of freedom, so adding another two yields and extra $\omega$.


When the two particles are electrons, we may say something about their total
spin. As electrons are fermions, their total wavefunction must be
anti-symmetric upon interchanging the labels $1$ and $2$.
\cref{eq:Phi-non-inter} is obviously symmetric, and so the
spin-wavefunction must necessarily be anti-symmetric. For the combination of
two spin-1/2 particles, there is only one candidate, namely the spin-0
singlet:

\begin{align}
    \chi_0 = \frac{1}{\sqrt 2}\qty(\ket{\uparrow\downarrow} -
    \ket{\downarrow\uparrow}).
\end{align}
A similar argument can be made for particles with different spins.

\subsubsection{Considerations from the Virial Theorem}

The Virial Theorem gives a general relation for the time-averaged kinetic
energy $\expval{K}$ and the corresponding potential energy
$\expval{V_{pot}}$ of a stable system of $N$ particles. In general the
theorem states:

\begin{align}
    \expval{K} = -\frac{1}{2}\sum_{k=1}^N \expval{\vec F_k \cdot \vec
    r_k},\label{eq:virial-theorem}
\end{align}
where $\vec F_k$ denotes the combined forces acting on particle $k$, located
at position $\vec r_k$. For a radial potential on the form $V(r)=ar^n$, such
that the potential between any two particles in the system depends on some
power of the inter-particle distance, the
theorem takes the following form:
\begin{align}
    \expval{K} = \frac{n}{2}\expval{V_{TOT}}
\end{align}
where $V_{TOT}$ denotes the sum of the potential energy $V(r)$ over all
pairs of particles.

Although the harmonic oscillator potential does not depend on the
\emph{inter-particle} distance, but rather on each particles position, it
works out to the same relation in our case. Computing the full relation for
our Hamiltonian for two electrons in two dimensions, it even works out so
that we can use the same relation on the HO potential and the Coulomb
potential separately, and add the result.\footnote{We can (and have) work(ed) this out
via \cref{eq:virial-theorem} directly using the corresponding forces for
the harmonic oscillator and Coulomb potential. A write-up of the derivation is not given
though, as this is a well known result.} This means that the Virial Theorem
predicts the following~\cite{Katriel2012}:

\begin{align}
    \expval{K} = \expval{V_{ext}} -
    \frac{1}{2}\expval{V_{int}}.\label{eq:virial-result}
\end{align}
Note that this implies that we should consider the \emph{total} kinetic
energy, and the \emph{total} external and internal potential energies, as opposed to per
particle.


\subsection{Liquid $^4$He}
\label{sec:liquid-helium-theory}

Consider now an infinite collection of $^4$He atoms packed with a given density,
$\rho$. As infinities are hard to work with, we model this by considering a
cubic simulation box with side lengths $L$ and periodic boundary conditions. The
infinite collection is then composed of stacking copies of such simulation boxes
together. ~\cref{fig:pbc-illustration} shows an illustration of the idea.

\begin{figure}[h]
  \centering
  \input{illustrations/PBC-illustration.tex}
  \caption[Illustration of periodic boundary conditions]{Illustration of $^4$He organized into a grid of identical simulation
    boxes. The actual boxes are three-dimensional.\citesource{writing/illustrations/PBC-illustration.tex}}
  \label{fig:pbc-illustration}
\end{figure}


The Hamiltonian for this system is taken as

\begin{align}
    \hat H &= -\sum_{i=1}^N \frac{\hbar^2}{2m}\laplacian_i + \sum_{i < j} V(r_{ij})
\end{align}
i.e.\ the kinetic energy of all atoms, plus a interaction potential dependent on
the distance between all pairs of atoms. The mass $m$ is the mass of one $^4He$
atom. The form of $V$ is not known analytically, but is experimentally probed to
great accuracy. Theorists have since fitted specific functional forms to the
experimental data, and we will do our calculations using one of these
potentials. The most commonly used is the simple Lennard-Jones (LJ)
potential~\cite{Kalos-1981}:

\begin{align}
    \label{eq:Lennard-Jones-def}
    V(r)
     &= 4\epsilon\qty[\qty( \frac{\sigma}{r} )^{12} - \qty( \frac{\sigma}{r} )^6 ]
\end{align}
with $\epsilon/\kappa = \SI{10.22}{\K}$\footnote{$\kappa$ is the Boltzmann constant.} and $\sigma = \SI{2.556}{\angstrom}$. This
models the competing forces of the atoms' mutual repulsion and attraction. The
positive term describes the short range Pauli repulsion due to overlapping
electron orbitals, and the negative term describes the long range attraction
due to phenomena such as van der Waals forces.

We can also use the slightly more accurate (and complicated) potential named HFDHE2~\cite{Aziz-hfdhe2}:

\begin{align}
    \label{eq:HFDHE2-def}
    V(r) &= \epsilon
    \left\{\!\begin{aligned}
        &A \exp(-\alpha \frac{r }{ r_m})\\
        &- F(r) \qty[ C_6  \qty(\frac{r_m }{ r})^6 + C_8  \qty(\frac{r_m }{ r})^8 + C_{10} \qty(\frac{r_m }{ r})^{10}]
    \end{aligned}\right\}
\end{align}
with

\begin{align}
    F(r) &= \begin{cases}
        \exp( - [D \frac{r_m}{r} -  1]^2 ) & \qfor \frac{r}{r_m} \leq D\\
        1 & \qotherwise
    \end{cases}
\end{align}
with the following parameters:
\begin{align}\label{eq:HFDHE2-parameters}
    \begin{split}
        &A = \num{0.5448504e6}\\
        &\alpha = \num{13.353384}\\
        &D = \num{1.241314}\\
        &r_m= \SI{2.9673}{\angstrom}
    \end{split}
    \begin{split}
        &\epsilon/\kappa = \SI{10.8}{\K}\\
        &C_6= \num{1.37732412}\\
        &C_8= \num{0.4253785}\\
        &C_{10}= \num{0.178100}\\
    \end{split}
\end{align}
Both potentials grow rapidly for small $r$, and tend to $0$ for large $r$. The
interesting sections of both potentials are shown
in~\cref{fig:helium-potentials-plot}. The potentials are very similar, with the
main difference being the depth of the well and how sharply the potential dies
off.

\begin{figure}[h]
  \centering
  \input{illustrations/helium-potentials.tex}
  \caption[Lennard-Jones and HFDHE2 potentials]{Lennard-Jones and HFDHE2 potentials used to model the potential
    between pairs of $^4$He atoms. Both potentials grow rapidly towards infinity
  when $r\to0$ and approach zero when $r\to\infty$.\citesource{writing/illustrations/helium-potentials.tex}}
  \label{fig:helium-potentials-plot}
\end{figure}



\subsubsection{A Note About Units}

In the literature it is common to express the energies in Kelvin per particle,
and lengths in angstrom~\cite{Kalos-1981, Aziz-hfdhe2, ruggeri2018}. In order to convert energies to temperatures we
divide by the Boltzmann constant, $\kappa$, because it has the unit of Joules per
Kelvin. The Hamiltonian becomes:
\begin{align}
  \hat H &= -\sum_{i=1}^N \frac{\hbar^2}{2m\kappa}\laplacian_i + \sum_{i<j}\frac{1}{\kappa}V(r_{ij}).
\end{align}

If we use SI units for the constants involved we get:
\begin{align}
  \frac{\hbar^2}{2m\kappa} = \SI{6.059651974e-20}{\metre^2\kelvin} = \SI{6.059651974}{\angstrom^2\kelvin},
\end{align}
which turns out to be a reasonably sized number when we use angstrom for
lengths.\footnote{Note that $\laplacian$ has units of $\text{length}^{-2}$, so
  the units work out to Kelvin.} We will use these values in the implementation,
and simply refer to energies in kelvin when we study this system. However, if
the reader ever wants to convert the units, for comparison with other works
perhaps, simply multiplying with the value of $\kappa$ in the unit system of
choice should yield the corresponding energy.


\subsubsection{Minimum Image}

Because we assume a periodic structure we must take this into account when
calculating distances. Consider two particles, A and B, located at opposite
corners of the simulation box. What is the distance between them? The intuitive
answer is $\norm{\vb r_A - \vb r_B} = \sqrt{3}\,L$, i.e.\ the length of the
diagonal of the cubic box. Nevertheless, the answer we should use is zero. The
reason is that there is a periodic copy of the box stacked such A and the
periodic B copy are located at the same corner. This way of calculating
distances is called minimum image, and says that we should use the shortest
possible distance. In general, if two particles have a distance of $\Delta x =
\flatfrac{L}{2} + \delta_x$ ($\delta_x\geq 0$, each spatial coordinate handled
individually), the minimum image distance is $\Delta x_{\mathit{min}} = \Delta x
- L = \flatfrac{L}{2} - \delta_x$.

In our implementation, whenever we need a distance between two particles, we use
the following prescription (example in Python):

\begin{lstlisting}[language=Python]
import numpy as np

# Example coordinates.
L = 5
p1, p2 = np.array([0, 0, 0]), np.array([2, 3, 4])

diff = p2 - p1  # [2, 3, 4]
diff_minimum = diff - L * np.round(diff / L)  # [2, -2, -1]
\end{lstlisting}
The last variable, \texttt{diff\_minimum} represents the minimum image distance
vector and this is the one used for any further calculations.

\subsubsection{Correcting for Periodicity in Potentials}

The potential $V(r_{ij})$ depend on the inter-particles distances. However,
there are an infinite amount of particle pairs if we consider the system as a
whole. We use periodic boundary conditions, and shall only consider pairs where
both particles are in the simulation box (but still respecting the minimum image
convention). This limitation excludes any interactions that act on length scales
larger than $\flatfrac{L}{2}$, and this can have a significant impact on the
total system.

In an attempt to limit this effect, we modify the potentials slightly. First we
explicitly truncate the potential to be zero for large distances, and shift it
slightly so that the function remains continuous. That is, considering $V(r)$
from~\cref{eq:Lennard-Jones-def}, we change it as follows:

\begin{align}
  \label{eq:Lennard-Jones-truncated-def}
  V_\mathit{trunc}(r) &=
                          \begin{cases}
                            V(r) - V(\flatfrac{L}{2}) & \qfor r \leq \flatfrac{L}{2}\\
                            0 & \qotherwise
                          \end{cases}.
\end{align}
Note that in order for this truncation to be sensible, we must use a
sufficiently large box so that $V(\flatfrac{L}{2})$ is sufficiently close to
zero. What exactly \emph{sufficiently} means is left rather vague, but we
mention it as a potential source of error.\\

The truncation obviously leads to slightly less precise results, but this can
partially be corrected for with so-called \emph{tail corrections}. The approach
is to model the potential contribution of all particles further away than
$\flatfrac{L}{2}$ in a mean-field manner. The result is simply adding a constant
term, and acts only to shift the total potential in a given direction. As the
purpose of this thesis is not to obtain the most realistic results possible, and
rather a \emph{relative} comparison of methods, we will not spend more time on
specific ways of implementing such corrections.


\end{document}
