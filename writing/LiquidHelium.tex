\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Liquid $^4$He}
\label{chp:liquid-helium}

We turn now to the much more challenging system of liquid Helium, presented
in~\cref{sec:liquid-helium-theory}. As before, we will first present the
benchmark result followed by the neural networks.


\section{Benchmark}

We will use one of the simpler benchmark wave functions for this system, known
as the McMillan form wave function~\cite{McMillan-1965}:

\begin{align}
  \label{eq:McMillan-wave-function-def}
  \psi_{M} &= \exp(-\frac{1}{2}\sum_{i<j} \qty(\frac{\beta}{r_{ij}})^5).
\end{align}
where $\beta$ is the only variational parameter and has units of angstrom. An
important observation now is the lack of any single particle wave function
factor. In the case of the Quantum Dot we had a Gaussian localized at the origin
as a result of the potential well. This system, however, is infinite and
periodic without any such influence driving it towards particular points in
space. Furthermore, because of the lack of an external field the single particle
solutions are just free particles, and does not help us understand the many-body
system.

\subsection{Finite Size Dependency}

An important aspect of all the results that we will present is that they are
highly dependent on the number of particles used in the simulation box, as well
as the size of the box it self. We will hold the number density of particles
constant, $\rho = \flatfrac{\num{0.365}}{\sigma^3}$ (where $\sigma =
\SI{2.556}{\angstrom}$ as defined in \cref{eq:Lennard-Jones-def}), and set the
side lengths of the simulation box, $L$, depending on
the number of particles $N$:

\begin{align}
  L = \sqrt[3]{\frac{N}{\rho}}.
\end{align}

\noindent As the assumption of periodicity is a simplifying approximation, we
introduce some erroneous effects because of it. These generally disappear as we
increase the number of particles (and hence the size of box), but the
computation time needed to run the simulations increase significantly with
increasing numbers. The purpose of the following analysis is to test the
\emph{relative} accuracy of different wave functions. With that in mind we have
used a small number of particles in the main results, which introduces a
significant error. The number of particles should still be large enough to
introduce all the relevant effects and produce valid test results.


\subsection{Optimizing}

We have optimized~\cref{eq:McMillan-wave-function-def} using $\num{10000}$
iterations of $\num{5000}$ MC cycles each. We used standard Metropolis sampling
along with the ADAM optimizer. \cref{fig:He-benchmark-training} shows the
progression of both the energy and variational parameter during training.
Because of the strong correlations involved, there is significantly more
variance in these results compared to the benchmark used for Quantum Dots. We
see the value for $\beta$ oscillating without any indication of converging to a
fixed value.

\cref{tab:He-benchmark-results} show the energy estimates from the final model.
The same wave function (i.e. same value for $\beta$) has also been used on
different number of particles to illustrate how the energy increases for larger
systems. Based on computations with larger and larger systems, the value for $N
= 256$ is quite close to the apparent convergence for $N\to\infty$.

\begin{figure}[h]
  \centering
  \resizebox{\linewidth}{!}{%
    \input{scripts/He-benchmark.py.tex}
  }
  \caption{\label{fig:He-benchmark-training}Left: Ground state energy produced by~\cref{eq:McMillan-wave-function-def}
    as a function of training steps. Right: Progression of the variational
    parameter $\beta$ of~\cref{eq:McMillan-wave-function-def} as a function of
    training steps. A convergence is clearly visible, although there is more
    variance in the value for $\beta$ compared to the parameters of the
    benchmark used for Quantum Dots.}
\end{figure}

\begin{table}[h]
  \centering
  \caption{\label{tab:He-benchmark-results}Predicted ground state energy of helium atoms at density $\rho =
\flatfrac{0.365}{\sigma^3}$. The number of particles used in the simulation box
is indicated by the superscript on $\psi_M$. Values obtained using $2^{22}$
Monte Carlo samples.}
  \input{scripts/He-benchmark.py.table.tex}
\end{table}


\section{Neural Networks}

We again attempt to apply a neural network to our wave function representation.
Liquid $^4$He is a far more challenging problem compared to the two-particle
quantum dot, and as such it will serve as an important test of the capabilities
of neural networks for general quantum mechanical many-body problems.

As before, we have not simply removed the original wave function, but rather
added the network as another correlation factor:

\begin{align}
  \label{eq:helium-dnn-mcmillan-def}
  \psi_{DNN}(\mat X) &= \psi_M(\mat X)\,f(\mat X),
\end{align}
where once again $f: \mathbb{R}^{N\times D}\to\mathbb{R}$ represents an
arbitrary neural network. 

Lastly, we limit our investigation to $N=32$ particles and still with density
$\rho=\flatfrac{0.365}{\sigma^3}$. This is done from a practical standpoint,
considering the substantial computational cost.

\subsection{Network Architecture}

The proposed network structure is one of many that are likely to perform well,
and in all likelihood this is not the optimal form. A more in depth analysis of
what structures perform best, including other types of networks (e.g.\ recurrent,
convolutional, residual etc.) is left for future research.\\

Based on the experimental results observed for quantum dots, in conjunction with
trail and error, we have used the following network structure:

\begin{center}
  \begin{tabular}{lcc}
    \toprule
    \addlinespace
    Layer & Nodes & Activation\\
    \addlinespace
    \midrule
    \addlinespace
    \addlinespace
    Input & 96 & ---\\
    Hidden 1& 144 & $\tanh$\\
    Hidden 2& 36 & $\tanh$\\
    Output & 1 & $\exp$\\
    \addlinespace
    \addlinespace
    \bottomrule
  \end{tabular}
\end{center}
Once again the number of nodes in the input layer is fixed by the number of
degrees of freedom ($D\times N$ for $D$ dimensions). The number of nodes in the
second layer is somewhat arbitrary, and could be changed. Importantly it is
larger than $96$, allowing the network to learn more features. Still, it is not
enough nodes to fully encode the relative difference between every pair of
coordinates, as would be required to reproduce $\psi_M$ fully. This number was
found to be large enough to allow learning, but still be reasonably efficient.
Lastly, the second hidden layer was set to $36$ nodes. This is also somewhat
arbitrary, but importantly it is smaller than $144$, allowing the gradual
narrowing down to the single output. The activation functions are the same as
those used for quantum dots.

Finally, based on the success of applying sorting on the inputs to the network
to impose symmetry we will reuse this strategy here. The wave function with
input sorting applied is denoted as $\psi_{SDNN}$.

\subsection{Optimization}

We produced the following results by optimizing $\psi_M$ and $\psi_{SDNN}$ with
similar hyperparameters. We initialized $\beta=\num{2.85}$ for both wave functions
for a fair comparison. We used $\num{250000}$ training iterations of
$\num{5000}$ MC cycles each. Contrary to the quantum dot system, where a large
set of hyperparameters yielded good results, we have found this problem to be
highly sensitive to good hyperparameters. We had to reduce the learning rate to
$\eta=\num{0.0001}$ to avoid divergence during training. We also found that the
networks performed better with a small degree of L2 regularization,
$\gamma=\num{0.00001}$. While other choices might still be better, there were
significantly less room for error in this much more challenging problem.

\begin{figure}[h]
  \centering
  \input{scripts/He-mcmillan-dnn.py.tex}
  \caption{\label{fig:He-dnn-training}Performance of $\psi_M$ and $\psi_{SDNN}$
    as a function of training steps. The network shows a clear energy reduction
    compared to the traditional benchmark.}
\end{figure}

\cref{fig:He-dnn-training} shows a graph of the ground state energy during the
course of training for both wave functions. On this time scale (i.e.\ $\num{25}$
times more training than used for the benchmark alone) we see $\psi_M$ converge
immediately to the same energy level as we saw in
\cref{fig:He-benchmark-training}. With the neural network, however, there is an
apparent immediate lowering of energy. Furthermore, after 
about \SIrange{30}{60}{\percent} of the total training we see a gradual lowering
in the energy before it plateaus again.

\cref{tab:He-dnn-results} shows the final energies produced from the two wave
functions after training. We see a significant lowering of the total energy.
Unlike the results for quantum dots, however, we do not get a lowering of the
variance, but rather a small increase. It is hard to say exactly how much of the
total error in energy is corrected for by the inclusion of the networks, due to
the lack of existing results using $N=32$ particles with the Lennard-Jones
potential, with the same potential corrections that we have included.
Nevertheless, the networks continue to show their ability to improve upon the
results of all the benchmark wave functions we have paired them with. As our
main interest in this work is to demonstrate this general ability, as opposed to
producing results closest to experimental results, we view these results as
positive encouragement to continue this line of research.

\begin{table}[h]
  \centering
  \input{scripts/He-mcmillan-dnn.py.table.tex}
  \caption{\label{tab:He-dnn-results}Predicted ground state energy of both
    $\psi_M$ and $\psi_{SDNN}$ after the same amount of optimization. Results
    obtained from $2^{23}$ Monte Carlo samples and with errors estimated using
    Blocking. See \cref{fig:He-dnn-training} for source code reference.}
\end{table}




\end{document}
