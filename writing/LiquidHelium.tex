\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Liquid $^4$He}
\label{chp:liquid-helium}

We turn now to the much more challenging system of liquid helium, presented
in~\cref{sec:liquid-helium-theory}. As before, we will first present the
benchmark result followed by the neural networks.


\section{Benchmark}

We will use one of the simpler benchmark wave functions for this system, known
as the McMillan form wave function~\cite{McMillan-1965}:

\begin{align}
  \label{eq:McMillan-wave-function-def}
  \psi_{M} &= \exp(-\frac{1}{2}\sum_{i<j} \qty(\frac{\beta}{r_{ij}})^5).
\end{align}
where $\beta$ is the only variational parameter and has units of angstrom. An
important observation now is the lack of any single particle wave function
factor. In the case of \glspl{qd} we had a Gaussian localized at the origin
as a result of the potential well. This system, however, is infinite and
periodic without any such influence driving it towards particular points in
space. Furthermore, because of the lack of an external field the single particle
solutions are just free particles, and does not help us understand the many-body
system.

\subsection{Finite Size Dependency}

An important aspect of all the results that we will present is that they are
highly dependent on the number of particles used in the simulation box, as well
as the size of the box it self. We will hold the number density of particles
constant, $\rho = \flatfrac{\num{0.365}}{\sigma^3}$ (where $\sigma =
\SI{2.556}{\angstrom}$ as defined in \cref{eq:Lennard-Jones-def}), and set the
side lengths of the simulation box, $L$, depending on
the number of particles $N$:

\begin{align}
  L = \sqrt[3]{\frac{N}{\rho}}.
\end{align}

\noindent As the assumption of periodicity is a simplifying approximation, we
introduce some erroneous effects because of it. These generally disappear as we
increase the number of particles (and hence the size of box), but the
computation time needed to run the simulations increase significantly with
increasing numbers. The purpose of the following analysis is to test the
\emph{relative} accuracy of different wave functions. With that in mind we have
used a small number of particles in the main results, which introduces a
significant error with respect to reality. The number of particles should nevertheless
be large enough to introduce all the relevant effects and allow for a valid
comparison of methods. With the same motivation, we limit our Hamiltonian to use
only the Lennard-Jones potential (\cref{eq:Lennard-Jones-def}), and leave tests
with HFDHE2 for future research.


\subsection{Optimizing}

We have optimized~\cref{eq:McMillan-wave-function-def} using $\num{10000}$
iterations of $\num{5000}$ \gls{mc} cycles each. We used standard Metropolis sampling
along with the ADAM optimizer. \cref{fig:He-benchmark-training} shows the
progression of both the energy and variational parameter during training.
Because of the strong correlations involved, there is significantly more
variance in these results compared to the benchmark used for \glspl{qd}. We
see the value for $\beta$ oscillating without any signs of converging to a
fixed value.

\Cref{tab:He-benchmark-results} shows the energy estimates from the final model.
The same wave function (i.e. same value for $\beta$) has also been used on
different numbers of particles to illustrate how the energy increases for larger
simulation boxes. Based on computations with larger and larger systems, the
value for $N = 256$ is quite close to the apparent convergence for $N\to\infty$.

\begin{figure}[h]
  \centering
  \resizebox{\linewidth}{!}{%
    \input{scripts/He-benchmark.py.tex}
  }
  \caption[Learning progression of McMillan wave function on $^4$He]{\label{fig:He-benchmark-training}Left: Ground state energy produced by~\cref{eq:McMillan-wave-function-def}
    as a function of training steps. Right: Progression of the variational
    parameter $\beta$ of~\cref{eq:McMillan-wave-function-def} as a function of
    training steps. A convergence is clearly visible, although there is more
    variance in the value for $\beta$ compared to the parameters of the
    benchmark used for quantum dots.\citesource{writing/scripts/He-benchmark.py}}
\end{figure}

\begin{table}[h]
  \centering
  \caption[Energy estimates of McMillan wave function on
$^4$He]{\label{tab:He-benchmark-results}Predicted ground state energy of helium
atoms at density $\rho = \flatfrac{0.365}{\sigma^3}$ using $\psi_{M}$ with
$\beta$ as determined after training. The number of particles used in the
simulation box is indicated by the superscript on $\psi_M$. Values obtained
using $2^{23}$ samples. Energies in units of Kelvin per particle
$[\si{\kelvin\per N}]$. See \cref{fig:He-benchmark-training} for source code
reference.}
  \input{scripts/He-benchmark.py.table.tex}
\end{table}
\clearpage


\section{Neural Networks}

We again attempt to apply a neural network to our wave function representation.
Liquid $^4$He is a far more challenging problem compared to the two-particle
quantum dot, and as such it will serve as an important test of the capabilities
of neural networks for general quantum mechanical many-body problems.

As before, we have not simply removed the original wave function, but rather
added the network as another correlation factor:

\begin{align}
  \label{eq:helium-dnn-mcmillan-def}
  \psi_{DNN}(\mat X) &= \psi_M(\mat X)\,f(\mat X),
\end{align}
where once again $f: \mathbb{R}^{N\times D}\to\mathbb{R}$ represents an
arbitrary neural network.

Lastly, we limit our investigation to $N=32$ particles and still with density
$\rho=\flatfrac{0.365}{\sigma^3}$. This is done from a practical standpoint,
considering the substantial computational cost.

\subsection{Network Architecture}

The proposed network structure is one of many that are likely to perform well,
and in all likelihood this is not the optimal form. A more in depth analysis of
what structures perform best, including other types of networks (e.g.\ recurrent,
convolutional, residual etc.) is left for future research.

Based on the experimental results observed for quantum dots, in conjunction with
trail and error, we have used the following network structure:

\begin{center}
  \begin{tabular}{lcc}
    \toprule
    \addlinespace
    Layer & Nodes & Activation\\
    \addlinespace
    \midrule
    \addlinespace
    \addlinespace
    Input & 96 & ---\\
    Hidden 1& 144 & $\tanh$\\
    Hidden 2& 36 & $\tanh$\\
    Output & 1 & $\exp$\\
    \addlinespace
    \addlinespace
    \bottomrule
  \end{tabular}
\end{center}
Once again the number of nodes in the input layer is fixed by the
number of degrees of freedom ($N\times D$ for $D$ dimensions). The number of
nodes in the second layer is somewhat arbitrary, and could be changed.
Importantly it is larger than $96$, allowing the network to learn more features
than it has inputs. Still, it is not enough nodes to fully encode the relative
difference between every pair of coordinates, as would be required to reproduce
$\psi_M$ fully. This number was found to be large enough to allow learning, but
still be reasonably efficient. Lastly, the second hidden layer was set to $36$
nodes. This is also somewhat arbitrary, but importantly it is smaller than
$144$, allowing the gradual narrowing down to the single output. The activation
functions are the same as those used for quantum dots.

Finally, we train two instances of the network, with and without input sorting
applied to impose symmetry, denoted once again by $\psi_{DNN}$ and
$\psi_{SDNN}$, respectively.


\subsection{Optimization}

We produced the following results by optimizing $\psi_M$, $\psi_{DNN}$ and $\psi_{SDNN}$ with
similar hyperparameters. We initialized $\beta=\num{2.85}$ for all wave functions
for a fair comparison. We used $\num{250000}$ training iterations of
$\num{5000}$ \gls{mc} cycles each. Contrary to the quantum dot system, where a large
set of hyperparameters yielded good results, we have found this problem to be
highly sensitive to good hyperparameters. We had to reduce the learning rate to
$\eta=\num{0.0001}$ to avoid divergence during training. We also found that the
networks performed better with a small degree of L2 regularization,
$\gamma=\num{0.00001}$. While other choices might still be better, there were
significantly less room for error in this much more challenging problem.

\begin{figure}[h]
  \centering
  \input{scripts/He-mcmillan-dnn.py.tex}
  \caption[Learning progression of a neural network on liquid
  helium]{\label{fig:He-dnn-training}Performance of $\psi_M$, $\psi_{DNN}$ and $\psi_{SDNN}$
    as a function of training steps. The network with input sorting, $\psi_{SDNN}$ shows a clear energy reduction
    compared to the traditional benchmark, albeit not as significant as the
    results of \cref{chp:quantum-dots}. The non-symmetric network, $\psi_{DNN}$, fails
    to learn anything beyond the benchmark.\citesource{writing/scripts/He-mcmillan-dnn.py}}
\end{figure}

\cref{fig:He-dnn-training} shows a graph of the ground state energy during the
course of training for all three wave functions. On this time scale (i.e.\ $\num{25}$
times more training than used for the benchmark alone) we see $\psi_M$ converge
immediately to the same energy level as we saw in
\cref{fig:He-benchmark-training}. For $\psi_{SDNN}$, however, there is an
apparent immediate lowering of energy, followed by a gradual decrease throughout
the remaining training time. It is not clear if $\psi_{SDNN}$ has completely
converged, or if small reductions might still be achieved if we train for even
longer. Nevertheless, such gains are likely small and not necessary to
demonstrate an improvement.


In contrast to the results seen for \glspl{qd}, the network without symmetry
imposed ($\psi_{DNN}$) fails to learn anything beyond the benchmark. It appears
that for this larger system, imposing the symmetry is vitally important in
helping the networks learn the correct correlations. We hypothesize that for only two
particles the network could learn the symmetry, allowing more
flexible learning, where as in this case the inputs are too complex for the
network to learn on its own.


\Cref{tab:He-dnn-results} shows the final energies produced from $\psi_{M}$,
$\psi_{DNN}$ and $\psi_{SDNN}$ after training. As indicated by the plot, we see
a statistically significant lowering of the total energy. Unlike the results for
quantum dots, however, we do not get a lowering of the variance, but rather a
negligible increase. It is hard to say exactly how much of the total error in energy
is corrected for by the inclusion of the networks like we could for \gls{qd}, due to the lack of existing
results using $N=32$ particles with the Lennard-Jones potential with the same
potential corrections that we have included. Nevertheless, the networks continue
to show their ability to improve upon the results of all the benchmark wave
functions we have paired them with. As our main interest in this work is to
demonstrate this general ability, as opposed to producing results closest to
experimental results, we view these results as further positive encouragement to
continue this line of research.

\begin{table}[h]
  \centering
  \caption[Energy estimates using a neural network on $^4$He]{\label{tab:He-dnn-results}Predicted ground state energy of
    $\psi_M$, $\psi_{DNN}$ and $\psi_{SDNN}$ after the same amount of optimization. Results
    obtained from $2^{23}$ samples and with errors estimated using
    blocking. Energies in units of Kelvin per particle $[\si{\kelvin\per N}]$. See \cref{fig:He-dnn-training} for source code reference.}
  \input{scripts/He-mcmillan-dnn.py.table.tex}
\end{table}




\end{document}
