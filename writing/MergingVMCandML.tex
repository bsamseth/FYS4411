\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Merging Variational Monte Carlo and Machine Learning}
\label{chp:mergin-vmc-with-ml}

The purpose of \gls{vmc} is to find a wave function which minimizes the expected
energy of the system. Stated like this, \gls{vmc} clearly fits under the machine
learning umbrella as a general function optimization problem. Still, it is not a
standard supervised learning problem such as linear regression, because we don't
have a data set of expected outputs for every input. Additionally there are some
extra challenges introduced from the quantum problems themselves, such as
symmetry requirements and cusp conditions. I believe these extra challenges are
part of the reason why \gls{vmc} optimizations have traditionally been done
independently from the rest of the \gls{ml} research advancements seen in the
last few decades.

In the last few years, more and more research seems to go into applying various
techniques from the more general field of \gls{ml} into the natural sciences,
including quantum mechanics and \gls{vmc}. A notable example
is~\textcite{Carleo602}, who demonstrated that an \gls{rbm}, a particular
machine learning model, was capable of representing the wave function for some
notable systems to great effect.

Despite of these recent advancements, we argue that there still exists a
great deal of hesitancy towards using general models from machine learning for
\gls{vmc}. This chapter aims to enable the use of \emph{any} arbitrary machine
learning model, and in so doing open the door for many new applications and
advancements in the field of \gls{vmc}.


\section{Discriminative vs. Generative Models}


The \gls{rbm} used by \textcite{Carleo602} was chosen (partly) because it is a
\emph{generative} model. A distinction is often made between two general types
of \gls{ml} models: \emph{discriminative} and \emph{generative}.\footnote{See
for instance \textcite{Ng-2001} for a more in-depth discussion of the topic.} To
understand their differences, imagine we have a data set $\mathcal{D} = \{\vx_i,
\vy_i\}_{i=1}^n$ of $n$ inputs $\vx_i$ and outputs $\vy_i$. In general we want
to uncover the relationship between $\vx$ and $\vy$, and we can imagine two
different ways of doing this. We could learn:

\begin{description}
\item[$p(\vy | \vx)$]\hfill\\
  The conditional \gls{pdf} of $\vy$ given $\vx$.
\item[$p(\vx, \vy)$] \hfill\\
  The joint \gls{pdf} of inputs and outputs.
\end{description}
The most familiar option is the first one. Linear regression is an example of
such a case where we have an input and want to get the corresponding output.
Additionally, instead of considering the \gls{pdf} $p(\vy|\vx)$ we
most often just define the output to be the output with the highest probability,
$\vy = \argmax_{\vy}p(\vb{Y} = \vy | \vX = \vx)$.

Less familiar for most is the second case. Here we model both inputs and outputs
at the same time. Using Bayes rule we can get both $p(\vy|\vx)$ and $p(\vx |
\vy)$ from the joint distribution. This enables us to not only make predictions on
outputs, but also to generate inputs. This means if we train a generative model
on images of cats and dogs, it could learn to not only classify the images
correctly, but also to generate new images.

\subsubsection{Benefits of a Generative Model}

So what does this have to do with \gls{vmc}? Our ultimate
goal in \gls{vmc} is to model a probability amplitude for system configurations
$\vX\rightarrow \psi(\vX)$. We want this, in part, so that we can sample
configurations from it. So wouldn't it be nice if we could have the model
generate the configurations directly, instead of having to go through the whole
machinery of the Metropolis-Hastings algorithm and its associated downsides?

When \textcite{Carleo602} used an \gls{rbm} for their wave function, this opened the
door for new ways of sampling (specifically Gibbs sampling), playing on the
generative nature of the \gls{rbm}. Such possibilities can improve both computational
performance and accuracy of estimates obtained. It also provides a nice
conceptual link between the model and the generation of configurations.

\subsubsection{Limitations of Generative Models}

While we can in principle obtain the same conditional probabilities of
discriminative models from the joint distribution, it turns out that learning
the joint distribution can be a harder task~\cite{Ng-2001}. This means that
while the result of a generative model has the potential to be more useful, the
increased simplicity of discriminative models can lead to more accurate results.
This is the classic dilemma of whether to do one thing well, or two things
decently. Ideally we would do both, but sometimes the trade off in accuracy is
significant.

Second, and arguably more importantly, limiting ourselves to generative models
is -- well, limiting. A vast pool of potential models are discriminative,
including the neural networks from \cref{sec:artificial-neural-networks}. Given
that we have the Metropolis-Hastings algorithm which works excellently in
most situations, it seems unnecessary to completely disregard so many options.
Part of the intent of this entire thesis is to illustrate how discriminative
models can be used effectively for \gls{vmc} purposes.


\section{Arbitrary Models as Trial Wave Functions}
\label{sec:arbitrary-models-as-trial-wave-functions}

Suppose we propose some arbitrary wave function $\psialpha(\vX)$, parameterized
by an arbitrary number of parameters $\valpha$. Subject to the
requirements on wave functions from \cref{sec:requirements-of-wave-functions},
there are a few things we need from this function in order to successfully run a
\gls{vmc} optimization. The complete list of required operations is as
follows:\footnote{These operations are rooted in the assumption that we are
  working with coordinates as the system configuration specification, and where
  the kinetic energy is part of the energy. We acknowledge that there are a
  whole class of Hamiltonians for which this is not suitable. In these cases,
  the required operations will likely need some changes. Nevertheless, the overarching
  procedure and techniques discussed should be agnostic to such changes.}

\begin{description}
\item[Evaluation:]\hfill\\
  Given a configuration $\vX$, produce a scalar value $\psialpha(\vX)$
\item[Gradient w.r.t. parameters:]\hfill\\
  In order to compute the cost function gradient, we need the following
  quantity:
  \begin{align}
    \label{eq:ml-vmc-operation-gradient}
    \frac{1}{\psialpha(\vX)}\grad_{\valpha}{\psialpha(\vX)} = \grad_{\valpha}{\ln\psialpha(\vX)}.
  \end{align}
\item[Gradient w.r.t. inputs:](optional)\hfill\\
  For use with \gls{is}, we need to compute the drift force
  associated with the wave function (for particle $k$):
  \begin{align}
    \label{eq:ml-vmc-operation-drfit-force}
    \frac{2}{\psialpha(\vX)}\grad_k{\psialpha(\vX)} = 2 \grad_k{\ln\psialpha(\vX)}.
  \end{align}
  Note that this is only strictly needed when \gls{is} is used, and
  can be omitted.
\item[Laplacian w.r.t. inputs:]\hfill\\
  For use with any Hamiltonian which includes kinetic energy, we need to compute
  the Laplacian of the wave function with respect to the configuration $\vX$.
  \begin{align}
    \label{eq:ml-vmc-operation-laplacian}
    \sum_i\frac{1}{\psialpha(\vX)}\laplacian_i\psialpha(\vX).
  \end{align}
\end{description}

Any function that supports these four operations can be used as a trial wave
function. We should of course take care to choose functions that satisfy the
standard requirements from \cref{sec:requirements-of-wave-functions}, but also
to choose a model which is likely to be a good candidate.

\subsection{Artificial Neural Networks as Trial Wave Functions}

\glspl{ann} give us an incredibly flexible way to define computationally
expressive models, and to easily adapt them by changing their architecture. For
a long time, (human) theorists have hand crafted Jastrow factors,
resulting in (good) trial wave functions rooted in the physics of the system.
The problem is that hand crafting only goes so far, and we reach the limits of
what we can construct and reason about analytically. Enter now a way to
experiment and play with complex models, in a flexible manner. Using ANNs could
enable us to prototype wave functions more rapidly, as well as possibly increase
the accuracy of the resulting simulations.

Convinced this is a great idea, we have some more working out to do. While
evaluating the output of the network is easy enough (see last chapter), we need
to derive the general expression for the other quantities we need.\\

In the following, let $\psi(\vx)$ be an artificial neural network which has the
quantum numbers of every particle as its inputs. $\vx$ is the concatenation of
all $\{\vx_i\}_{i=1}^N$ for $N$ particles. For our case, that means each
particle coordinate is one input to the network, and the output of the network
is the value of wave function.

\subsubsection{Gradient w.r.t. Parameters}

Back in \cref{sec:ml-backprop} we derived the formulas for the gradient of a
general cost function, with respect to the weights and biases of any network. In
the case of \gls{vmc}, we need the quantity given in
\cref{eq:ml-vmc-operation-gradient}. We can get this trivially through the
backpropagation algorithm by defining the cost function to be the output of the
network:

\begin{align}
  \mathcal{C}(\psi(\vx)) = \psi(\vx).
\end{align}
Computing the ``cost function'' gradient using backpropagation, we simply divide
by $\psi(\vx)$ to get the quantity in \cref{eq:ml-vmc-operation-gradient}:

\begin{align}
    \frac{1}{\psi(\vX)}\grad_{\valpha}{\psi(\vX)} = \frac{1}{\psi(\vx)}\qty[\grad_{\valpha}{\mathcal{C}(f(\vx))}].
\end{align}

\subsubsection{First Order Partial Derivative w.r.t. Inputs}

\Cref{eq:ml-vmc-operation-drfit-force} requires us to compute the first order
derivatives of the networks output w.r.t. its individual inputs. With reference
to the notation used in \cref{sec:artificial-neural-networks}, we get:

\begin{align}
  \pdv{a_k^{(l)}}{x_j} &= \pdv{\sigma^{(l)}\qty(z_k^{(l)})}{z_i^{(l)}}\pdv{z_i^{(l)}}{x_j}\\
  &= \dot\sigma^{(l)}\qty(z_k^{(l)})\pdv{z_k^{(l)}}{x_j}\\
  &= \dot\sigma^{(l)}\qty(z_k^{(l)})\pdv{}{x_j}\qty(W_{ki}^{(l)}a_i^{(l-1)} + b^{(l)}_k)\\
  &= \dot\sigma^{(l)}\qty(z_k^{(l)})\qty(W^{(l)}_{ki}\pdv{a_i^{(l-1)}}{x_j})\label{eq:first-order-partial-nn},\\
  \pdv{a_k^{(0)}}{x_j} &= \pdv{x_k}{x_j} = \delta_{kj}.
\end{align}
Once again we divide by $\psi$ (and multiply by 2) to get the values needed in
\cref{eq:ml-vmc-operation-drfit-force}.

\subsubsection{Second Order Partial Derivatives w.r.t. Inputs}

For \cref{eq:ml-vmc-operation-laplacian} we also need the second order
derivatives. Continuing from \cref{eq:first-order-partial-nn} we get:

\begin{align}
  \pdv[2]{a_k^{(l)}}{x_j} &= \dot\sigma^{(l)}\qty(z_k^{(l)})W_{ki}^{(l)}\pdv[2]{a_i^{(l-1)}}{x_j} + \pdv{\dot\sigma^{(l)}\qty(z_k^{(l)})}{x_j}W_{ki}^{(l)}\pdv{a_i^{(l-1)}}{x_j}\\
  &= \dot\sigma^{(l)}\qty(z_k^{(l)})W_{ki}^{(l)}\pdv[2]{a_i^{(l-1)}}{x_j} + \ddot\sigma^{(l)}\qty(z_k^{(l)})\pdv{z_k^{(l)}}{x_j}W_{ki}^{(l)}\pdv{a_i^{(l-1)}}{x_j}\\
                          &= \dot\sigma^{(l)}\qty(z_k^{(l)})W_{ki}^{(l)}\pdv[2]{a_i^{(l-1)}}{x_j} + \ddot\sigma^{(l)}\qty(z_k^{(l)})\qty(W_{ki}^{(l)}\pdv{a_i^{(l-1)}}{x_j})^2,\\
                            \pdv[2]{a_k^{(0)}}{x_j} &= \pdv[2]{x_k}{x_j} = 0.
\end{align}
Dividing by $\psi$ yields the desired result.


\subsection{Imposing Symmetry Requirements}

We know from \cref{sec:requirements-of-wave-functions} that we have certain
requirements which need to be met in order for the wave function to be sensible.
Assuming we pick a model that is suitably smooth and continuous, the most
challenging requirement is that of particle exchange symmetry. As discussed in
\cref{sec:slater-jastrow}, we typically want to design our wave function
candidates to be symmetric, and add a Slater determinant for fermions if need
be. The problem is that arbitrary functions are not likely to be symmetric to the
interchange of its inputs. Neural networks are intrinsically not symmetric.

We present some different strategies of imposing such a symmetry:

\subsubsection{Learn the Symmetry}

The simplest strategy is perhaps the most appealing, as we simply let the model
realize on its own that  it is best to be symmetric. After all we know the ideal
wave function is, so an ideally trained model \emph{should} be able to learn
this.

In practice this might be too much to ask, and the model might settle on a
non-symmetric local minimum. Additionally, one might argue that the symmetry
requirement \emph{must} be satisfied at all times during training for the
results to be physical. This will depend on the particular system we investigate.


\subsubsection{Pool the Output}

We can draw inspiration from the world of \glspl{cnn}, and obtain the symmetry by
way of a pooling operation. Let $F(\vX)$ be the final model output and let
$f(\vX)$ be the originally proposed model.

The simplest version is to let $F$ be the sum of $f$ applied to every
permutation of $\vX$. We could use the product instead of the sum, or any other
commutative binary operator. While this would fix the symmetry, it would have a
complexity of $\mathcal{O}(N!)$ for $N$ particles.

A computationally better idea is to let $f$ be a function of only two particles,
$f(\vx_i, \vx_j)$, and then let $F$ be the sum of all possible permutations of
two particles out of the $N$ available:

\begin{align}
  \label{eq:sum-pooling}
  F(\vX) &= \sum_{i=1}^N\sum_{j\neq i}^N f(\vx_i, \vx_j)\qq{(Sum Pooled)}.
\end{align}
This has complexity $\mathcal{O}(N^2)$, but limits us to learn
two-body interactions. Three-body interactions could be included with another
sum, but would increase the complexity accordingly. For problems which require higher
order correlations this approach might not be suitable.

\subsubsection{Sort the Inputs}
\label{sec:sorting-inputs-symmetry}

Perhaps the most computationally efficient way to impose the symmetry is to
simply transform the inputs in a symmetric manner before passing them on to the
model. An intuitive choice is to sort the inputs in ascending order of ``size''.
Importantly we should sort by particle, and not sort all coordinates
independently. One way to do this is to sort the particles by their absolute
distance from the origin. This approach has the benefit of a much improved
complexity of $\mathcal{O}(N\log N)$. Additionally, we might
imagine that ordering the particles by some extrinsic metric might help the
model extract useful information more easily.\footnote{Consider a Hamiltonian
  that depends strongly on the closest/furthest distance of any particle to the
  origin. Any model will quickly be able to pick up on the fact that the
  first/last input is highly correlated with the energy. If these particles
  could be at any input, understanding this connection could be more
  challenging.}\\

\end{document}

