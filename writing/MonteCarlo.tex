\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Monte Carlo Methods}
\label{chp:monte-carlo}

In \autoref{chp:variational-monte-carlo} we found that we needed a way to
sample system configurations $\vX$ from the probability distribution  described
by the wave function. We needed this as a means to evaluate integrals. Specifically, we needed to evaluate the expectation value of the Hamiltonian w.r.t. a given wave function $\psialpha$:

\begin{align}
    \expval{\hat H} &= \frac{\expval{\hat H}{\psialpha}}{\braket{\psialpha}} = \int\dd{\vX} P_{\psi_{\valpha}} (\vX) E_L(\vX)\label{eq:chp-mc-anon-1}\\
    &\approx \frac{1}{N} \sum_{i = 1}^N E_L(\vX_i)
    \qq{where} \vb X_i\sim P_{\psi_{\valpha}}
\end{align}
where $N$ is set sufficiently large to satisfy the required accuracy. Due to
the central nature of this method, we will start by a proper presentation of
Monte Carlo Integration as it is used here, followed the details of how to
obtain the samples we need.

\section{Monte Carlo Integration}

Monte Carlo Integration (MCI) is a general technique for numeric evaluation of any arbitrary integral. In general it concerns evaluating any multidimensional definite integral, written as

\begin{align}
    I = \idotsint_\sigma\dd{\vx} f(\vx)
\end{align}
where $\sigma$ denotes a subset of $\mathbb{R}^m$, with a $m$-dimensional volume given by
\begin{align}
    V = \idotsint_\sigma\dd{\vx}
\end{align}
In its simplest form, MCI samples $N$ \emph{uniformly} distributed points
$\vx_1,\dots,\vx_N$ from $\sigma$, and uses a Riemann sum formulation of the integral $I$:
\begin{align}
    I = \lim_{N\to\infty} M_N \defeq \lim_{N\to\infty}\sum_{i=1}^N f(\vx_i) \frac{V}{N} = V\expval{f(\vx)}_\sigma
\end{align}
where $\expval{\cdot}_\sigma$ denotes the expectation value over all points in $\sigma$. For finite $N$ we may obtain estimates of the error we make in approximating the integral. First, we estimate the variance of the integrand:
\begin{align}
    \Var[f(\vx)] \defeq \sigma^2_N = \frac{1}{N-1} \sum_{i=1}^N\qty(f(\vx_i) - \frac{1}{N}\sum_{i=j}^N f(\vx_j))^2
\end{align}
It then follows, by the properties of variance,
\begin{align}
    \Var[M_N] = \Var\qty[ \frac{V}{N} \sum_{i=1}^Nf(\vx_i)] = \frac{V^2}{N^2}\sum_{i=1}^N\Var[f(\vx_i)] = \frac{V^2\sigma^2_N}{N}
\end{align}
\begin{align}
    \implies \Std[M_N] = \sqrt{\Var[M_N]} = \frac{V\sigma_N}{\sqrt N}
\end{align}
This tells us that the expected statistical error we make goes like
$\mathcal{O}(1 / \sqrt N)$, and depends linearly on the size of the volume.
This illustrates both the advantage, and disadvantage of Monte Carlo
integration compared to other, deterministic integration methods. Its advantage
is its simple dependency on the volume, and its independence from the
particular number of dimensions in the integral. Other methods tend to depend
exponentially on the dimensionality, and as such MCI is often the best choice
for multidimensional integrals. Its disadvantage is the relatively slow
convergence rate, which is asymptotically much worse than other
approaches~\cite{Numerical-Recipes-Press-et-al}.

\subsection{Importance Sampling}

In some suitable cases we can improve quite dramatically on the simple,
straightforward approach presented above. To illustrate this, say we
would like to evaluate the following integral:

\begin{align}
    \label{eq:mci-importance-example-func}
    I = \intfy\dd{x}f(x)=\intfy\dd{x} \frac{\exp{-\flatfrac{x^2}{2} }}{\sqrt{2\pi \qty(1 +x^2)}}  = \num{0.78964}.
\end{align}

\begin{figure}
    \centering
    \resizebox{0.8\linewidth}{!}{%
    \begin{tikzpicture}
        \begin{axis}[every axis plot post/.append style={
                mark=none,domain=-5:5,samples=50,smooth}, % All plots: from -2:2, 50 samples, smooth, no marks
            axis x line=bottom, % no box around the plot, only x and y axis
            axis y line=left,
            enlargelimits=upper] % extend the axes a bit to the right and top
            \addplot[semithick, color0] {1/(sqrt(2*pi))*exp(-(x^2)/2)*(1+x^2)^(-0.5)};
            \addplot[semithick, color1] {1/(sqrt(2*pi))*exp(-(x^2)/2)};
            \addlegendentry{$f(x)$}
            \addlegendentry{$\mathcal{N}(0, 1)$}
        \end{axis}
    \end{tikzpicture}
    }
    \caption{\label{fig:mci-importance-example-func-plot}Plot of the function in \autoref{eq:mci-importance-example-func}}
\end{figure}
The integrand is plotted in \autoref{fig:mci-importance-example-func-plot}, and
the observant reader might recognise this as the product of a normal
distribution and a Student-t distribution. The correct value for the integral
is also given, so we have a reference for the results.

For comparison, lets start with the straightforward approach from before.
Because the integral goes to infinity the "volume" $V$ would not be well
defined, and we are forced to truncate the region manually. From looking at the
graph in \autoref{fig:mci-importance-example-func-plot} we may say that $x\in
[-5, 5]$ should account for the vast majority of the total integral. That means
we use the following estimate:

\begin{align}
    \label{eq:chp-mc-anon-2}
    I\approx \frac{10}{N}\sum_{i=1}^Nf(x_i)\qq{where} x_i\sim\text{Uniform}(-5, 5)
\end{align}

The main issue with this approach is that 1) we need to truncate the integral
manually and 2) now matter where we place the box boundaries we will tend to
sample a lot of $x_i$'s in areas where $f$ gives very small contributions to
the integral. Ideally we would like our sample points to be distributed as
closely as possible to $f$, in order to capture as much information as we can.
This is the idea of importance sampling. Instead of using the uniform
distribution for sampling, we use some probability distribution which more
closely resembles the integrand, call it $g(x)$. Formally we then restate the integral as follows:

\begin{align}
    I = \intfy\dd{x}f(x) = \intfy\dd{x} \frac{f(x)}{g(x)} g(x)
\end{align}
This can be interpreted as the expectation of $z(x)=\flatfrac{f(x)}{g(x)}$ for $x$'s drawn from $g$, and so the corresponding estimation is then:

\begin{align}
    I \approx \frac{1}{N} \sum_{i=1}^N \frac{f(x_i)}{g(x_i)} \qq{where} x_i\sim g
\end{align}
Setting $g = \text{Uniform}(-5, 5)$ recovers \autoref{eq:chp-mc-anon-2}, so this is simply the natural generalization of the standard approach for an arbitrary distribution function $g$.


Going back to the example, we should now chose a distribution function that
closely resembles the integrand, while still being simple to sample from. In
this (contrived) example, a natural choice is to use the standard normal
distribution, $g = \mathcal{N}(0, 1)$. In
\autoref{fig:mci-importance-example-func-plot} we can see how $g(x)$ is
enclosing $f(x)$ much more tightly than any rectangular box might hope to.

\begin{figure}
   \centering
    \resizebox{\linewidth}{!}{%
        \input{scripts/monte_carlo_int_example.py.tex}
    }
    \caption{\label{fig:mci-importance-example-func-convergence}Convergence of
    the integral in \autoref{eq:mci-importance-example-func}, using regular
    Monte Carlo integration and Importance Sampling with $g = \mathcal{N}(0,
    1)$. The points indicate the approximated value of the integral for each
    particular experiment, and the shaded areas indicate the corresponding
    $\SI{95}{\percent}$ confidence intervals for the expected value. We see the
    latter displaying both greater accuracy and tighter confidence intervals.
    The source code for this graphic can be found~\cite[TODO: Add
    path]{MS-thesis-repository}, and \LaTeX{} output generated
    by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

\autoref{fig:mci-importance-example-func-convergence} show the convergence of
the Monte Carlo approximations towards the correct value for an increasing
number of sampled points. The drawn line is the mean value at each run, while
the shaded areas show the corresponding $\SI{95}{\percent}$ confidence
intervals. Because of the more suited sampling distribution, we obtain results
which are more accurate, and perhaps most importantly, tighter confidence
bounds. In general, using better sampling distributions will tend to give us
lower variance results.

\section{Sampling from Arbitrary Probability Distribution Functions}

So far we have taken for granted the ability to sample numbers from various
probability distribution functions. We dedicate this section to discuss sampling from the bottom up, culminating in a detailed presentation of the most central algorithm in this thesis, the Metropolis-Hastings algorithm.

\subsection{The Uniform Distribution}

The basic building block upon which we shall build all other random sampling
techniques is the ability to sample random numbers from the uniform
distribution, $\text{Uniform}(a, b)$. This is the simplest distribution
possible, where all numbers in the range $[a, b]$ have the same probability
density. The definition of its PDF is simply

\begin{align}\label{eq:uniform-pdf-definition}
    p_U(x\,|\, a, b) \defeq  \begin{cases}
        \frac{1}{b - a} &\qfor x\in [a, b]\\
        0&\qotherwise
    \end{cases}.
\end{align}
Most commonly we operate only with the \emph{standard uniform distribution}, $\text{Uniform}(0, 1)$. Any other range can be simply related via the trivially verifiable identity
\begin{align}
    p_U(x\,|\, a,b) = p_U\qty(\left. \frac{x-a}{b-a} \,\right|\, 0, 1).
\end{align}

But how exactly do we obtain random realizations from this PDF? Firstly we need
to depart from the typical notion of random, and realize that we are unable to
write an algorithm which produces a truly random result. While we could in
principle rely on nature to provide sources of complete, true randomness (so as
the exact behaviour of a quantum mechanical obserable), this is impractical
when we need fast, on-demand samples. Instead we settle for
\emph{pseudo-random} samples. A pseudo-random sequence $x_1, x_2,\dots x_n$
implies that one cannot\footnote{In any practical way, barring exhaustive trial
and error of every conceivable underlying algorithm.} predict $x_{n+1}$ without
insight into how the sequence is generated. In other words, the (ideal)
pseudo-random sequence is indistinguishable from a truly random sequence for
anyone observing the numbers $x_i$ alone.

There are a multitude of algorithms that can generate a sequence of
pseudo-random \emph{integers}, with a varying properties\footnote{Such properties
could include range of possible numbers, period, level of bias and how
cryptographically secure they are.}. One of oldest and most common family of
such algorithms is a linear congruential generator (LCG)~\cite{Knuth-1997-ACP-270146}. It defines a pseudo-random sequence by the simple recurrence relation
\begin{align}
    \label{eq:linear-congruential-generator-relation}
    x_{n+1} = (ax_n + c)\mod m,
\end{align}
where $a, c$ and $m$ are constants which determine the behaviour of the
algorithm, and $x_0$ needs to be specified manually. For instance,
\textcite{Numerical-Recipes-Press-et-al} use $a = 1664525$, $c=1013904223$ and
$m=2^{32}$.

This produces uniformly distributed integers in the range $[0, m)$. Numbers
from the standard uniform distribution can then simply be obtained by dividing
the integers by $m$. The complete algorithm is shown in Algorithm \autoref{alg:uniform-LCG-sampling}.

\begin{algorithm}[h]
    \caption{Sampling from $\text{Uniform}(l, u)$}
    \label{alg:uniform-LCG-sampling}
    \begin{algorithmic}[1]
        \Require $a, c, m$ and $x_0$
        \Function{unif}{lower bound $l$, upper bound $u$}
            \State $x\gets x_0$
            \Repeat
                \State $x\gets (ax + c)\mod m$
                \State \Yield $(\flatfrac{x}{m}) \times (u - l) + l$
            \Until{done}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Inverse Transform Sampling}

Armed with uniformly distributed random numbers, we now turn towards generating
numbers from other PDFs. Let $U\sim\text{Uniform}(0,1)$ be a uniformly
distributed stochastic variable, and let $X$ be a stochastic variable
associated with a PDF, $p$, and a corresponding cumulative distribution
function (CDF), $F$, i.e.

\begin{align}
    F(x) = \text{Pr}\qty(X\leq x) = \int_{-\infty}^x \dd{t}p(t).
\end{align}
We would like to define a transformation $T:[0, 1]\mapsto\mathbb{R}$ that can map from uniform numbers to numbers that follow the given CDF, i.e. define $T$ such that $T(U)\disteq X$. We have:
\begin{align}
    F(x) = \text{Pr}\qty(X\leq x) = \text{Pr}\qty(T(U)\leq x) = \text{Pr}\qty(U \leq T^{-1}(x)) = T^{-1}(x),
\end{align}
because $T^{-1}(x)\in[0, 1]$ by definition, and assuming that $T^{-1}(x)$ is strictly monotone. It follows then that $F^{-1}(U)\disteq X$, i.e. if the CDF is strictly monotone (as CDFs should be) then we can get realizations of $X$ by applying $F^{-1}$ to realizations of $U$.

The algorithm to sample from any PDF with a tractable inverse cumulative
distribution function is then extremely simple, and for completeness it is
listed in Algorithm \autoref{alg:inverse-transform-sampling}.

\begin{algorithm}[h]
    \caption{Inverse Transform Sampling}
    \label{alg:inverse-transform-sampling}
    \begin{algorithmic}[1]
        \Require Cumulative distribution function $F$
        \Ensure Random $x$ with CDF equal to $F$
        \Repeat
          \State \Yield $F^{-1}(\text{unif}(0, 1))$
        \Until{done}
    \end{algorithmic}
\end{algorithm}

\subsection{The Metropolis-Hastings Algorithm}

Algorithm \autoref{alg:inverse-transform-sampling} works great for a number of standard
PDFs. For many cases, however, we do not have a tractable form for the inverse
CDF, which renders the algorithm useless. So what about the wave functions we
are interested in? The PDF in question is, as defined in \autoref{eq:wavefunc-probability-density-def},

\begin{align}
    P_{\psi}(\vX) \defeq \frac{\abs{\psi}^2}{\int\dd{\vX}\abs{\psi}^2}.
\end{align}
Sadly, computing $F^{-1}$ for this PDF, if even possible, would be a very costly
operation. In fact, the normalization integral in the denominator is enough to
make practical sampling from $P_\psi$ impossible. Even worse, considering that
$\psi$ is updated continuously throughout a VMC calculation, we could not even
cache the result of the integral after computing it once.

Luckily, we have a solution: The Metropolis-Hastings Algorithm. While the
algorithm it self is quite simple, the argument for \emph{why} it works is a
little more involved. The following sections will be dedicated to the mathematical
underpinnings. In the mean time, the full algorithm is shown in~\Cref{alg:metropolis-hastings-general}.

\begin{algorithm}[h]
    \caption{Metropolis-Hastings Algorithm}
    \label{alg:metropolis-hastings-general}
    \begin{algorithmic}[1]
        \Require Probability density $P(\vx)$, proposal distribution $q(\vx'|\vx)$
        \Ensure Random $\vx$ drawn from $P$
        \State Initialize $\vx$, randomly or otherwise
        \Repeat
          \State Sample $\vx'\sim q(\vx'|\vx)$
          \State $u\gets\text{unif}(0, 1)$
          \State $A\gets P(\vx')q(\vx|\vx')\qty[P(\vx)q(\vx'|\vx)]^{-1}$
          \If{$u \leq A$}
            \State $\vx \gets \vx'$
          \EndIf
          \State \Yield $\vx$
        \Until{done}
    \end{algorithmic}
\end{algorithm}

The most important thing about~\Cref{alg:metropolis-hastings-general} is the
fact that we only need to know the ratios of probabilities. This means that we
don't need the probabilities to be normalized to unity\footnote{We do need the
probabilities to be \emph{normalizable} though. In our case, where $P\propto
\abs{\Psi}^2$, we have already assumed this requirement
in~\cref{sec:requirements-of-wave-functions}.}, meaning we don't have to compute
the costly integral in~\cref{eq:wavefunc-probability-density-def}. For our
purposes this is essential, and~\cref{alg:metropolis-hastings-general} is the
reason why VMC is possible.

\subsubsection{Markov Chains}

The Metropolis-Hastings algorithm builds what is called a \emph{Markov Chain}. A
Markov Chain is a type of stochastic model that describes a sequence of possible
states. We imagine some finite space of possible states\footnote{The states can
either be enumerable in space and continuous in time, continuous in space and
enumerable in time (our most common case), or enumerable in both.
Generalizations to continuous space and time are possible, but beyond the scope
of this thesis.} and a set of probabilities describing the likelihood of moving
from one to the other. The defining property of a Markov Chain, as opposed to
other stochastic processes is that the transition probabilities $q(x'|x)$ from a
state $x$ to a state $x'$ depends only on the current state and not on any
previous states.

For well behaved Markov Chains, as the number of steps in the chain increases,
the distribution of the states will asymptotically approach a stationary
distribution, call it $\pi(x)$. By well behaved we mean that the Markov Chain
described by $q(x'|x)$ must have a \emph{unique}



\end{document}
