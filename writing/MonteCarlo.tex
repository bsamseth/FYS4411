\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Monte Carlo Methods}
\label{chp:monte-carlo}

In \autoref{chp:variational-monte-carlo} we found that we needed a way to sample
system configurations $\vX$ from the probability distribution described by the
wave function. We needed this as a means to evaluate integrals. Specifically, we
needed to evaluate the expectation value of the Hamiltonian w.r.t. a given wave
function $\psialpha$:

\begin{align}
    \expval{\hat H} &= \frac{\expval{\hat H}{\psialpha}}{\braket{\psialpha}} = \int\dd{\vX} P_{\psi_{\valpha}} (\vX) E_L(\vX)\label{eq:chp-mc-anon-1}\\
    &\approx \frac{1}{N} \sum_{i = 1}^N E_L(\vX_i)
    \qq{where} \vb X_i\sim P_{\psi_{\valpha}}
\end{align}
where $N$ is set sufficiently large to satisfy the required accuracy. Due to
the central nature of this method, we will start by a proper presentation of
Monte Carlo Integration as it is used here, followed the details of how to
obtain the samples we need.

\section{Monte Carlo Integration}

Monte Carlo Integration (MCI) is a general technique for numeric evaluation of
any arbitrary integral. In general it concerns evaluating any multidimensional
definite integral, written as

\begin{align}
    I = \idotsint_\sigma\dd{\vx} f(\vx)
\end{align}
where $\sigma$ denotes a subset of $\mathbb{R}^m$, with a $m$-dimensional volume given by
\begin{align}
    V = \idotsint_\sigma\dd{\vx}
\end{align}
In its simplest form, MCI works by sampling $N$ \emph{uniformly} distributed points
$\vx_1,\dots,\vx_N$ from $\sigma$, and uses a Riemann sum formulation of the integral $I$:
\begin{align}
  \label{eq:mci-def}
    I = \lim_{N\to\infty} M_N \defeq \lim_{N\to\infty}\sum_{i=1}^N f(\vx_i) \frac{V}{N} = V\expval{f(\vx)}_\sigma
\end{align}
where $\expval{\cdot}_\sigma$ denotes the expectation value over all points in
$\sigma$ when all points are equally likely. The estimate becomes increasingly
accurate for increasing $N$, and is exact in the limit where $N\to\infty$.

\subsection{Estimating the Error}

For any finite $N$ the result of using MCI will not be exact. Because of this,
it is immensely useful to be able to assign a statistical certainty to the
result. For instance, say we estimate the ground state energy of a system using
two different wave functions. We perform the integrals and get (to the first
five digits) $\expval{H}_A = \SI{1.0315}{eV}$ and $\expval{H}_B =
\SI{1.0943}{eV}$. Recalling that the lowest energy is the most physically
accurate, can we say that $A$ is better than $B$? As stated, these results are
borderline meaningless, because we lack information of the precision of the
integrals. Assume that in reality, the numbers were
$\expval{H}_A=\SI{1.03(8)}{eV}$ and $\expval{H}_B=\SI{1.09(4)}{eV}$, where the
numbers in parenthesis indicate the uncertainty in the last digit. Now we know
that the two numbers are not different in a statistically significant way, and
any difference could be entire random. We would need to use more sample points
to lower the uncertainty to be able to say anything more about them.\\

We obtain a statistical estimate of the error we make in approximating the
integral by considering the variability of the individual terms of
\cref{eq:mci-def}. Let's start by estimating the variance of the integrand:
\begin{align}
    \Var[f(\vx)] \defeq \sigma^2_N = \frac{1}{N-1} \sum_{i=1}^N\qty(f(\vx_i) - \frac{1}{N}\sum_{i=j}^N f(\vx_j))^2
\end{align}
It then follows, by the properties of variance,
\begin{align}
  \label{eq:mci-annon-2}
    \Var[M_N] = \Var\qty[ \frac{V}{N} \sum_{i=1}^Nf(\vx_i)] = \frac{V^2}{N^2}\sum_{i=1}^N\Var[f(\vx_i)] = \frac{V^2\sigma^2_N}{N}
\end{align}
\begin{align}
  \label{eq:mci-sem-def}
    \implies \Std[M_N] = \sqrt{\Var[M_N]} = \frac{V\sigma_N}{\sqrt N}
\end{align}
\cref{eq:mci-sem-def} is the standard deviation of the sample mean, otherwise
known as the \emph{standard error of the mean}. It is common to use this
directly as the uncertainty of the expectation values.

This tells us that the expected statistical error we make when using MCI goes
like $\mathcal{O}(1 / \sqrt N)$, and depends linearly on the size of the volume
and standard deviation of the integrand it self. This illustrates both the
advantage, and disadvantage of MCI compared to other,
deterministic integration methods. Its advantage is its simple dependency on the
volume, and its independence from the particular number of dimensions in the
integral. Other methods tend to depend exponentially on the dimensionality, and
as such MCI is often the best choice for multidimensional integrals. Its
disadvantage is the relatively slow convergence rate, which is asymptotically
much worse than other approaches~\cite{Numerical-Recipes-Press-et-al}.

\subsubsection{Correction for Autocorrelation}

Actually, \cref{eq:mci-annon-2} includes a mistake. We applied a property of
variance on a sum of random of random variables. The equation is correct if and
only if the samples $x_i$ are perfectly independent. Unfortunately for us,
the samples will not always be so, depending on the algorithm used to generate
them. We can sample independently from a uniform distribution and some other
notable distributions (\cref{sec:sampling-arb-prob-dist-funcs}), but not in
general. This will be especially true for the algorithms we use to sample from
arbitrary wave functions (\cref{sec:metro-hastings-alg}).

The version of \cref{eq:mci-annon-2} that is true in general accounts for
autocorrelated samples:\footnote{Covariance is a measure of the joint
  variability between two random variables. Autocorrelation is used to refer to
  the covariance between a random variable and itself at pairs of time points.}


\begin{align}
  \label{eq:mci-annon-3}
    \Var[M_N] = \Var\qty[ \frac{V}{N} \sum_{i=1}^Nf(\vx_i)] = \frac{V^2}{N^2}\sum_{i=1}^N\sum_{j=1}^N\Cov[f(\vx_i), f(\vx_j)].
\end{align}
This is always greater than or equal to \cref{eq:mci-annon-2}, and equal only if
the off-diagonal covariance terms (where $i\neq j$) are all zero. Not accounting
for the autocorrelation in the samples will therefore lead us to underestimate
the error, which is arguably much worse than the opposite.

There is, unfortunately a heavy cost to computing the full covariance.
Typically, to keep the expectation values accurate, we will use very large
values for $N$, often on the order of tens or hundreds of millions. The double loop in
\cref{eq:mci-annon-3} will then quickly become unfeasible to compute, with
trillions or quadrillions of iterations. In practice we need to estimate it.
Yes, we are going to estimate the estimate of the error we make in an estimate. 

The strategy we shall use for all later error analysis is called Blocking,
specifically the article by \textcite{Jonsson-2018}. Say we have a sequence of
sample value $f(x_i)$:

\begin{align}
  D_0 = \mqty[f(x_1) & f(x_2) & \dots & f(x_N)],
\end{align}
and let's assume that there is some non-zero covariance between samples that are
sufficiently close in the sequence. The idea of Blocking is to group
\say{blocks} of samples together, replacing them by their mean value. For
instance, after one blocking transformation of the above sequence we get:

\begin{align}
  D_1 = \mqty[\qty(\frac{f(x_1) + f(x_2)}{2}) & \qty(\frac{f(x_3) + f(x_4)}{2}) & \dots & \qty(\frac{f(x_{n-1}) + f(x_n)}{2})]
\end{align}
If $N = 2^d$ with $d > 1$ we can repeat the process to obtain $d$ different
sequences $D_i$. These transformations conserve the value of \cref{eq:mci-annon-3}
while decreasing the covariance between samples~\cite{Jonsson-2018}. That implies that measuring
the variance with \cref{eq:mci-annon-2} on each $D_i$ will yield greater and
greater errors, until there is no more covariance and the error estimate
converges. 

\cref{fig:blocking-example-diagram} shows this process in practice. The example
data is $2^{27}$ samples of the mean radial displacement of a single
one-dimensional particle placed in a harmonic oscillator. Due to how the data is
generated, we expect there to be a significant portion of autocorrelation
present. The figure shows the standard error from \cref{eq:mci-annon-2},
calculated on the data sets obtained by performing repeated blocking
transformations. We can see it rise steadily up until the block sizes are big
enough to remove all the covariance, at which point the estimate converges. As
the data set becomes to small the results start to become unpredictable. The
optimal estimate is indicated by the red circle, as determined by the automated
procedure developed by \textcite{Jonsson-2018}, which in this case occured after
$\num{12}$ blocking transformations.

If not explicitly stated otherwise, all error estimates in this thesis will
include this correction.


\begin{figure}[h]
  \centering
  \resizebox{0.7\linewidth}{!}{%
    \input{scripts/blocking-example-diagram.py.tex}
  }
  \caption{The estimated standard error of a random process excibiting symptoms
    of autocorrelation, shown as a function of the effective size of the data set as
    blocking transformations are applied. As the block sizes increase the 
    covariance goes away, while the variance increases. After convergence, the
    optimal estimate is illustrated by the red dot, as determined by an automated blocking
    procedure~\cite{Jonsson-2018}.}
  \label{fig:blocking-example-diagram}
\end{figure}



\subsection{Importance Sampling}

In some suitable cases we can improve quite dramatically on the simple,
straightforward integration approach given in \cref{eq:mci-def}. To illustrate this, say we
would like to evaluate the following integral:

\begin{align}
    \label{eq:mci-importance-example-func}
    I = \intfy\dd{x}f(x)=\intfy\dd{x} \frac{\exp{-\flatfrac{x^2}{2} }}{\sqrt{2\pi \qty(1 +x^2)}}  = \num{0.78964}.
\end{align}

\begin{figure}
    \centering
    \resizebox{0.7\linewidth}{!}{%
    \begin{tikzpicture}
        \begin{axis}[every axis plot post/.append style={
                mark=none,domain=-5:5,samples=50,smooth}, % All plots: from -2:2, 50 samples, smooth, no marks
            axis x line=bottom, % no box around the plot, only x and y axis
            axis y line=left,
            enlargelimits=upper] % extend the axes a bit to the right and top
            \addplot[semithick, color0] {1/(sqrt(2*pi))*exp(-(x^2)/2)*(1+x^2)^(-0.5)};
            \addplot[semithick, color1] {1/(sqrt(2*pi))*exp(-(x^2)/2)};
            \addlegendentry{$f(x)$}
            \addlegendentry{$\mathcal{N}(0, 1)$}
        \end{axis}
    \end{tikzpicture}
    }
    \caption{\label{fig:mci-importance-example-func-plot}Plot of the function in
      \cref{eq:mci-importance-example-func}, enveloped by a standard normal distribution.}
\end{figure}
The integrand is plotted in \autoref{fig:mci-importance-example-func-plot}, and
the observant reader might recognise this as the product of a normal
distribution and a Student-t distribution. The correct value for the integral
is also given, so we have a reference for the results.

For comparison, lets start with the straightforward approach from before.
Because the integral goes to infinity the "volume" $V$ would not be well
defined, and we are forced to truncate the region manually. From looking at the
graph in \autoref{fig:mci-importance-example-func-plot} we may say that $x\in
[-5, 5]$ should account for the vast majority of the total integral. That means
we use the following estimate:

\begin{align}
    \label{eq:chp-mc-anon-2}
    I\approx \frac{10}{N}\sum_{i=1}^Nf(x_i)\qq{where} x_i\sim\text{Uniform}(-5, 5)
\end{align}

The main issue with this approach is that 1) we need to truncate the integral
manually and 2) now matter where we place the box boundaries we will tend to
sample a lot of $x_i$'s in areas where $f$ gives very small contributions to
the integral. Ideally we would like our sample points to be distributed as
closely as possible to $f$, in order to capture as much information as we can.
This is the idea of importance sampling. Instead of using the uniform
distribution for sampling, we use some probability distribution which more
closely resembles the integrand, call it $g(x)$. Formally we then restate the integral as follows:

\begin{align}
    I = \intfy\dd{x}f(x) = \intfy\dd{x} \frac{f(x)}{g(x)} g(x)
\end{align}
This can be interpreted as the expectation of $z(x)=\flatfrac{f(x)}{g(x)}$ for $x$'s drawn from $g$, and so the corresponding estimation is then:

\begin{align}
    I \approx \frac{1}{N} \sum_{i=1}^N \frac{f(x_i)}{g(x_i)} \qq{where} x_i\sim g
\end{align}
Setting $g = \text{Uniform}(-5, 5)$ recovers \autoref{eq:chp-mc-anon-2}, so this is simply the natural generalization of the standard approach for an arbitrary distribution function $g$.


Going back to the example, we should now chose a distribution function that
closely resembles the integrand, while still being simple to sample from. In
this (contrived) example, a natural choice is to use the standard normal
distribution, $g = \mathcal{N}(0, 1)$. In
\autoref{fig:mci-importance-example-func-plot} we can see how $g(x)$ is
enclosing $f(x)$ much more tightly than any rectangular box might hope to.

\begin{figure}
   \centering
    \resizebox{0.7\linewidth}{!}{%
        \input{scripts/monte_carlo_int_example.py.tex}
    }
    \caption{\label{fig:mci-importance-example-func-convergence}Convergence of
    the integral in \autoref{eq:mci-importance-example-func}, using regular
    Monte Carlo integration and Importance Sampling with $g = \mathcal{N}(0,
    1)$. The points indicate the approximated value of the integral for each
    particular experiment, and the shaded areas indicate the corresponding
    $\SI{95}{\percent}$ confidence intervals for the expected value. We see the
    latter displaying both greater accuracy and tighter confidence intervals.
    The source code for this graphic can be found~\cite[TODO: Add
    path]{MS-thesis-repository}, and \LaTeX{} output generated
    by~\cite{nico_schlomer_2018_1173090}.}
\end{figure}

\autoref{fig:mci-importance-example-func-convergence} show the convergence of
the Monte Carlo approximations towards the correct value for an increasing
number of sampled points. The drawn line is the mean value at each run, while
the shaded areas show the corresponding $\SI{95}{\percent}$ confidence
intervals. Because of the more suited sampling distribution, we obtain results
which are more accurate, and perhaps most importantly, tighter confidence
bounds. In general, using better sampling distributions will tend to give us
lower variance results.

\section{Sampling from Arbitrary Probability Distribution Functions}
\label{sec:sampling-arb-prob-dist-funcs}

So far we have taken for granted the ability to sample numbers from various
probability distribution functions. We dedicate this section to discuss sampling from the bottom up, culminating in a detailed presentation of the most central algorithm in this thesis, the Metropolis-Hastings algorithm.

\subsection{The Uniform Distribution}

The basic building block upon which we shall build all other random sampling
techniques is the ability to sample random numbers from the uniform
distribution, $\text{Uniform}(a, b)$. This is the simplest distribution
possible, where all numbers in the range $[a, b]$ have the same probability
density. The definition of its PDF is simply

\begin{align}\label{eq:uniform-pdf-definition}
    p_U\qty(x\;\middle|\; a, b) \defeq  \begin{cases}
        \frac{1}{b - a} &\qfor x\in [a, b]\\
        0&\qotherwise
    \end{cases}.
\end{align}
Most commonly we operate only with the \emph{standard uniform distribution}, $\text{Uniform}(0, 1)$. Any other range can be simply related via the trivially verifiable identity
\begin{align}
    p_U\qty(x\;\middle|\; a,b) = p_U\qty( \frac{x-a}{b-a} \;\middle|\; 0, 1).
\end{align}

But how exactly do we obtain random realizations from this PDF? Firstly we need
to depart from the typical notion of random, and realize that we are unable to
write an algorithm which produces a truly random result. While we could in
principle rely on nature to provide sources of complete, true randomness (so as
the exact behaviour of a quantum mechanical obserable), this is impractical
when we need fast, on-demand samples. Instead we settle for
\emph{pseudo-random} samples. A pseudo-random sequence $x_1, x_2,\dots x_n$
implies that one cannot\footnote{In any practical way, barring exhaustive trial
and error of every conceivable underlying algorithm.} predict $x_{n+1}$ without
insight into how the sequence is generated. In other words, the (ideal)
pseudo-random sequence is indistinguishable from a truly random sequence for
anyone observing the numbers $x_i$ alone.

There are a multitude of algorithms that can generate a sequence of
pseudo-random \emph{integers}, with a varying properties\footnote{Such properties
could include range of possible numbers, period, level of bias and how
cryptographically secure they are.}. One of oldest and most common family of
such algorithms is a linear congruential generator (LCG)~\cite{Knuth-1997-ACP-270146}. It defines a pseudo-random sequence by the simple recurrence relation
\begin{align}
    \label{eq:linear-congruential-generator-relation}
    x_{n+1} = (ax_n + c)\mod m,
\end{align}
where $a, c$ and $m$ are constants which determine the behaviour of the
algorithm, and $x_0$ needs to be specified manually. For instance,
\textcite{Numerical-Recipes-Press-et-al} use $a = 1664525$, $c=1013904223$ and
$m=2^{32}$.

This produces uniformly distributed integers in the range $[0, m)$. Numbers
from the standard uniform distribution can then simply be obtained by dividing
the integers by $m$. The complete algorithm is shown in Algorithm \autoref{alg:uniform-LCG-sampling}.

\begin{algorithm}[h]
    \caption{Sampling from $\text{Uniform}(l, u)$}
    \label{alg:uniform-LCG-sampling}
    \begin{algorithmic}[1]
        \Require $a, c, m$ and $x_0$
        \Function{unif}{lower bound $l$, upper bound $u$}
            \State $x\gets x_0$
            \Repeat
                \State $x\gets (ax + c)\mod m$
                \State \Yield $(\flatfrac{x}{m}) \times (u - l) + l$
            \Until{done}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Inverse Transform Sampling}

Armed with uniformly distributed random numbers, we now turn towards generating
numbers from other PDFs. Let $U\sim\text{Uniform}(0,1)$ be a uniformly
distributed stochastic variable, and let $X$ be a stochastic variable
associated with a PDF, $p$, and a corresponding cumulative distribution
function (CDF), $F$, i.e.

\begin{align}
    F(x) = \text{Pr}\qty(X\leq x) = \int_{-\infty}^x \dd{t}p(t).
\end{align}
We would like to define a transformation $T:[0, 1]\mapsto\mathbb{R}$ that can map from uniform numbers to numbers that follow the given CDF, i.e. define $T$ such that $T(U)\disteq X$. We have:
\begin{align}
    F(x) = \text{Pr}\qty(X\leq x) = \text{Pr}\qty(T(U)\leq x) = \text{Pr}\qty(U \leq T^{-1}(x)) = T^{-1}(x),
\end{align}
because $T^{-1}(x)\in[0, 1]$ by definition, and assuming that $T^{-1}(x)$ is strictly monotone. It follows then that $F^{-1}(U)\disteq X$, i.e. if the CDF is strictly monotone (as CDFs should be) then we can get realizations of $X$ by applying $F^{-1}$ to realizations of $U$.

The algorithm to sample from any PDF with a tractable inverse cumulative
distribution function is then extremely simple, and for completeness it is
listed in Algorithm \autoref{alg:inverse-transform-sampling}.

\begin{algorithm}[h]
    \caption{Inverse Transform Sampling}
    \label{alg:inverse-transform-sampling}
    \begin{algorithmic}[1]
        \Require Cumulative distribution function $F$
        \Ensure Random $x$ with CDF equal to $F$
        \Repeat
          \State \Yield $F^{-1}(\text{unif}(0, 1))$
        \Until{done}
    \end{algorithmic}
\end{algorithm}

\subsection{The Metropolis-Hastings Algorithm}
\label{sec:metro-hastings-alg}

Algorithm \autoref{alg:inverse-transform-sampling} works great for a number of standard
PDFs. For many cases, however, we do not have a tractable form for the inverse
CDF, which renders the algorithm useless. So what about the wave functions we
are interested in? The PDF in question is, as defined in \autoref{eq:wavefunc-probability-density-def},

\begin{align}
    P_{\psi}(\vX) \defeq \frac{\abs{\psi}^2}{\int\dd{\vX}\abs{\psi}^2}.
\end{align}
Sadly, computing $F^{-1}$ for this PDF, if even possible, would be a very costly
operation. In fact, the normalization integral in the denominator is enough to
make practical sampling from $P_\psi$ impossible. Even worse, considering that
$\psi$ is updated continuously throughout a VMC calculation, we could not even
cache the result of the integral after computing it once.

Luckily, we have a solution: The Metropolis-Hastings Algorithm. This algorithm
is the workhorse behind of a great ocean of applications, including VMC, and is
considered to be among the most influential algorithms of the 20th century.
While the algorithm itself is quite simple, the argument for \emph{why} it
works is a little more involved. The following sections will be dedicated to the
mathematical underpinnings. In the mean time, the full algorithm is shown
in~\Cref{alg:metropolis-hastings-general}.

\begin{algorithm}[h]
    \caption{Metropolis-Hastings Algorithm}
    \label{alg:metropolis-hastings-general}
    \begin{algorithmic}[1]
        \Require Probability density $P(\vx)$, proposal distribution $q\qty(\vx'\;\middle|\;\vx)$
        \Ensure Random $\vx$ drawn from $P$
        \State Initialize $\vx$, randomly or otherwise
        \Repeat
          \State Sample $\vx'\sim q\qty(\vx'\;\middle|\;\vx)$
          \State Sample $u\sim\text{unif}(0, 1)$
          \State $A\gets \flatfrac{P(\vx')q\qty(\vx\;\middle|\;\vx')}{P(\vx)q\qty(\vx'\;\middle|\;\vx)}$
          \If{$u \leq A$}
            \State $\vx \gets \vx'$
          \EndIf
          \State \Yield $\vx$
        \Until{done}
    \end{algorithmic}
\end{algorithm}

The most important thing about~\Cref{alg:metropolis-hastings-general} is the
fact that we only need to know the ratios of probabilities,
$\flatfrac{P(\vx')}{P(\vx)}$. This means that we don't need the probabilities to
be normalized to unity\footnote{We do need the probabilities to be
\emph{normalizable} though. In our case, where $P\propto \abs{\Psi}^2$, we have
already assumed this requirement in~\cref{sec:requirements-of-wave-functions}.},
meaning we don't have to compute the costly integral
in~\cref{eq:wavefunc-probability-density-def}. For our purposes this is
essential, and~\cref{alg:metropolis-hastings-general} is the reason why VMC is
possible.

\subsubsection{Formal Derivation}

The Metropolis-Hastings algorithm builds what is called a \emph{Markov Chain}. A
Markov Chain is a type of stochastic model that describes a sequence of possible
states. We imagine some space of possible states and a set of transition
probabilities, $T\qty(\vx'\;\middle|\;\vx)$, describing the likelihood of moving between them.
The defining property of a Markov Chain, as opposed to other stochastic
processes is that the transition probabilities $T\qty(\vx'\;\middle|\;\vx)$ from a state $\vx$
to a state $\vx'$ depends only on the current state and not on any previous
states. In other words, it matter only where you are, not where you have been.

For well behaved Markov Chains, as the number of steps in the chain increases,
the distribution of the states will asymptotically approach a stationary
distribution, $\pi(\vx)$. The Metropolis-Hastings algorithm works by carefully
constructing $T\qty(\vx'\;\middle|\;\vx)$ such that $\pi(\vx)=P(\vx)$, where $P(\vx)$ is the
desired probability distribution. 

In order to find the correct $T\qty(\vx'\;\middle|\;\vx)$, we require that the stationary
distribution $\pi(\vx)$ exists and that it is unique:

\begin{description}
\item[Existence:] A sufficient condition for the existence of $\pi(\vx)$ is that
  of \emph{detailed balance}. This says that the probability of being in state
  $\vx$ and transitioning to $\vx'$ is equal to the probability of being in state
  $\vx'$ and transitioning to $\vx$:
  \begin{align}
    \label{eq:detailed-balance}
    \pi\qty(\vx)T\qty(\vx'\;\middle|\;\vx) = \pi\qty(\vx')T\qty(\vx\;\middle|\;\vx').
  \end{align}
\item[Uniqueness:] The distribution $\pi(\vx)$ is unique if the Markov chain is
  \emph{ergodic}. This is the case when we don't return to the same state after
  a fixed number of transitions, and that all states can be reached in a finite
  number of transitions.
\end{description}
Deriving the correct $T\qty(\vx'\;\middle|\;\vx)$ starts by factoring the transition
probabilities into a proposal distribution, $q\qty(\vx'\;\middle|\;\vx)$, and an
acceptance ratio $A\qty(\vx',\vx)$. The idea is to use the former to propose the
next state, and use the latter to accept or reject the proposal. Inserting this
into \cref{eq:detailed-balance} and rearranging we get:

\begin{align}
  \frac{A\qty(\vx',\vx)}{A\qty(\vx,\vx')} = \frac{P\qty(\vx')}{P\qty(\vx)}\frac{q\qty(\vx\;\middle|\;\vx')}{q\qty(\vx'\;\middle|\;\vx)}.
\end{align}
We can now freely choose an acceptance ratio that satisfies the above. The
common choice is:

\begin{align}
  A\qty(\vx',\vx) = \min\qty(1, \frac{P\qty(\vx')}{P\qty(\vx)}\frac{q\qty(\vx\;\middle|\;\vx')}{q\qty(\vx'\;\middle|\;\vx)})
\end{align}

At this point, all that remains is to specify a proposal distribution
$q\qty(\vx'\;\middle|\;\vx)$. Depending on the choice of $q$ we get a sampler with
different attributes. We have implemented two version, presented in the follwing
sections.

\subsubsection{Metropolis Algorithm}

The simplest choice, referred to as simply the Metropolis algorithm, is to
choose a distribution such that
$q\qty(\vx'\;\middle|\;\vx)=q\qty(\vx\;\middle|\;\vx')$. That way the acceptance
ratio simplifies significantly, fully canceling out $q$. In our implementation
we have used a uniform proposal distribution:

\begin{align}
  q\qty(\vx' \;\middle|\; \vx) &= p_U\qty(\vx' \;\middle|\; \vx -\frac{\Delta x}{2}, \vx + \frac{\Delta x}{2}),
\end{align}
where $\Delta x$ is a \emph{step size} that regulates how large each
perturbation should be. This leads to the simplest form of
\cref{alg:metropolis-hastings-general}, given for completeness in
\cref{alg:metropolis-simple}. The algorithm given here is adapted particularly
for our purposes, where we want to sample a matrix, $\mat
X\in\mathbb{R}^{N\times D}$, for $N$ particles in $D$ dimensions. In particular,
our implementation yields a new sample after moving only one of the particles.
This is a common case for VMC applications, because this allows for a code
optimization that is crucial when using Slater determinants in wave functions.
We don't use such wave functions in this thesis, but future work might benefit
from this choice.

\begin{algorithm}[h]
    \caption{Metropolis Algorithm}
    \label{alg:metropolis-simple}
    \begin{algorithmic}[1]
        \Require Probability density $P(\vX)=\abs{\psi(\vX)}^2$, step size $\Delta x$
        \Ensure Random $\vX$ drawn from $P$
        \State Initialize $\vX$, randomly or otherwise
        \Repeat
          \For{$i \gets 1:N$}
            \State $\vX'\gets \vX$
            \For{$d \gets 1:D$}
              \State Sample $\delta\sim \text{unif}(-0.5, 0.5)$
              \State $X_{i,d}'\gets X_{i,d} + \Delta x \cdot \delta$
            \EndFor
            \State Sample $u\sim\text{unif}(0, 1)$
            \State $A\gets \flatfrac{P\qty(\vX')}{P\qty(\vX)}$
            \If{$u \leq A$}
              \State $\vX \gets \vX'$
            \EndIf
            \State \Yield $\vX$
          \EndFor
        \Until{done}
    \end{algorithmic}
\end{algorithm}

\subsubsection{Importance Sampling}

\cref{alg:metropolis-simple} suffers from the same issues that ordinary MCI
does, as opposed to the enhancement of Importance sampling. The proposal
distribution used in the Metropolis algorithm, while simple to implement, can
certainly be improved upon. The idea is to guide the random walk in space
towards areas of greater probability. We will now give a brief physical
motivation for the strategy, glossing over most technical details.

Particles will tend towards regions of space where $P(\vX)$ is large. We may say
that this is the result of a \emph{quantum drift force}. Without further
motivation, we define the drift force acting on particle $k$ as follows:

\begin{align}
  \vb{F}_k(\vX) = \frac{2\grad_k{\Psi(\vX)}}{\Psi(\vX)},
\end{align}
which points in the direction of greatest increase in $\Psi$ with respect to
particle $k$'s position. We also expect the
system to exhibit some degree of random motion, as it is a quantum system. This
combination of drift and random motion is described by the Langevin equation,

\begin{align}
 \pdv{\vx_k}{t} &= D\vb{F}_k + \vb{\eta}, 
\end{align}
where $D$ is called the drift coefficient, and $\vb{\eta}$ is a vector of
uniformly distributed random values (with zero mean) and accounts for the random
motion. We fix $D=\flatfrac{1}{2}$. Applying Euler's method to the Langevin
equation we can get:

\begin{align}
  \vx_k' = \vx_k + \frac{1}{2}\vb{F}_k\,\Delta t + \vb{\xi}\sqrt{\Delta t},
\end{align}
where $\Delta t$ is a time step similar to $\Delta x$ for plain Metropolis, and
$\vb{\xi}$ is a vector of values drawn from the standard normal distribution.
This is our new recipe for generating proposals $\vx'$. The final piece is 
to find the corresponding probability distribution,
$q\qty(\vx'\;\middle|\;\vx)$.

To this end, we consider the Fokker-Planck equation, which for one particle in
one dimension is written as

\begin{align}
  \label{eq:mc-annon-4}
  \pdv{\Psi}{t} &= D\pdv{}{x}\qty(\pdv{}{x} - F)\Psi,
\end{align}
where $D$ is as before and $F$ is the one-dimensional analog of $\vb{F}_k$. This
describes the time-evolution of a probability distribution under the influence
of a drift force and random impulses. We've let $\Psi$ play the role of the
probability distribution. \cref{eq:mc-annon-4} yields a solution given by the
following Green's function (for one particle):

\begin{align}
  \label{eq:mc-annon-5}
  G\qty(\vx_k', \,\vx_k,\,\Delta t) &\propto \exp[-\frac{\norm{\vx_k' - \vx_k - D\Delta t\,\vb{F}_k}^2}{4D\Delta t}].
\end{align}
This is interpreted as the probability of transitioning to position $\vx_k'$
from $\vx_k$ within a time interval $\Delta t$, so that finally we have the
proposal distribution:

\begin{align}
  q\qty(\vx'\;\middle|\;\vx) &=G\qty(\vx', \,\vx,\,\Delta t). 
\end{align}
Note that because we ever only need the ratio of $q$ values, there is no problem
that \cref{eq:mc-annon-5} only gives the proportionality. The final algorithm is
shown in \cref{alg:metropolis-importance}.

\begin{algorithm}[h]
    \caption{Metropolis-Hastings with Importance Sampling.}
    \label{alg:metropolis-importance}
    \begin{algorithmic}[1]
        \Require Probability density $P(\vX) = \abs{\psi(\vX)}$, step size $\Delta t$
        \Ensure Random $\vX$ drawn from $P$
        \State Initialize $\vX$, randomly or otherwise
        \Repeat
          \For{$i \gets 1:N$}
            \State $\vX'\gets \vX$
            \For{$d \gets 1:D$}
              \State Sample $\xi\sim \mathcal{N}(0, 1)$
              \State $X_{i,d}'\gets X_{i,d} + \sqrt{\Delta t} \cdot \xi +
              \frac{1}{2}\Delta t\,(\vb{F}_i(\psi))_d$
            \EndFor
            \State Sample $u\sim\text{unif}(0, 1)$
            \State $A\gets \flatfrac{P\qty(\vX')}{P\qty(\vX)}$
            \State $A\gets A\cdot \flatfrac{q\qty(\vx_i\;\middle|\;\vx_i')}{q\qty(\vx_i'\;\middle|\;\vx_i)}$
            \If{$u \leq A$}
              \State $\vX \gets \vX'$
            \EndIf
            \State \Yield $\vX$
          \EndFor
        \Until{done}
    \end{algorithmic}
\end{algorithm}
See \cref{sec:verify-sampling} for a demonstration of how
\cref{alg:metropolis-importance} compares to \cref{alg:metropolis-simple}.
\end{document}
