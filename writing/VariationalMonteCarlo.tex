
\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Variational Monte Carlo}
\label{chp:variational-monte-carlo}

Variational Monte Carlo (VMC) is a method for obtaining the ground state wave function of
a quantum mechanical system, and it constitutes the framework for all the calculations we
perform in this thesis. As this method is of great significance to us, this chapter is
devoted to all relevant definitions, derivations and technical details required for a
successful study of the systems of interest.


\section{The Variational Principle}

The fundamental principle that enables VMC is the observation that the wave function that
describes the ground state of a system, is the wave function which produces the lowest
expectation value of the energy. Read this statement a couple of times and it almost seems
like a circular definition, because we define the ground state as the state with the
lowest energy. Still, this simple observation is what allows us a way to go forward.

Let's formalize the above statement.

\begin{theorem}[Variational Principle]\label{thm:variational-principle}
    Suppose a time-independent Hamiltonian, $\hat H$, an associated set of eigenvalues,
    $E_0\leq E_1\leq\dots$ and
    a set of orthonormal eigenvectors, $\{\ket{\phi_n}\}$ spanning the associated
    Hilbert space, such that the Time-Independent
    Schr√∂dinger equation is satisfied,

    \begin{align}
        \hat H\ket{\phi_n} = E_n\ket\phi_n.
    \end{align}
    For any arbitrary $\ket\psi$ in the Hilbert space, the expectation value of $\hat H$
    for this wave function must satisfy
    \begin{align}
        \expval{\hat H}&\defeq \expval{\hat H}{\psi} \geq E_0.
    \end{align}
    where $E_0$ is the exact ground state energy, with equality if and only if
    $\ket\psi=\ket\phi_0$.
\end{theorem}
\begin{proof}
    Because $\{\ket{\phi_n}\}$ constitutes a complete orthonormal basis of the Hilbert
    space, we can expand $\ket\psi$ as
    \begin{align}
        \ket\psi = \sum_n c_n\ket\phi_n \qq{with} \sum_n \abs{c_n}^2 = 1.
    \end{align}
    We then have
    \begin{align}
        \expval{\hat H}{\psi} &= \qty(\sum_m c^*_m\bra{\phi_m})\hat H\qty(\sum_n
        c_n\ket{\phi_n})\\
        &= \sum_n\sum_m c_m^*c_n\bra{\phi_m}\hat H\ket{\phi_n}\\
        &= \sum_n\sum_m c_m^*c_nE_n\bra{\phi_m}\ket{\phi_n}\\
        &= \sum_n\sum_m c_m^*c_nE_n\delta_{mn}\\
        &= \sum_n \abs{c_n}^2 E_n\\
        &\geq\sum_n \abs{c_n}^2 E_0 = E_0.
    \end{align}
    It is clear that equality only occurs when $c_n = \delta_{0n}$, which implies
    $\ket\psi=\ket{\phi_0}$.
\end{proof}

If we require the test wave function $\ket\psi$ to be orthogonal to $\ket{\phi_0}$, then
the above derivation results in a similar condition for determining the first excited
state. By applying the same reasoning iteratively, the same can be said about any higher
energy state. So in principle, we can use the variational principle to first find the
ground state, then find an orthogonal state that gives the first excited energy, then find
a state orthogonal to both that gives the second excited energy etc. We will not pursue
any excited states in this thesis, but it is useful to know that we could in principle
extend our results if need be.

\section{The Variational Monte Carlo Algorithm}

Rooted in the Variational Principle, the general approach to finding a good estimate of
the ground state goes as follows:

\begin{enumerate}
    \item Define the Hamiltonian of interest, $\hat H$
    \item Propose a trial wave function $\ket{\vb \alpha}$ dependent on some free parameters
        $\vb \alpha=\qty(\alpha_1, \alpha_2,\dots, \alpha_m)$
    \item Return the best possible wave function $\ket{\vb\alpha}$ such that $\expval{\hat
        H}{\vb\alpha}$ is minimized w.r.t the parameters $\vb \alpha$.
\end{enumerate}
The first step is trivial, as we shall assume that the Hamiltonians we want to examine are
given in advance.\footnote{Clearly, deriving/defining Hamiltonians that accurately describe a given
system is not a trivial task in general, and might require great levels of theoretical
work and/or experimental data analysis. But from our perspective, this work has already
been done.}

The second and third points are where we will spend our efforts, and so we devote extra
attention to these two points in particular.


\section{The Trial Wave Function}

Critical to the outcome of a VMC calculation is the quality of the proposed trail wave
function. In order to have stable and sensible results we should also take care that the
wave function obeys the requirements stated in
\autoref{sec:requirements-of-wave-functions}. Beyond these basic assertions, it is
imperative that the functional form of the proposed wave function is capable of expressing
the true underlying ground state, or at least a good approximation of it. As an example,
take the simple case of the idealized one-dimensional harmonic oscillator presented in
\autoref{sec:simple-non-inter-HO}, with the ground state expressed as
\begin{align}
    \phi_0(x) \propto \exp(- \frac{1}{2} x^2)
\end{align}
If we proposed a trial wave function on a form
\begin{align}
    \psi(x; \alpha)\propto \exp(- \alpha x^2),
\end{align}
we could expect VMC to produce the exact ground state after optimizing $\alpha$. The above
is a good trial wave function because its functional form contains the exact state.

Suppose now we proposed something different, maybe because a of lacking theoretical
understanding. For the sake of example, say we chose this visually similar
function\footnote{Visually similar in the sense that plotting them both yield similar bell
shaped curves, not in terms of their algebra}:
\begin{align}
    \psi(x; \alpha)\propto \frac{1}{1 + \alpha x^2}
\end{align}
Now matter how we tune the $\alpha$ parameter here, we will never achieve the lowest
ground state energy, simply because the ground state is not possible to express within this
functional form. We can now only hope to get approximations, acting as upper bounds on the
ground state energy.

\subsection{Slater-Jastrow: Standard Approach}

In order to come up with good trail wave functions we have a standard approach which often
serves us well. Typically we are considering a system of many interacting particles in
some external potential (e.g. the quantum dots system). In such cases the typical approach
is to build a trial wave function as a product of a single-particle part, $S$, and a correlation
part $J$

\begin{align}
    \ket\psi = \ket{S} \ket{J}
\end{align}

\subsubsection*{Slater}

The idea with this structure is that we put all the physical insight we have into these
two components individually. For the single particle part we include what we can deduce
about the system if we idealize it in some way. Typically that involves looking at the
states when there is only one particle, or considering some simplified interaction between
all of them (e.g. Hartree-Fock approximation). The basis functions we obtain from such an
analysis are then placed in a determinant/permanent structure so as to fix the symmetry
requirements of fermions/bosons, respectively. This factor is then called the \emph{Slater
factor}.


For clarity, we present a couple different Slater forms. We consider the two-dimensional
quantum dots system from \autoref{sec:quantum-dots-theory} for $N$ particles.  We have
$\vb x_i=(x_i, y_i)$ being the position of the particle $i$, and $\vb X\defeq (\vb x_1,
\dots \vb x_N)$ defined as a shorthand. Let $\vb\Phi$ be a matrix defined by $\Phi_{i,j}
\defeq \phi_{i}(\vb x_j)$, where the $\phi_i$ is part of the set of orbitals
$\{\phi_{0,0}, \phi_{1,0},\phi_{0,1}, \phi_{1,1},\dots\}$, as defined in
\autoref{eq:ho-single-particle-orbitals}. Lastly, let $\vb \Phi^{(M)}$ denote the
resulting matrix with $M$ rows, one for each of the $M$ lowest energy orbitals. Finally,
if the particles are fermions we define the Slater factor as\footnote{Spin considerations not mentioned here, as they will not be important for the particular systems we will investigate. In general though, the structure of the Slater factor needs to account for spin in a well defined way.}

\begin{align}
    S(\vb X)_\text{fermions} \defeq \text{determinant}\qty(\vb\Phi^{(M)}).
\end{align}
Due to the properties of the determinant, this has the anti-symmetry required for
fermions.

In the case of bosons, we could just change the above to
\begin{align}
    S(\vb X)_\text{bosons} \defeq \text{permanent}\qty(\vb\Phi^{(M)}).
\end{align}
However, there are far less computationally expensive ways to get a \emph{symmetric}
result, as required for bosons. A simple product of single particle ground states is
sufficient in this case.
\begin{align}
    S(\vb X)_\text{bosons} \defeq \prod_{i}^N \phi_{0,0}(\vb x_i).
\end{align}

The general structure is constant across different systems, changing only the basis
functions we use to express the matrix elements.

In some cases however, we will struggle find basis functions that describe a meaningful
portion of the systems behaviour in this way. In such challenging cases, the Slater factor
could be reduced to only being responsible for supplying the correct symmetry. In the case
of bosons, that would mean the complete removal of the Slater factor from the trial wave
function. We will see this when looking further into the liquid Helium system.



\subsubsection*{Jastrow}

Anything not covered by this analysis is meant to be accounted for in the correlation
term, $J$. This is typically a function on the following generalized form, commonly called a Jastrow
factor:
\begin{align}
    J(\vb X) &= \exp(\sum_{i < j} U(r_{ij}))
\end{align}
where $U(r)$ is some function dependent on the inter-particle distance only. Note that by
convention the Slater factor has the correct symmetry, so the correlation factor should
always be symmetrical in order to maintain the same total symmetry.

Any variational parameters $\vb \alpha$ are typically introduced as part of the Jastrow
factor. The Slater factor represents everything we are sure should be included
from a theoretical approach, while the Jastrow term should ideally account for all our ignorance.
It is therefore typically expressed with a few free parameters.

Again, for the sake of clarity, we state an example Jastrow factor. One of the most
commonly used is the Pade-Jastrow factor~\cite{Drummond-Towler-Needs-2008}:
\begin{align}
    J(\vb X) = \prod_{i < j} \exp(\frac{\alpha r_{ij}}{1 + \beta r_{ij} } )
\end{align}
Here $\beta$ is the only variational parameter, and $\alpha$ is fixed depending on the
dimensionality and spin of the particles. More complicated versions exist, containing
higher order polynomials of $r_{ij}$ in the exponential. This particular form
will prove to be a very good choice for the quantum dots system.




\section{Optimization}

At this point, we have a Hamiltonian $\hat H$ defining our system of interest,
and we have proposed a trial wave function $\ket{\vb\alpha}$ based on the
theoretical intuition we have. The wave function has by design some free
parameters $\vb\alpha$ which we now want to pin down to their optimal values.

From the Variational Principle (\autoref{thm:variational-principle}), we know
that the best set of parameters are those that minimizes the expected energy,
$\expval{\hat H}{\vb\alpha}$. As such, a natural place to start would be to
determine an efficient way to evaluate said energy.

\subsection{Local Energy}

Denote the trial wave function as $\psi_{\vb\alpha}(\vb X)$, where $\vb\alpha$
are the variational parameters and $\vX=(\vx_1, \vx_2,\dots,\vx_n)$ is the
matrix of all the degrees of freedom for each particle (e.g. position
coordinates). The expectation value for the energy of the system is

\begin{align}
    \expval{\hat H} &= \frac{\expval{\hat H}{\vb\alpha}}{\braket{\vb\alpha}}\\
    &= \frac{\int \dd{\vb X} \psialpha^*\hat H\psialpha}{\int\dd{\vX}\abs{\psialpha}^2}.
    % &= \int\dd{\vX} P(\vX,\valpha)E_L(\vX,\valpha),
\end{align}
%
We now restructure this integral a bit. Remembering the interpretation of
the wave function as a probability distribution, we can write the \emph{probability
density} for a given system configuration $\vX$ as

\begin{align}
    P_{\psi_{\vb{\alpha}}}(\vX) \defeq \frac{\abs{\psialpha}^2}{\int\dd{\vX}\abs{\psialpha}^2}\label{eq:wavefunc-probability-density-def}
\end{align}
where we account for the possibility of $\psialpha$ not being normalized to unity. Further, we define a quantity we will call the \emph{local energy},
\begin{align}
    E_L &\defeq \frac{1}{\psialpha} \hat H\psialpha.\label{eq:local-energy-def}
\end{align}
The reason for these definitions is that now we can write the energy expectation as follows:

\begin{align}
    \expval{\hat H} &= \int\dd{\vX} P_{\psi_{\valpha}} (\vX) E_L(\vX).
\end{align}
Why is this better? Because this is simply the weighted average of the local
energy, something which has a straightforward discrete formulation:
\begin{align}
    \expval{\hat H} &= \lim_{N\rightarrow \infty} \frac{1}{N} \sum_{i = 1}^N E_L(\vX_i)
    \qq{where} \vb X_i\sim P_{\psi_{\valpha}}
\end{align}
So, if we can sample $\vX$ from the distribution described by our trial wave
function, we simply need to evaluate the local energy at a sufficiently large
number of configurations  to obtain a good estimate of the expectation value of
the Hamiltonian. How exactly to do the sampling is a rather large topic, and
will be handled in \autoref{chp:monte-carlo}.

\subsection{Updating Parameters}

Now that we can evaluate the energy, the function to minimize, we could in
principle start optimizing our parameters. If the parameters are few and
discrete we could simply evaluate the energy for all of them and pick the one
which gives the best result. However, this approach will quickly become
intractable for even a modest number of parameters, and more intelligent
methods should be pursued.

Perhaps the most common optimization strategy relies on computing the gradient
of the energy w.r.t. $\valpha$, and then change $\valpha$ in the opposite
direction of this. After all, a gradient by definition points in the direction
of steepest ascent, so going the other way will be the direction of steepest
decent. How exactly to make these changes are a topic of its own, and discussed in depth in \autoref{chp:machine-learning}, but the simplest implementation is as follows:

\begin{align}
    \valpha^{(i+1)} = \valpha^{(i)} - \eta \grad_{\valpha}{\expval{\hat H}}
\end{align}
for a suitably chosen hyperparameter $\eta\in\mathbb{R}$, typically $\eta\ll 1$. After a sufficient number of iterations, we hope that the parameters converge to their optimal values.

This raises another question, namely how to compute $\grad_{\valpha}{\expval{\hat H}}$. Using the chain rule and the hermiticity of the Hamiltonian\footnote{TODO: Give full derivation.}, we can obtain the following expression:

\begin{align}
    \grad_{\valpha}{\expval{\hat H}} &=
    2\qty[\expval{\frac{E_L}{\Psi_T}\pdv{\Psi_T}{\alpha}} -
    \expval{E_L}\expval{\frac{1}{\Psi_T}\pdv{\Psi_T}{\alpha}}]\label{eq:local-energy-gradient-alpha}
\end{align}

With this final piece of the puzzle we have everything we need in order to perform a VMC calculation for the ground state of a system.

\end{document}
