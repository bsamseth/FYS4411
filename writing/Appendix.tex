\documentclass[Thesis.tex]{subfiles}
\begin{document}
\chapter{Appendices}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thesubsection}{\Alph{section}.\Roman{subsection}}

\section{Notation Reference}

This thesis strives to use a consistent set of rules for notation throughout.
Where possible, standard notation choices have been made so as to be easily
comparable to other works. Sometimes, however, we cannot all agree on how things
should be written, and this thesis inevitably diverges from some part of the
readers preferences. In an attempt to soften the inevitable rage of notation
disagreements, this section serves as a reference for all standard
notation used in this thesis, such that the meaning of every expression should
be unambiguous and clear.

\subsection{Scalars, Vectors and Matrices}

\begin{description}
\item[Scalars:] $x$\hfill\\
  Use lowercase symbols.
\item[Vectors:] $\vx$ \hfill\\
  All vectors are column vectors, and use lowercase, bold-face symbols.
\item[Matrices:] $\mat X$ \hfill \\
  Use uppercase, bold-face symbols.
\end{description}

Example: The matrix $\mat X\in\mathbb{R}^{m\times n}$ is defined by its column
vectors $\{\vx_i\}_{i=1}^n$ (with $\vx_i\in\mathbb{R}^{m}$) such that $ \mat X =
(\vx_1\ \vx_2\ \dots\ \vx_n)$ column vectors, such that the scalar $X_{ij}$ is
the $i$'th component of $\vx_j$.

\subsection{Indices}

Some indices are meant to be summed over, while others remain fixed throughout a
calculation. To help making the distinction between the two, the following
sequences of indices is used for each class, in decreasing order of
precedence:

\begin{description}
\item[Summation indices:] $i, j, a, b, c, d, e, f$
\item[Fixed indices:] $k, l$
\end{description}

\subsection{Summation}

\subsubsection{Scope}

The summation symbol, $\sum$, effects only the term imediately after it. That
is,
\begin{align}
  \sum_{i=1}^3 i + 1 = \qty(\sum_{i=1}^3 i) + 1 = 7, \qq{not} \cancel{\sum_{i=1}^3i + 1 = \sum_{i=1}^3(i + 1) = 9}
\end{align}

\subsubsection{Implied Summations}

For notational brevity (and to avoid visual clutter), summations are not always
stated explicitly as in the above example. In these cases the limits of the
summation are determined by their context.

\begin{description}
\item[Explicit limits:] \hfill\\
  In $\sum_{i=1}^n$, the sum goes from 1 to n, inclusively.
\item[Implicit limits:]\hfill\\
  When limits are clearly defied by their context, the following is equivalent:
  $\sum_{i=1}^n \equiv \sum_{i}^n\equiv \sum_i$.
\item[Implicit summation symbols:]\hfill\\
  We make heavy use of the Einstein summation convention when this is
  appropriate. Any summation index (see above) which appears more than once with
  \emph{a single term} is implicitly summed over all its possible values.
  Superscripts in parenthesis are excluded from this rule, i.e. $a^{(k)}b^{(k)}$
  does not have an implied sum.
\end{description}

Example: Consider the matrix equation $\vb{x} = \mat{A}\vb{b}$, for
$\mat{A}\in\mathbb{R}^{m\times n}$ and $\vx, \vb{b} \in \mathbb{R}^n$. Written
explicitly, all of the following statements about the elements of $\vx$ are equivalent:
\begin{align}
  \begin{split}
  x_k &= \sum_{i=1}^n A_{ki}b_i,\qq{ } x_k = \sum_{i}^n A_{ki}b_i,\\
  x_k &= \sum_{i} A_{ki}b_i,\qq{ } x_k = A_{ki}b_i
  \end{split}
\end{align}
\subsection{Symbols}
\begin{center}
\begin{tabular}{cl}
  Symbol & Reads as\\
  \hline\\
  $=$ & is equal to\\
  $\defeq$ & is defied as\\
  $\equiv$ & is equivalent to\\
  $\disteq$ & is distributed equal to
\end{tabular}
\end{center}

\section{Symmetry Metric}

In the context of our Variational Monte Carlo framework, we should in general
take care that the ansatz we make for the functional form of $\Psi_T$ is such
that it obeys \autoref{theorem:spin-statistic}. Not all proposed wavefunctions
will necessary \emph{guarantee} this property, however, and optimization might
find a minima in which this is not true. A such, we shall define a metric to
measure the \emph{symmetry-ness} of a wavefunction.

Before giving a definition, there are a couple of desired properties such a
metric should have.

\begin{enumerate}
    \item Fully symmetric functions should yield a distinct, finite value.
    \item Fully anti-symmetric functions should yield a distinct, finite value,
        which must be different from fully symmetric functions.
    \item Scale invariant, i.e. scaling the function by
        some constant factor does not change its symmetry metric.
    \item Computationally feasible to evaluate.
\end{enumerate}

\begin{definition}{Permutation Operator.}

     Let $\mathcal{P}_n$ denote the set of all permutations of the first $n$
     natural numbers, $\{1, 2, \dots, n\}$, and let $\vec\alpha\in \mathcal{P}_n$
     be one of these. Given a function $\Psi(\vec X_1, \vec X_2,\dots,\vec X_n)$ of
     $n$ vectors $\vec X_i\in \mathbb{R}^f$, we let $P_{\vec\alpha}$ be an
     operator with the following property:

    \begin{align}
        P_{\vec\alpha}\Psi(\vec X_1,\vec X_2,\dots,\vec X_n)
        \defeq
        \Psi(\vec X_{\alpha_1},\vec X_{\alpha_2},\dots,\vec X_{\alpha_n}).
    \end{align}
\end{definition}

\begin{definition}{Symmetry Metric.}

    Given a function $\Psi(\vec X_1, \vec X_2,\dots,\vec X_n)$ of $n$ vectors $\vec
    X_i\in \mathbb{R}^f$. We use the notation $\dd{\vec X}\defeq \dd{\vec X_1}\dots\dd{\vec X_n}$.
    The Symmetry Metric of $\Psi$ is then defined as the following:

    \begin{align}\label{eq:symmetry-metric-def}
        S(\Psi) &\defeq  \ddfrac{\intfy \dd{\vec X}\abs{\frac{1}{n!}
        \sum_{\vec\alpha\in \mathcal{P}_n} P_{\vec\alpha}\Psi}^2}{\intfy\dd{\vec X}
        \max_{\vec\alpha\in \mathcal{P}} \abs{P_{\vec\alpha}\Psi}^2}
    \end{align}
\end{definition}
\begin{corollary}
    The symmetry metric is bounded to the interval $[0, 1]$ for all functions where the integrals are defined.
\end{corollary}
\begin{proof}
    Both the numerator and denominator of \autoref{eq:symmetry-metric-def} are
    integrals of absolute values. Hence they cannot be negative, which proves
    the lower bound $S(\Psi) \geq 0$ for all $\Psi$ where the integrals are defined. Further, the
    absolute value of the average of a set must be less than or equal to the maximum
    absolute value of the set, and equal iff. all values are equal. That is,
    \begin{align}
        \abs{\frac{1}{n!} \sum_{\vec\alpha\in \mathcal{P}_n} P_{\vec\alpha}\Psi}^2
        \leq \max_{\vec\alpha\in \mathcal{P}} \abs{P_{\vec\alpha}\Psi}^2.
    \end{align}
    From this it follows that $S(\Psi) \leq 1$.
\end{proof}
\begin{corollary}
   The symmetry metric is scale invariant, i.e.
    \begin{align}
        S(c\Psi) = S(\Psi),
    \end{align}
    for any scalar $c\in \mathbb{C}$.
\end{corollary}
\begin{proof}
   Obvious.
\end{proof}
\begin{corollary}
    For a symmetric function $\Psi$ we have

    \begin{align}
        S(\Psi) &= 1.
    \end{align}
\end{corollary}
\begin{proof}
    For a symmetric function we have $P_{\vec\alpha}\Psi=\Psi$ for all
    $\vec\alpha\in\mathcal{P}_n$ by definition. The result then follows directly.\end{proof}
\begin{corollary}
    For a anti-symmetric function $\overline\Psi$ we have
    \begin{align}
        S(\overline\Psi)&=0.
    \end{align}
\end{corollary}
\begin{proof}
   Of the $n!$ possible permutations, half can be written as an even number of
    pairwise exchanges, and half as an odd number. This implies
    \begin{align}
        \sum_{\vec\alpha\in \mathcal{P}_n} P_{\vec\alpha}\Psi = 0,
    \end{align}
    and thus the result following from this.
\end{proof}

This definition of the symmetry metric has all the attributes we wanted, except
the computational cost. The asymptotic complexity of $S$ is $\mathcal{O}(n!)$,
which is about as bad as asymptotic complexities get. In addition, the integrals
are many-dimensional and intractable for any non-trivial $n$. In practice we
will therefore approximate $S$ by only considering a random subset of all the
permutations, and of course approximating the integrals with Monte Carlo
integration as before.

This now provides a consistent metric for comparing potential wavefunctions in
terms of their symmetry. This will be helpful when considering trail
wavefunctions whose functional form does not guarantee symmetry, and to observe
to what extent optimizing them changes the symmetry metric favourably.

\section{Derivatives for the Symmetric RBM}

The standard RBM definition is stated for reference:
\begin{align}\label{eq:rbm-re-def}
    \Psi(\mathbf{X}) &= \exp(-\sum_{i=1}^M \frac{(X_i - a_i)^2}{2\sigma^2} )
                        \prod_{j=1}^N\qty[1 + \exp(b_j + \sum_{i=1}^M \frac{X_i
                        W_{ij}}{\sigma^2})]\\
    &\defeq \exp(-\sum_{i=1}^Mu_i)\prod_{j=1}^N\qty(1 + \exp(v_j)).
\end{align}

We now define that the RBM has $M$ degrees of freedom, as before, and that each
particle has $f$ of them related to its coordinates. In this case, for the RBM
to be symmetric upon the exchange of particles, each group of $f$ entries in
$\vec X$ should be treated equally within the wavefunction.

We define $\vec b\in \mathbb{R}^{N}$ as before, i.e. one bias value per hidden
node in the RBM. Let $\vec a\in \mathbb{R}^{f}$ be a vector of biases, with one
value \emph{per coordinate direction}. Further, let $\vec W\in \mathbb{R}^{f
\times N}$ be a matrix of weights. The weights connect each coordinate to every
hidden node. For notational breviety, we also denote $c(i) \defeq i \pmod{f} + 1$. We
define the symmetric version of the RBM then as follows:

\begin{align}\label{eq:rbm-sym-def}
    \Psi_S(\mathbf{X}) &=\exp(-\sum_{i=1}^M \frac{\qty(X_i - a_{c(i)})^2}{2\sigma^2} )
                        \prod_{j=1}^N\qty[1 + \exp(b_j + \sum_{i=1}^M \frac{X_i
                        W_{c(i)j}}{\sigma^2})]\\
    &\defeq \exp(-\sum_{i=1}^Mu_i')\prod_{j=1}^N\qty(1 + \exp(v_j')).
\end{align}
We notice that \autoref{eq:rbm-sym-def} is equivalent to \autoref{eq:rbm-re-def}
when $f=M$, and that all that has changed is the subsittution $i\rightarrow
c(i)$. The symmetry of this function is now clear, seing as shifting any index
$i\rightarrow i + k\times f$ for some integer factor $k$ will result in the same
output. This operation is equivalent to changing the order of the particles.

We now derive the explicit derivative that are needed in within the VMC
framework.

\begin{align}
    \frac{1}{\Psi_S} \pdv{\Psi_S}{a_{c(k)}} &= \pdv{\ln
    \Psi_S}{\Psi_S}\pdv{\Psi_S}{a_{c(k)}} = \pdv{\ln \Psi}{a_{c(k)}}\\
    &= \pdv{}{a_{c(k)}}\qty[ -\sum_{i=1}^M \frac{\qty(X_i -
    a_{c(i)})^2}{2\sigma^2} + \text{Constant w.r.t. } a_{c(i)} ]\\
    &= \sum_{i=k\atop{c(i)=c(k)}}^M \frac{X_i -
    a_{c(i)}}{\sigma^2}.\label{eq:rbm-sym-deriv-a}
\end{align}
where the last sum indicates a sum over all $i$ where $1 \leq i \leq M$ such
that $c(i) = c(k)$. All these terms will contain the same bias $a_{c(k)}$.

Now the hidden node biases:

\begin{align}
    \frac{1}{\Psi_S} \pdv{\Psi_S}{b_k} &= \pdv{\ln\Psi_S}{b_j}\\
    &= \pdv{}{b_k}\qty[\text{Constant w.r.t. }b_k +
    \sum_{j=1}^N\ln\qty(1+\exp(v_j'))]\\
    &= \frac{\exp(v_k')}{1 + \exp(v_k')} = \frac{1}{1 + \exp(-v_k')}\label{eq:rbm-sym-deriv-b}
\end{align}
which is the same form as for the regular RBM, as we would expect because the
hidden node biases are defined the same way.

Lastly we consider the weights:
\begin{align}
    \frac{1}{\Psi_S} \pdv{\Psi_S}{W_{kl}} &= \pdv{\ln\Psi_S}{W_{kl}}\\
    &= \pdv{}{W_{kl}}\qty[\text{Constant w.r.t. }W_{kl} +
    \sum_{j=1}^N\ln\qty(1+\exp(v_j'))]\\
    &= \frac{\exp(v_l')}{\ln(1 + \exp(v_l'))} \pdv{v_l'}{W_{kl}}\\
    &= \frac{1}{1 + \exp(-v_l')}\sum_{i=k\atop{c(i)=c(k)}}^M
    \frac{X_i}{\sigma^2} \label{eq:rbm-sym-deriv-W}
\end{align}
which is simply the sum over all suitable $k$'s of the derivatives we obtain
from the regular RBM.

Finally, we shall compute the first- and second order derivatives needed. Before
jumping into this, we can note that the transformation $\Psi\rightarrow\Psi_S$ really
only changed the indices of the weights and biases, but did not change how the
$X_i$ components appear in the wavefunction. If we were to start carying out the
same precedure as done for RBM in ??, we would quickly see that we end with the
same result, s.t. the same transformation. We obtain

\begin{align}
    \frac{1}{\Psi_S} \pdv{\Psi_S}{X_k} &= \frac{1}{\sigma^2}\qty(a_{c(k)}-x_k +
    \sum_j^N \frac{w_{c(k)j}}{1+e^{-v_j'}}).\label{eq:rbm-sym-first-order-deriv}
\end{align}
\begin{align}
    \begin{split}
    \frac{1}{\Psi_S}\pdv[2]{\Psi_S}{X_k}
        &= -\frac{1}{\sigma^2} + \sum_j^N
        \frac{w_{c(k)j}^2}{\sigma^4}
        \frac{e^{-v_j'}}{\qty(1+e^{-v_j'})^2} \\
        &\quad{   } +
        \frac{1}{\sigma^4}\qty(a_{c(k)}-x_k+\sum_j^N
        \frac{w_{{c(k)}j}}{1+e^{-v_j'}})^2
    \end{split}\label{eq:rbm-sym-second-deriv}
\end{align}
\end{document}
